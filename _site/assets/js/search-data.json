{"0": {
    "doc": "Across Cloud Platforms",
    "title": "Working across cloud platforms",
    "content": "Chapter Layout . | Objective | Key Questions to Ask | IBM’s Approach | Sighed Benefits  | Risks | Client Stories | . ",
    "url": "http://localhost:4000/githubPage/across-platforms#working-across-cloud-platforms",
    "relUrl": "/across-platforms#working-across-cloud-platforms"
  },"1": {
    "doc": "Across Cloud Platforms",
    "title": "Across Cloud Platforms",
    "content": " ",
    "url": "http://localhost:4000/githubPage/across-platforms",
    "relUrl": "/across-platforms"
  },"2": {
    "doc": "Automating Non-Cloud Environment",
    "title": "Automating Non-cloud Environment",
    "content": " ",
    "url": "http://localhost:4000/githubPage/automate-noncloud#automating-non-cloud-environment",
    "relUrl": "/automate-noncloud#automating-non-cloud-environment"
  },"3": {
    "doc": "Automating Non-Cloud Environment",
    "title": "What is it &amp; why do it?",
    "content": "While enterprises are still transitioning from traditional to cloud infrastructure and adopting more &amp; more cloud-native technology &amp; practices, they still own and operate infrastructure which is neither ready for not compatible with much of these modern cloud-native technologies and practices. More importantly, there is a lot of inefficiencies in how these systems are operated when seen with a contemporary lens locking up huge opportunities and being able to unlock that cost-benefit in any manner - even if not in its entirety leads to big contributions to the overall Cloud adoption ROI. Examples . Need examples where for lack of automation, traditional environments are costing time and money . Benefits . | Infrastructure utilization optimization | Reduced cost of non-reliable systems | Allows agility for the system | Allows agility for dependent cloud initiatives | . ",
    "url": "http://localhost:4000/githubPage/automate-noncloud#what-is-it--why-do-it",
    "relUrl": "/automate-noncloud#what-is-it--why-do-it"
  },"4": {
    "doc": "Automating Non-Cloud Environment",
    "title": "IBM Approach",
    "content": "Ansible &amp; other automation technology based solutions. ",
    "url": "http://localhost:4000/githubPage/automate-noncloud#ibm-approach",
    "relUrl": "/automate-noncloud#ibm-approach"
  },"5": {
    "doc": "Automating Non-Cloud Environment",
    "title": "Automating Non-Cloud Environment",
    "content": " ",
    "url": "http://localhost:4000/githubPage/automate-noncloud",
    "relUrl": "/automate-noncloud"
  },"6": {
    "doc": "BNP Paribas",
    "title": "Multicloud Transformation &amp; Operating Model at BNP Paribas",
    "content": ". IBM worked with BNP Paribas over five months to deliver a Strategic Engagement defining its 2022 Multicloud Transformation and Operating Model, supported by a Cloud Competence Center, specifically to provide: . | Creation of new Cloud Services, in particular PaaS | Integration of brokered Public Cloud Services | Adoption of new skills and roles to drive an Agile culture | Review of security and compliance for Cloud and DevOps | Review of app portfolio for containerization and migration | Design of key assets, including Architecture Center Portal | Design of a multi-Cloud DevOps Toolchain and Practice | . This covered a multicloud &amp; legacy portfolio of 4.000+ Applications in terms of service catalog, chargeback, architecture topology, toolchain, migration, etc. ",
    "url": "http://localhost:4000/githubPage/bnp-paribas-opmodel#multicloud-transformation--operating-model-at-bnp-paribas",
    "relUrl": "/bnp-paribas-opmodel#multicloud-transformation--operating-model-at-bnp-paribas"
  },"7": {
    "doc": "BNP Paribas",
    "title": "Client Context",
    "content": ". | Among the world’s Top 10 Banks based out of Europe; was lacking an accelerated approach for a holistic Cloud Transformation. | The Project dedicated to the Client-IBM JV (multi-billion contract), providing IT &amp; Cloud services to the Group’s business units. | New Group CIO demanding IT to show true Cloud capabilities comparable to leading market player to avoid increasing shadow IT. | Strategy &amp; Initiation phase spanning across the IT organization including Operating Model and CCC Creation. | . ",
    "url": "http://localhost:4000/githubPage/bnp-paribas-opmodel#client-context",
    "relUrl": "/bnp-paribas-opmodel#client-context"
  },"8": {
    "doc": "BNP Paribas",
    "title": "IBM Solution",
    "content": "JV with IBM: A holistic Multicloud Transformation &amp; Business-centric Operating Model driven through a CCC. | IBM Solution | . | | . | 3 use-case based pilot solutions in the process of rollout: DevOps Toolchain, Architecture Center Portal, IBM Cost &amp; Asset Management. | 8 new Services / Capabilities identified as 2019 backlog. Initial Cloud Affinity Assessment of 3000+ Applications performed (using BlueCAT). | 2 New organizational entities, CCC and Transformation Management Unit designed for “Crawl” phase. | A holistic 2022 Cloud Transformation Roadmap and Governance System. | . ",
    "url": "http://localhost:4000/githubPage/bnp-paribas-opmodel#ibm-solution",
    "relUrl": "/bnp-paribas-opmodel#ibm-solution"
  },"9": {
    "doc": "BNP Paribas",
    "title": "BNP Paribas",
    "content": " ",
    "url": "http://localhost:4000/githubPage/bnp-paribas-opmodel",
    "relUrl": "/bnp-paribas-opmodel"
  },"10": {
    "doc": "Capabilities & Transition",
    "title": "What capabilities do I need to operate to fulfil our vision?",
    "content": ". ",
    "url": "http://localhost:4000/githubPage/capabilities#what-capabilities-do-i-need-to-operate-to-fulfil-our-vision",
    "relUrl": "/capabilities#what-capabilities-do-i-need-to-operate-to-fulfil-our-vision"
  },"11": {
    "doc": "Capabilities & Transition",
    "title": "We can only see a short distance ahead, but we can see plenty there that needs to be done.",
    "content": "- Alan Turing . ",
    "url": "http://localhost:4000/githubPage/capabilities",
    "relUrl": "/capabilities"
  },"12": {
    "doc": "Capabilities & Transition",
    "title": "No one is less ready for tomorrow than the person who holds the most rigid beliefs about what tomorrow will contain.",
    "content": "– Wacker, Taylor &amp; Means . Table of Contents . | What capabilities are required to deliver digital products &amp; services and platform services? | Capability Evolution . | Engagement &amp; Demand | Product Lifecycle Management | Fulfillment &amp; Provisioning | Service Operations | Governance &amp; Control | . | How do we transition? | . ",
    "url": "http://localhost:4000/githubPage/capabilities",
    "relUrl": "/capabilities"
  },"13": {
    "doc": "Capabilities & Transition",
    "title": "What capabilities are required to deliver digital products &amp; services and platform services?",
    "content": "A broad range of capabilities are required to ideate, develop, support and control digital products and services throughout their lifecycle. | Engagement &amp; Demand: the capabilities needed to translate business strategy, needs and requests into products and platforms and ensure the right skills are available across the organisation. | Product Lifecycle Management: the capabilities needed to architect, develop and release both application and platform/technology products and services. | Fulfillment &amp; Provisioning: the capabilities required to make products and services easily consumable and rapidly available for use when needed. | Service Operations: the capabilities required to support and operate products and services and ensure they are performant, resilient and meet business and user service levels. | Governance &amp; Control: the capabilities required to support and operate products and services, and ensure they are performant, resilient and meet business and user service levels. | . Many of these capabilities cut across the organisation, but have a ‘centre of gravity’ of where the direction and approach for these is set. | Many of these capabilities will be required across various areas of the organisation and will play a role in developing and supporting digital products in an effective, flexible and agile way. | However, there is typically a ‘centre of gravity’ within the organization that helps set the direction, standards and process for how that should be done across the organisation. | Where this ‘centre of gravity’ resides can vary depending on the maturity of the organisation and the level of autonomy in which parts of the organisation are given. | . | Capabilities | . | | . ",
    "url": "http://localhost:4000/githubPage/capabilities#what-capabilities-are-required-to-deliver-digital-products--services-and-platform-services",
    "relUrl": "/capabilities#what-capabilities-are-required-to-deliver-digital-products--services-and-platform-services"
  },"14": {
    "doc": "Capabilities & Transition",
    "title": "Capability Evolution",
    "content": "The key question to ask is how these capabilities are changing for leading digital, platform and cloud enabled organisations? Some capabilities are reinventing themselves and some new capabilities are found to be necessary for optimization that is happening in the operating model of digital enterprises. Engagement &amp; Demand . | Aspects | Traditional | With Digital, Platforms &amp; Multicloud | Change Implication | . | Demand | Demand is planned, typically with tight controls &amp; constraints | Variable Demand can be readily supported with less constraint, integrated Bus DevOps |   | . | Digital Technology &amp; Services Strategy | Strategy focus is asset &amp; technology-centric. Adoption driven by vendor features on a cyclical basis | Continuous strategy focus is Business Value-Centric (Time to Value, User Experience). Greater focus on Technology adoption timing, experimentation pilots. End to end service execution can be displayed against strategic goals via dashboards with analytics. | Design Thinking applied to Technology Adoptions. Enterprise Architecture re-focus on Bus Architecture &amp; Business Service Components. Sense &amp; respond to Technology maturity changes | . | Demand Management | Focus is typically on IT and project driven demand, IT requirements or major business events. CAPEX as a constraint | Forecast in a different way, focus shifts to business usage behaviors &amp; needs for services. Use automation to detect patterns and capacity need changes. OPEX consumption estimating | Business Partner roles can now focus on strategic demand &amp; value outcomes. Estimate based on Pay-per-usage. Service Catalogs required | . | Product Introduction &amp; Roadmap | Focus is typically on IT and project driven IT requirements, staged releases | Agile based approach, with significant potential for frequent release cycles. But SoR integration mast be managed | New Methods to guide business product owners. New ways of working with IT teams, to plan for change. | . | Data Science and Analytics | Data is managed as a storage cost and constraint. Data not treated as a strategic asset | Data as a Service (DaaS) greatly increases the speed and the capacity to add data services with speed, agility, and with cost impacts transparent. Big Data management disciplines for Cognitive AI and Analytics consumption | Enterprise Architecture teams need to update approaches, patterns and controls to drive DaaS adoption and establish Data platform services | . | Customer Service Assistance | Traditional Helpdesk &amp; support staff, IT help pages | Helpdesk teams need to be able to manage incidents across cloud providers and IT teams. Greater focus on self-support and use of Virtual Assistance. Business Relationship / Account Management disciplines | New procedures, help scripts etc. Virtual Assistance and Robotic Process Automation for high frequency topics | . | Adoption, Communities &amp; Communication | Created in an ad-hoc way usually within organisational silos | Communities and tribes that cut across the organisation focusing on developing core skills, capabilities and collaboration needed to develop digital products and services | Develop clear communities with specialty and focus areas, create a culture of cross organisation working, incentivizing giveback to the organisation | . | Talent &amp; Knowledge Strategy | Traditional model | Skills are evolving rapidly, as cloud platforms and solutions evolve. Heavy reliance on external talent bases | Centre of Excellence approach and Joint Venture &amp; Collabration models to jump start capability change | . Product Lifecycle Management . | Aspects | Traditional | With Digital, Platforms &amp; Multicloud | Change Implication | . | Product Lifecycle Management | Release cycles are staged and somewhat constrained for the business | Cloud Solutions enable multi-speed IT and increased parallel changes |   | . | Architecture and Design | Complex multi-layer architecture planning skills required | Cloud platforms provide a foundation architecture, focus is on re-using existing legacy, and integrating new solutions. Shift to End User experiences | Legacy modernisation skills required. Design Thinking Methods. Ensure regulatory &amp; policy compliance is embedded in the process | . | Product Planning | Long cycle planning, with features needs to be added in elongated release cycles | Clear product ownership driving a priortised backlog that is aligned to changing business needs | More flexible budgeting and planning, with shorter cycle roadmaps dor development | . | Product Engineering &amp; Development | Build and Unit Test monolithic application code | DevOps with Microservices and container based approach. | New development methods &amp; DevOps discipline | . | Configuration &amp; Version Management | Typically a pain point across AD &amp; infrastructure, and rarely well automated | Stronger focus on Config &amp; Version management across environments, enabled through technology. Multi-concurrent versions can be enabled AB Testing | Re-training of staff, tool changes and new working disciplines. | . | Product Testing, Packing &amp; Deployment | Build and Test in dedicated environments – traditional staged approach | DevOps approach: Test with containers and in Production partitions. A/B Testing of features. Service integration experience testing focus (eg: single-sign on). Operations testing for risk | Streamlining the process &amp; automation to avoid quality being a bottleneck. Ensure regulatory &amp; policy compliance is embedded in the process | . | Release &amp; Change Management | Release cycles are staged and somewhat constrained for the business | Business gain potentially radically variable and fast feature deployment cycles. Automation to drive change control compliance - DevOps | Release &amp; deployment management process changes. Controls change. Empowerment of ADM teams - DevOps | . Fulfillment &amp; Provisioning . | Aspects | Traditional | With Digital, Platforms &amp; Multicloud | Change Implication | . | Service Access &amp; Orchestration | Mixed models | Self Servicing, catalog based approach.Single-sign-on globally | Need for single sign-on &amp; catalog capability across multi-clouds. Corp Policy to manage authorised cloud services, security access rights and self-service clip levels. | . | Platform Environment Provisioning | Developers face long wait times to secure access to IT environments | Developers can self-serve IT environments, improving quality | New policies and processes for developers, and for IT environment managers | . | Data Provisioning | In-house online install catalog | Application Store &amp; Continuous updatesContainer based approach - Cloud native applications and DB’s are provisioned in to the cloud environment mirroring dev and test environments reducing errors | Current personal supporting provisioning services will have greatly reduced demand. | . | API &amp; Integration Management | Application &amp; Data Integration requires centralised IT teams, intensive effort &amp; bottlenecks in lifecycle | Open approach to Application &amp; Data Integration, decoupled, fewer bottlenecks | Application Development strategy change. API Lifecycle management processes. | . | Capacity Management | Capacity pre-planned and constrained by In-house or Supplier Technology Stacks | Capacity practices alter - depending on the cloud model (Eg: public vs private, on-premise). Dynamic capacity and self-service needs to be controlled to avoid run-away costs, and minimise wastage | New capacity planning and management practices according to IT deployment model. Active lifecycle approach required to reduce ‘Zombie’ accounts and Inactive capacity | . | Asset &amp; Information Management | Assets and Information controlled centrally or dedicated provider. | Asset &amp; Information sit across Cloud Providers - focus of IT is to regulate, audit &amp; optimise. | Change to policies and processes. Use of technology (APIs) to automate data collection. Cost management disciplines for data archive and retrievals | . | Talent Acquisition, Management and Training | Training Courses &amp; CertificationsPower User support mechanisms | Continuous Just In Time &amp; experiential learning.Agile communities | Garage Days &amp; Hackathons.Center of Excellence modelsSocial collaboration enablers | . Service Operations . | Aspects | Traditional | With Digital, Platforms &amp; Multicloud | Change Implication | . | Service Operations | Focus on prioritising issues and problems, housekeeping and maintenance | Focus on continuous operation, prevention and containment, continuous tech refresh |   | . | Customer Service Level Management | Standardised SLA models, controlled in-house | SLAs vary across cloud providers. Service integration through Operating Level agreements &amp; automation | Policies need to be established, particularly for mission critical business processes, to avoid business performance issues | . | Platform Performance Management | Performance is internally controlled, but also constrained by pre-build technology stacks | Performance shifts to cloud providers and platform integrators, with varying characteristics. Network latency as a typical constraint. | IT need to adapt monitoring across multicloud providers with a focus on latency issues and new technology standards (eg: containers) | . | Availability &amp; Service Reliability Engineering | IT Support staff model – System Administrators and DBAs. IT manages availability to SLAs, with a focus on scripts, maintenance windows and recover | Prevention based practices – Site Reliability Engineers. Automation to continuously detect, prevent and recover. | Site Reliability Engineering skills and IT Self-Healing script automation. | . | Services &amp; API Event Monitoring | IT incidents handled through predictable supplier based models | Strong focus on prevention, root-case analysis and recovery - DevOps Site reliability Engineering, event management, and skills at Helpdesk level. Increased complexity | Processes need to robust for resolving problems &amp; issues in timely manner. Strong root cause skills | . | Incident &amp; Problem Management | ITIL based support approach | Emphasis on early detection and prevention, auto-recovery | Embed Site Reliability Engineering. Auto-recovery, Event Analytics. Integrate with legacy operations | . | Continuous Improvement | Done through lessons learnt at the end of major programmes or initiatives | Ingrained as part of the fabric of working, with a culture that seeks out improvement. SRE capability constantly seeking to automate and reduce TOIL | Clear metrics to measure improvements, culture and incentives to delver improvements | . | Security &amp; Business Continuity | Security enablers to prevent vulnerability (eg; virus scan). Manual fall backs for continuity | Continuous threat posture, with security operating across boundaries, detecting trends and weakness ahead of major events. Dynamic continuity changes &amp; Robust fallback | Automation &amp; AI to pre-empt the risk. Platform standards to minimize variation vulnerability | . Governance &amp; Control . | Aspects | Traditional | With Digital, Platforms &amp; Multicloud | Change Implication | . | Policies &amp; Standards | IT Policies are Asset and technology Centric, for internal IT teams. Policies and standards are relatively static | Cloud IT Policies &amp; standards are utilized by the Business, suppliers and external users, enforced by central governing teams. Standards are platform and As-A-Service centric. Standards continuously evolve with new Technology | Policies &amp; Standards to enable greater Self-Service, by external parties (Eg: APIs), and for platform standardization. Focus on continuous updates | . | Financial Planning &amp; Cloud Economics | Fixed period planning cycles (annual, per project), with significant investments for asset refresh | The IT service framework now prices IT Services incrementally from the IT vendors to get accurate Operational Expense forecasting. Consumption driven planning with Business Product Owners | IT budget planning processes change. Project estimating &amp; planning process changes. Budget allocation &amp; chargeback models change, aligning to Business Product Owners - need to be handled carefully. Handling business case viability challenges | . | Partnering &amp; Sourcing | IT supplier based models. Value assessed on asset basis. Long procurement cycles | “Vendors” are replaced with partners who are tied in seamlessly to the essential delivery of IT services. Value assessed on business outcomes. Rapid proof of value procurement cycles | Procurement and sourcing business rules need to be modified to reflect the new opex nature of cloud operations. | . | Portfolio Management | PO is driving mixed model of CAPEX programmes &amp; project releases. Traditional PPM | Project staffs will plan projects incrementally and measure value for incremental progress. ROI metrics will be easier to acquire and justify. | Training of PMO staff’s in agile management techniques is required. Governance changes required | . | Information &amp; Security control | Internal controls are managed against assets and | Security polices and controls are proven for each cloud service fielded. For industries with regulatory concerns, security works ahead of ahead of new imitative to show the impact of cloud tools and procedures. | Security Organizations need extensive education/exposure to how cloud operations should be tailored for security polices and control, | . | Risk, Audit and Compliance | Control is managed centrally, and controls are typically transparent | Cloud Service Provider contracts assure compliance. Understanding the compliance and regulations for using public cloud providers requires clients working with their providers to validate their deployment patterns. | Proactively work with compliance authorities to implement new policies and control’s - but may stretch the talent and experience of incumbent personnel, such as for GDPR issues. | . ",
    "url": "http://localhost:4000/githubPage/capabilities#capability-evolution",
    "relUrl": "/capabilities#capability-evolution"
  },"15": {
    "doc": "Capabilities & Transition",
    "title": "How do we transition?",
    "content": "Assessing the maturity of these capabilities helps determine the areas for focus of operating model change and sets a baseline for measuring improvement. Undertaking a maturity assessment across these capabilities provides a number of benefits to defining the future operating model transformation . | Bring visibility to current practices adoption | Identifying and prioritise gaps in existing capabilities | Expose pain points | Provide a future view of ambitions and goals with to help focus capability transformation | Structure the transformation priorities | Target quick wins and early proof points | . Additionally, this maturity assessment case act as a ‘yard stick’ to measure progress of the operating model transformation, alongside other KPIs that help support the measurement of each capability. | TOM Capability Maturity | . | | . ",
    "url": "http://localhost:4000/githubPage/capabilities#how-do-we-transition",
    "relUrl": "/capabilities#how-do-we-transition"
  },"16": {
    "doc": "Capabilities & Transition",
    "title": "Capabilities & Transition",
    "content": " ",
    "url": "http://localhost:4000/githubPage/capabilities",
    "relUrl": "/capabilities"
  },"17": {
    "doc": "Chaos Engineering",
    "title": "Chaos Engineering",
    "content": ". Table of Contents . | Overview | Background: The Challenge | Chaos Engineering &amp; Application Development | Evolution | IBM Approach . | Strengthen Reliability Disciplines | Understand the System | Experiment on Every Component | Strive for Production | Contain the Impact | Measure, Learn, Improve | Increase Complexity Gradually | Socialize Continuously | . | IBM’s Chaos Engineering Method . | 1. Understand the System End-to-End | 2. Gain Organizational Agreement | 3. Create a hypothesis and plan related experiments | 4. Enable observability | 5. Prepare your experiment | 6. Run chaos experiments | 7. Analyze the results to prove or disprove the hypothesis | 8. Communicate findings and improve | 9. Grow blast radius and repeat | 10. Evolve and expand to production | . | IBM Publications | . ",
    "url": "http://localhost:4000/githubPage/chaos-engineering",
    "relUrl": "/chaos-engineering"
  },"18": {
    "doc": "Chaos Engineering",
    "title": "Overview",
    "content": "Chaos Engineering is an evolution to business continuity and service resiliency planning. It is the discipline of experimenting on a service to build confidence in its ability to withstand failure in production. Chaos Engineering aims to introduce failures within a system to identify and fix failures proactively before they impact business operations. | Chaos Engineering | . | | . ",
    "url": "http://localhost:4000/githubPage/chaos-engineering#overview",
    "relUrl": "/chaos-engineering#overview"
  },"19": {
    "doc": "Chaos Engineering",
    "title": "Background: The Challenge",
    "content": "Like all things, an IT system lives as part of a bigger ecosystem and other elements of the ecosystem have a bearing on the health and operation of the system. While all enterprise want their systems to be robust and design for systems to be available in general, hardly enough effort is spent to make their systems resilient to such faults in the environment - the infrastructure, the connectivity, other systems or the application itself. While some of these faults can be thought of and considered during designing the application, . | Not all can be thought through. In some cases, doing so may offset of the value of building the system itself. | Manifestations of such faults vary as well as these fault themselves respond differently to the environment they exist in. | . Considering external faults as random as they can be as chaos, engineering your system to respond to &amp; survive such faults is Chaos Engineering. It is a set of practices largely focused on hypothesizing and experimenting to verify the hypotheis and using this cycle as a tool to uncover and address faults in design, deployment, tooling and operation. | Scientific Method | . | | . ",
    "url": "http://localhost:4000/githubPage/chaos-engineering#background-the-challenge",
    "relUrl": "/chaos-engineering#background-the-challenge"
  },"20": {
    "doc": "Chaos Engineering",
    "title": "Chaos Engineering &amp; Application Development",
    "content": "Chaos engineering practices are important in context of application development in a number of ways and help development teams build capabilities in the application to tackle the following kind of failures by providing fault injection methods across a range of application development activities: . | Infrastructure &amp; Network Failures - Cloud native applications run on a set of resources typically managed by external service providers. An outage to any of the underlying resource can take down the application. This is also true for network failure events. These outages can range from a storage device failure, a corrupt instance to outage of an availability zone. Ability to sustain such an event is essential. While some of these scenarios can be thought of, reproducing such events is next to impossible requiring massive coordination to create mostly unseen situations. By providing capability to reproduce such events, chaos engineering tools provide a way to build for and test for such outages. | Application Failures - Application Development is done at a much faster pace with agile practices trading off deep requirement and impact analysis in the processes. In such an environment, a testing method that can create scenarios beyond the specified acceptance criteria and requirements can add great value to the resiliency of the product. | External Application Failures - Integration is the core of modern applications and being resilient to faults in another systems an app is integrated with becomes utmost important. Faults that can be in form of errors or reliability or performance of the other system and can occur at any point for any duration. Ability to induce such faults into an app under development can greatly help developers identify and cater for such faults. | . Apart from capabilities to reproduce and address failure scenarios, Chaos engineering tools can also help manage a running application. Using Chaos Engineering practices, an application team can effectively manage the utilization, security and performance of an application. ",
    "url": "http://localhost:4000/githubPage/chaos-engineering#chaos-engineering--application-development",
    "relUrl": "/chaos-engineering#chaos-engineering--application-development"
  },"21": {
    "doc": "Chaos Engineering",
    "title": "Evolution",
    "content": "Point of views and guiding principles for Chaos Engineering have started to take shape, e.g. Principles of Chaos. ",
    "url": "http://localhost:4000/githubPage/chaos-engineering#evolution",
    "relUrl": "/chaos-engineering#evolution"
  },"22": {
    "doc": "Chaos Engineering",
    "title": "IBM Approach",
    "content": "Chaos Engineering is the discipline of experimenting on a software system - often in production - in order to build confidence in the system’s capability to withstand turbulent and unexpected conditions. Although the name suggests chaos, the experiments are well designed and planned: at the beginning, failure points are identified and based on this a hypothesis is formulated. A corresponding experiment is built (node failure, packet drops, slow links) and then run against the system. The result of the experiment is measured and compared against the hypothesis to either proof or disproof it. These experiments are typically performed in so-called “game days”, to test the resilience of an application in a controlled environment. However, experiments may also be performed in production environments to assure the robustness of the application. The goal of testing in production is to be able to detect problems that cannot be surfaced in pre-production testing. IBM has identified the following Principles for Chaos Engineering. Each of these 8 principles is support by a concise statement describing the What, Why, and How of that principle. | Chaos Engineering Principles | . | | . Strengthen Reliability Disciplines . Chaos Engineering builds on established reliability engineering practices. Organizations should leverage mature business continuity processes and system reliability patterns. Through the discipline of identifying and mitigating reliability issues, the organization becomes more resilient. Understand the System . As a precursor to planning a viable Chaos Engineering experiment, the system and its business outcomes should be well understood. A baseline should be formed by understanding the system as a whole, its emergent properties and functions, as well as its topology, architecture, dependencies, steady-state behavior, output response, and characteristics such as availability, latency and throughput. Experiment on Every Component . Treat all components, layers, services, and dependencies as subjects for potential experiments whenever possible. These experiments should uncover any gaps between expected and actual behavior and should validate fallbacks. Strive for Production . Experiment first upstream in non-production environments. Build confidence by learning about the system behavior during different scenarios and ensuring the system’s ability to gracefully withstand and recover from failures and unexpected events. As confidence is gained, transition towards production. Contain the Impact . Avoid cascading failures by containing tests scope and detaching dependencies. Different containment strategies should be applied to help limit the scope, minimize impact on business, and confirm their effectiveness. Measure, Learn, Improve . Infuse observability into all system components. Measure experiment results to understand the business impact. Learn from the collected insights to improve system reliability and guide future experiments. Increase Complexity Gradually . Increase complexity gradually while adjusting the granularity of the experiments. Expanding scope and combining tests may reveal previously unknown weaknesses. Socialize Continuously . Effective and transparent communication is critical to the success of any Chaos Engineering program. This creates a culture that recognises the benefits of introducing rigorously controlled risks to drive increased resilience. IBM uses a range of tools and platform capabilities around chaos engineering thus, enabling development teams to weed out such issues and therefore, build resilient products . | Istio (the core of OpenShift Service Mesh) provides capabilities to induce HTTP errors, response delays for applications deployed in container applications. Refer this article. | Netflix’s The Simian Army is a set of tools (called monkeys) to induce various kind of failures in a cloud environment e.g bringing down a virtual machine. | . ",
    "url": "http://localhost:4000/githubPage/chaos-engineering#ibm-approach",
    "relUrl": "/chaos-engineering#ibm-approach"
  },"23": {
    "doc": "Chaos Engineering",
    "title": "IBM’s Chaos Engineering Method",
    "content": "IBM’s methodology for Chaos Engineering is an iterative journey that describes the planning, execution, scaling and learning of Chaos Experiments towards a robust and resilient service as well as organization. For each of the 10 steps, a description of the key activities is provided. This leads to a comprehensive methodology of performing Chaos Engineering in the enterprise. | Methodology | . | | . Here is the details of the phases of this method: . 1. Understand the System End-to-End . A successful Chaos Engineering experiment requires a clear understanding of the system end-to-end: its weaknesses, its failure events and even any skepticism towards the reliability of that system. This will help determine the answer to the question “what are we testing?”. 2. Gain Organizational Agreement . Chaos Engineering helps organizations make systems more resilient and reliable. This intent of enhancing resiliency through continuous failure testing may not be an easy sell, as it intentionally introduces minor short-term risks, to avoid major long-term risks by improving overall reliability. 3. Create a hypothesis and plan related experiments . Simply put, a hypothesis is a collection of ‘what ifs’. It is a prediction about the expected system behavior upon introduction of a certain system change or changes. One or more chaos experiments can be used to either prove or disprove the hypothesis. 4. Enable observability . In order to understand the impact of the experiments, it’s important to have objective measurements that can illustrate the health and behavior of the system. When it takes the manual work of experts to make subjective evaluations of system health, their time required represents an ongoing tax on each experiment. Monitoring should be holistic, it shouldn’t only cover black box monitoring such as disk space, CPU usage etc., but it should also cover white box monitoring such as queries per seconds, transactions per seconds etc. 5. Prepare your experiment . A Chaos Engineering experiment requires thoughtful preparation and planning to minimize service disruption. A risk management analysis should be done by factoring in the current availability metrics for the system against what is expected, the impact the test can bring to end user experience and most importantly, data integrity. 6. Run chaos experiments . A successful chaos experiment follows a structured approach that ensures standardized communication and data collection. Different methods can be applied to mitigate risks and help improve the chances of a positive outcome. Experiments should proceed within the framework of a predefined and communicated schedule so that stakeholders know what to expect. Frequent and thorough communication is critical to avoid unnecessary pitfalls during the execution of an experiment. Chaos experiments can be run during scheduled ‘GameDays’ and eventually as part of automated continuous testing. 7. Analyze the results to prove or disprove the hypothesis . Chaos Engineering experiments will hopefully produce meaningful and relevant results. The quality of the generated data is dependent on how comprehensively observability tools have been configured. With the results in hand, we can analyze the outcome, compare it to what was predicted and then make educated decisions on how to proceed. 8. Communicate findings and improve . Chaos Engineering experiments may uncover system defects and weaknesses that must be addressed. After the results are reviewed and analyzed, the findings need to be clearly communicated and converted into actionable plans with appropriate ownership. 9. Grow blast radius and repeat . Chaos Engineering is a practice of continuous experimenting and learning. Before expanding scope and growing complexity, a consistent baseline needs to be achieved (have confidence in a consistent baseline). Experiments should start small to allow the organization to mature (gain confidence) and gradually increase in scope and complexity. This means experiments should iteratively target larger blast radiuses and widen their scope to eventually touch the system end to end. 10. Evolve and expand to production . It is important to demonstrate safety in a non-production environment before running an experiment in production. Remember that non-production and production environments may yield different experiment outcomes and insights. Moreover, production data and traffic may not be available outside of production. It is crucial to start your production chaos testing with ‘GameDays’. As capabilities mature and consistent results are achieved, chaos experiments can then be run periodically via unattended automation and during deployments. ",
    "url": "http://localhost:4000/githubPage/chaos-engineering#ibms-chaos-engineering-method",
    "relUrl": "/chaos-engineering#ibms-chaos-engineering-method"
  },"24": {
    "doc": "Chaos Engineering",
    "title": "IBM Publications",
    "content": ". | IBM’s principles of chaos engineering - Learn to improve the reliability and availability of your systems by following IBM’s principles and 10 steps for chaos engineering | Use chaos engineering to assess application reliability | Run chaos engineering experiments by using Gremlin on IBM Cloud | Get started with chaos engineering by using Gremlin on IBM Cloud | Chaos Engineering vs Voodoo | Overcoming chaos on the way to the moon | . ",
    "url": "http://localhost:4000/githubPage/chaos-engineering#ibm-publications",
    "relUrl": "/chaos-engineering#ibm-publications"
  },"25": {
    "doc": "Client Stories",
    "title": "What our clients are doing?",
    "content": ". Cross-cutting client stories which are examples for new age application development models: . | BNP Paribas Story of Operation Model transformation | . ",
    "url": "http://localhost:4000/githubPage/client-stories#what-our-clients-are-doing",
    "relUrl": "/client-stories#what-our-clients-are-doing"
  },"26": {
    "doc": "Client Stories",
    "title": "Client Stories",
    "content": " ",
    "url": "http://localhost:4000/githubPage/client-stories",
    "relUrl": "/client-stories"
  },"27": {
    "doc": "Cloud Agnostic",
    "title": "Cloud Agnostic",
    "content": " ",
    "url": "http://localhost:4000/githubPage/cloud-agnostic#cloud-agnostic",
    "relUrl": "/cloud-agnostic#cloud-agnostic"
  },"28": {
    "doc": "Cloud Agnostic",
    "title": "Chapter Layout",
    "content": ". | Objective | Key Questions to Ask | IBM’s Approach | Sighed Benefits  | Risks | Client Stories | . ",
    "url": "http://localhost:4000/githubPage/cloud-agnostic#chapter-layout",
    "relUrl": "/cloud-agnostic#chapter-layout"
  },"29": {
    "doc": "Cloud Agnostic",
    "title": "Cloud Agnostic",
    "content": " ",
    "url": "http://localhost:4000/githubPage/cloud-agnostic",
    "relUrl": "/cloud-agnostic"
  },"30": {
    "doc": "Code Quality Management",
    "title": "Code Quality Management",
    "content": "Contents . | Challenges, Patterns for these, Evolution we see and anticipate, IBM approach and capability | . ",
    "url": "http://localhost:4000/githubPage/codequality#code-quality-management",
    "relUrl": "/codequality#code-quality-management"
  },"31": {
    "doc": "Code Quality Management",
    "title": "Code Quality Management",
    "content": " ",
    "url": "http://localhost:4000/githubPage/codequality",
    "relUrl": "/codequality"
  },"32": {
    "doc": "Co-existence Scenarios",
    "title": "Co-existence Scenarios",
    "content": "Modernization of monolithic application to microservice based application needs monolithic &amp; modern application to co-exist until the modernization is complete. To facilitate this co-existence, a co-existence integration layer needs to be built using various integration patterns across on-premise and cloud. Thus, the co-existence layer is a by-product of incremental modernization to deliver modernized functionality into production in controlled risk adverse manner. Co-existence supports multiple patterns for the incremental modernization in both the scenarios listed below: . ",
    "url": "http://localhost:4000/githubPage/coexistence-scenarios",
    "relUrl": "/coexistence-scenarios"
  },"33": {
    "doc": "Co-existence Scenarios",
    "title": "Scenario 1: Current state reads from and writes to modernized data",
    "content": "| Current state reads from and writes to modernized data | . | | . | Current state datasets are decommissioned; and the data is completely moved to the modernized state with its access is guarded by the domain/data services designed around that dataset. The current state reads and updates this data residing in the target state. | This pattern can be used when modernized data to current state data transformation complexity is minimal and performance requirements are relaxed, thereby allowing real time transformation for reading &amp; writing to modernized data from the current state. | Examples : . | A status code, indicator, or flag value is needed on the current state that was transformed into a new value set when loaded to the modernized data model. | A complex batch job, in part or is not modernized along with a given modernization roll out, but a status code or aggregated value needs to be transformed and updated on the modernized database since the current state equivalent table was decommissioned. | . | . ",
    "url": "http://localhost:4000/githubPage/coexistence-scenarios#scenario-1-current-state-reads-from-and-writes-to-modernized-data",
    "relUrl": "/coexistence-scenarios#scenario-1-current-state-reads-from-and-writes-to-modernized-data"
  },"34": {
    "doc": "Co-existence Scenarios",
    "title": "Scenario 2: Modernized state reads and writes to current state data",
    "content": "| Modernized state reads and writes to current state data | . | | . | Modernized data needs a subset of data from the current state (e.g. master data). A functionality is moved incrementally and the modernized data needs to update a subset of data from the current state. | This pattern supports incremental modernization roll out where modernized data and current state data can be brought together in context within the modernized application. | Examples : . | The modernized solution needs customer data reflected in the modernized user interface that has not yet been added to the modernized data model (e.g. customer name). | Critical business owned applications need replica of current state key data that cannot be changed in the timeframe planned for a given modernization roll out. | . | . ",
    "url": "http://localhost:4000/githubPage/coexistence-scenarios#scenario-2-modernized-state-reads-and-writes-to-current-state-data",
    "relUrl": "/coexistence-scenarios#scenario-2-modernized-state-reads-and-writes-to-current-state-data"
  },"35": {
    "doc": "Contact Us",
    "title": "Contact Us",
    "content": ". | Name | Email | Role | . | Thomas Crane | tom.crane@us.ibm.com | Senior Partner - HCT Global Build Offering Leader | . | Swanand Barve | swabarve@in.ibm.com | IBM Distinguished Engineer, Cloud Application Development &amp; Modernization | . | Jermaine Edwards | jedward2@us.ibm.com | IBM Distinguished Engineer, HCT | . | Tuan M. Nguyen | tuannm@sg.ibm.com | Program Manager, HCT Move &amp; Build Offerings | . ",
    "url": "http://localhost:4000/githubPage/contact",
    "relUrl": "/contact"
  },"36": {
    "doc": "Cosourcing",
    "title": "Cosourcing",
    "content": "Chapter Layout . | Objective | Key Questions to Ask | IBM’s Approach | Sighed Benefits  | Risks | Client Stories | . ",
    "url": "http://localhost:4000/githubPage/cosourcing#cosourcing",
    "relUrl": "/cosourcing#cosourcing"
  },"37": {
    "doc": "Cosourcing",
    "title": "Cosourcing",
    "content": " ",
    "url": "http://localhost:4000/githubPage/cosourcing",
    "relUrl": "/cosourcing"
  },"38": {
    "doc": "Financial Models",
    "title": "Financial Models",
    "content": ". Table of Contents . | Overview | Cloud Economic Considerations | Value Levers | . ",
    "url": "http://localhost:4000/githubPage/costing",
    "relUrl": "/costing"
  },"39": {
    "doc": "Financial Models",
    "title": "Overview",
    "content": "Financial models for IT are evolving too, in line with the evolving digital enterprise and how they operate. Here are some of the key attributes of a suitable financial model to enable the enterprise to function effectively as an innovative organization and with agility: . | Asset light - Shifting from CAPEX to OPEX and dynamic accounting models | IT Spend as a % of Customer Growth &amp; Revenue Potential, ARPU | Consumption driven IT economics | Benefit &amp; Value driven, not cost constraint decisions | Committed investments to transform, start-up seed funding to innovate | Optimise to Re-Invest | . ",
    "url": "http://localhost:4000/githubPage/costing#overview",
    "relUrl": "/costing#overview"
  },"40": {
    "doc": "Financial Models",
    "title": "Cloud Economic Considerations",
    "content": "An enterprise can generate value for itself from cloud by a combination of optimizing its IT infrastructure &amp; operational cost and innovating its way to new revenue that it could not realize earlier. | Cloud Economics | . | | . | Efficiently Utilize Cloud Resources . | Proactively identify and eliminate unused resources | Identify and optimize idle resources | Right size resources for efficient cost and performance | . | Operational Optimization of Cloud Resources . | Leverage dynamic auto scaling for cost optimized performance | Identify opportunities for movement to lower cost instances/families | Optimize and reduce operational costs of data resources | Balance performance and costs of cloud resources | . | Financial Governance Mechanisms for Cost Management . | Structural governance: Evaluate cost implications during design | Role governance: Determine the access, provisioning and budget authority for defined cloud roles | User governance: Create “menus” of available patterns for each role with cloud provisioning authority | Quality governance: Audit effectiveness and adherence to governance across all three levels | . | Pricing, Reserving and Discounting of Cloud Resources . | Evaluate bulk pricing options for potential discounts prior to provisioning | Monitor use of cloud resources against purchased discounts | Utilize automated recommendations and alerting to identify opportunities for bulk purchases based on usage trends | . | . ",
    "url": "http://localhost:4000/githubPage/costing#cloud-economic-considerations",
    "relUrl": "/costing#cloud-economic-considerations"
  },"41": {
    "doc": "Financial Models",
    "title": "Value Levers",
    "content": "Different levels of value can be realized through layers of transformation: . | Costing Layers | . | | . Value is driven through an exhaustive set of levers across 5 sources and needs to be supported by a business case. These are the 5 sources: . | Business acceleration . | Get 3.5X more benefit than public only | New insights and better client experiences | Faster time to market for all your apps | Innovation in a secure, consistent way | . | Application modernization . | Speed app release from months to weeks | Modernize 66% more apps | Consistent skills and agile dev ops | Automation and less rework. | . | Infrastructure cost efficiency . | Cut infrastructure costs by 4x with less maintenance | 95% incident reduction/higher resiliency | Greater utilization | 10% infra cost savings | . | Regulatory and risk . | Reduce compliance | Spend by ~25% | Gain a single pane of control | Consistent security/compliance policies | Automation on a consistent stack | . | Strategic optionality . | Realize a more agile, flexible architecture | Avoid vendor lock in | Match workloads to the right model | Optimize cost by moving workloads | . | . ",
    "url": "http://localhost:4000/githubPage/costing#value-levers",
    "relUrl": "/costing#value-levers"
  },"42": {
    "doc": "Change Data Capture",
    "title": "Change Data Capture",
    "content": ". | Change Data Capture | . | | . | CDC is an approach to data integration that is based on the identification, capture and delivery of the changes made to enterprise data sources. This pattern is used to replicate legacy application’s state to modernized application(s). | CDC can be used when changes to core programs at the source state are not feasible or yet to be planned. | It is a set of software design patterns used to determine and track the data that has changed so that action can be taken using the changed data. | CDC could be implemented in both unidirectional and bidirectional data synchronization. | CDC occurs often in datawarehouse environments since capturing and preserving the state of data across time is one of the core functions of a data warehouse, but CDC can be utilized in any database or data repository system. | CDC can deliver the change logs to target databases, message queues, or an extract, transform, load (ETL) solution. | CDC is mostly supported by out-of-the-box technical components without any changes to core programs. | . ",
    "url": "http://localhost:4000/githubPage/data-patterns-cdc",
    "relUrl": "/data-patterns-cdc"
  },"43": {
    "doc": "Data Virtualization",
    "title": "Data Virtualization",
    "content": "| Data Virtualization | . | | . | One of the primary problems that clients face is how to provide a “unified view of data” across regions. | Most of the companies have grown as a result of acquisitions of regional companies that in many cases operate independently and maintain their local IT infrastructure. There are data model mismatches, different levels of granularity when representing the same entity, and different interfaces and protocols to access the same kind of information, making difficult to achieve a unified view of data. | To solve these issues, data virtualization provides an abstraction layer that keeps a unified and common representation of the business entities, in the form of a Canonical Business Model that can be consumed by all the enterprise applications regardless of the underlying data schemas in every location. | It helps customers capture data from multiple data sources and allows them to be treated as a single source, delivering the right data in the required form and at the right time to any application and/or user, in real time. | The benefits of data virtualization for companies include quickly combining different sources of data, improving productivity, accelerating time value, eliminating latency, maintaining data warehouse, and reducing the need for multiple copies of data as well as less hardware. | . ",
    "url": "http://localhost:4000/githubPage/data-patterns-data-virtualization",
    "relUrl": "/data-patterns-data-virtualization"
  },"44": {
    "doc": "DataOps",
    "title": "DataOps &amp; Data Pipelines",
    "content": ". Table of Contents . | Data Ops . | Drivers for DataOps | . | Data Pipelines | DevOps vs DataOps | DataOps-as-a-Service | DataOps Ecosystem | . ",
    "url": "http://localhost:4000/githubPage/dataops#dataops--data-pipelines",
    "relUrl": "/dataops#dataops--data-pipelines"
  },"45": {
    "doc": "DataOps",
    "title": "Data Ops",
    "content": "| DataOps | . | | . Data Ops lies at the conjunction of data engineering, data science and data analytics. Drivers for DataOps . Problems . | Data Scientists spend a lot of time working on the data for various phases such as data filtration, cleansing, deduplication, profiling and transformation | As part of developing analytical models, the data scientists are required to test the models against datasets. With each iteration, the data needs to be refreshed and the models are required to be retrained. The metadata also changes and this requires to be monitored , giving rise to a need for KPIs for data | . Drivers . | Data must be delivered at the speed of analytics to meet the demand | Accelerate Data preparation and Data Enrichment for Data Scientists | Deliver DataOps as a Service for Data Platforms and Solutions – as an extension of DevOps &amp; DevSecOps | Data Scientists should be able to consume data quickly with up to date datasets which are consistent throughout the life cycle of data without compromising on integrity | . ",
    "url": "http://localhost:4000/githubPage/dataops#data-ops",
    "relUrl": "/dataops#data-ops"
  },"46": {
    "doc": "DataOps",
    "title": "Data Pipelines",
    "content": "Data has a workflow in a Data Platform between the point of source through the various stages of transformation to the point of consumption. | Data Pipeline | . | | . | This includes Ingestion, Integration, Storage, Processing, Testing, Serving, Machine Learning &amp; analytics. | . | . ",
    "url": "http://localhost:4000/githubPage/dataops#data-pipelines",
    "relUrl": "/dataops#data-pipelines"
  },"47": {
    "doc": "DataOps",
    "title": "DevOps vs DataOps",
    "content": "| DevOps | DataOps | . | Development + Operations | Data + Operations | . | Develop Software | Deliver Data | . | Application Developers | Data Scientists &amp; Data Analysts | . | Application Design | Data Engineering | . | Integration of Developers, Testers, Operators and Users | Integration of Data practitioners | . | Application Development Life Cycle | Data Life Cycle Management | . ",
    "url": "http://localhost:4000/githubPage/dataops#devops-vs-dataops",
    "relUrl": "/dataops#devops-vs-dataops"
  },"48": {
    "doc": "DataOps",
    "title": "DataOps-as-a-Service",
    "content": "| DataOps-as-a-Service | . | | . ",
    "url": "http://localhost:4000/githubPage/dataops#dataops-as-a-service",
    "relUrl": "/dataops#dataops-as-a-service"
  },"49": {
    "doc": "DataOps",
    "title": "DataOps Ecosystem",
    "content": "| DataOps Ecosystem | . | | . ",
    "url": "http://localhost:4000/githubPage/dataops#dataops-ecosystem",
    "relUrl": "/dataops#dataops-ecosystem"
  },"50": {
    "doc": "DataOps",
    "title": "DataOps",
    "content": " ",
    "url": "http://localhost:4000/githubPage/dataops",
    "relUrl": "/dataops"
  },"51": {
    "doc": "New Ways of Working",
    "title": "Innovation &amp; Agility in Delivery Approach",
    "content": ". Table of Contents . | New Ways of Working | . ",
    "url": "http://localhost:4000/githubPage/delivery-approach#innovation--agility-in-delivery-approach",
    "relUrl": "/delivery-approach#innovation--agility-in-delivery-approach"
  },"52": {
    "doc": "New Ways of Working",
    "title": "New Ways of Working",
    "content": "To develop the future IT operating model, we recommend taking an agile, user centric approach. Take a persona lead approach, define user journeys to develop an operating model that is built for the people who will demand, consume and operate services. Elaborate and build out each operating model dimension in an agile and iterative way, informed by the user centric journeys and feedback on previous releases of the operating model. | Delivery Approach | . | | . Operating model dimensions are elaborated in an iterative way and based around key personas, the operating model is elaborated around those who will demand, consume and operate services. | Iterative Drivers | . | | . ",
    "url": "http://localhost:4000/githubPage/delivery-approach",
    "relUrl": "/delivery-approach"
  },"53": {
    "doc": "Design Thinking",
    "title": "Design Thinking / Hypothesis MVP",
    "content": "Chapter Layout . | Objective | Key Questions to Ask | IBM’s Approach | Sighed Benefits  | Risks | Client Stories | . ",
    "url": "http://localhost:4000/githubPage/design-thinking#design-thinking--hypothesis-mvp",
    "relUrl": "/design-thinking#design-thinking--hypothesis-mvp"
  },"54": {
    "doc": "Design Thinking",
    "title": "Design Thinking",
    "content": " ",
    "url": "http://localhost:4000/githubPage/design-thinking",
    "relUrl": "/design-thinking"
  },"55": {
    "doc": "Digital Integration",
    "title": "Digital Integration",
    "content": ". | Overview | Key Questions to Ask | IBM’s Approach . | Main Constituents of Digital Integration . | Microservices Fabric | . | DevOps | . | References | . ",
    "url": "http://localhost:4000/githubPage/hybrid-digital-integration",
    "relUrl": "/hybrid-digital-integration"
  },"56": {
    "doc": "Digital Integration",
    "title": "Overview",
    "content": "This is a transformation journey that requires business transformation, building new architectural layers, adopting new architectural patterns, modernizing &amp; moving existing workloads, exposing existing processes/services as APIs: . | Engage customers anytime anywhere | Paradigm shift from business processes to customer journeys | Offer a seamless customer experience beyond the original line of businesses | Become agile to respond to market and regulatory changes | Keeping innovation at the forefront - not just in business but also in technology (bringing new ideas to market rapidly) | Extreme automation to reduce costs | . Following image gives an overview of what it takes to become digital: . | Digital Mandates | . | | . From an architecture &amp; technology perspective this translates to: . | Building new architectural layers and patterns (Microservices, API, EDA, AI/insights) | Modernization to remove impediments to rapid business transformation and innovation | Adopting new technologies and development practices | Reducing technical debt | Exposing processes and data to partner and developer ecosystem to accelerate innovation (in a secure way ofcourse) | Cloud migration/development | Ensuring IT systems are resilient and available | Automation of software development processes | . What it means is a holistic approach is required for Digital transformation. i.e.: . | Running siloed initiatives (e.g. enhancing systems of engagement, modernizing applications) is not enough | Just developing systems of engagement will not realize the objectives of digital transformation | There is a need for well-defined delivery practices, architecture and standards | Services &amp; API are the backbone of any digital strategy. 80% of the effort is in Integration to backend systems and Process Digitisation | . ",
    "url": "http://localhost:4000/githubPage/hybrid-digital-integration#overview",
    "relUrl": "/hybrid-digital-integration#overview"
  },"57": {
    "doc": "Digital Integration",
    "title": "Key Questions to Ask",
    "content": "It is essential to understand client’s digital strategy and their progress before proposing changes or revamping their strategy. Key questions to ask here are: . | What is your modernization strategy? | What is your cloud strategy? | What is your API strategy? | Are you modernizing your applications using Microservices architectural style? | How are you handling co-existence? | How are you doing DevOps? | What are you doing in terms of business process transformation? How are you automating them? | What is your integration strategy? | . ",
    "url": "http://localhost:4000/githubPage/hybrid-digital-integration#key-questions-to-ask",
    "relUrl": "/hybrid-digital-integration#key-questions-to-ask"
  },"58": {
    "doc": "Digital Integration",
    "title": "IBM’s Approach",
    "content": "The core of any digital transformation program is the digital integration layer as depicted in the diagram below . | Digital Integration Layer | . | | . Following are the key capabilities that the digital integration layer should possess: . | Digital Integration Layer Capabilities | . | | . Main Constituents of Digital Integration . There are 4 main constituents of digital integration: . | Business processes/Customer journeys | Microservices fabric | API | Analytics (real-time and batch) | . Microservices Fabric . Following is a conceptual view of the Microservices platform: . | Microservices Platform - Conceptual View | . | | . Microservices need to provide multiple ways of invocation. Primarily: . | REST endpoints - these endpoints are then exposed as API endpoints on the API Gateway (or used to create APIs) | EDA or Stream data processing endpoints - microservices can be chained together to develop stream data processing logic | . Following picture depicts a typical Microservice development blueprint with REST : . | Microservices Development Blueprint | . | | . Following picture depicts a typical way in which microservices are chained together to build stream data processing logic . | Streaming Data Processing | . | | . DevOps . Following picture depicts a typical CI/CD pipeline . | DevOps - CI/CD | . | | . ",
    "url": "http://localhost:4000/githubPage/hybrid-digital-integration#ibms-approach",
    "relUrl": "/hybrid-digital-integration#ibms-approach"
  },"59": {
    "doc": "Digital Integration",
    "title": "References",
    "content": ". | IBM Institute of Business Value white paper on Digital Transformation | IBM’s Smarter Business Microsite | . ",
    "url": "http://localhost:4000/githubPage/hybrid-digital-integration#references",
    "relUrl": "/hybrid-digital-integration#references"
  },"60": {
    "doc": "Dynamic Delivery",
    "title": "Dynamic Delivery",
    "content": ". Table of Contents . | Leveraging the Power of Co-Creation | Ceremonies &amp; Collaboration | Challenges of Distributed Agile | . ",
    "url": "http://localhost:4000/githubPage/dynamic-delivery",
    "relUrl": "/dynamic-delivery"
  },"61": {
    "doc": "Dynamic Delivery",
    "title": "Leveraging the Power of Co-Creation",
    "content": "Globally Distributed Flexible Squad Composition . A program/account can have various flavors of these squad compositions, based on the workload &amp; requirement Some programs can start onsite heavy, and may move towards more offshore depending on the workload shifts and productivity. Co-located Squad . | Workloads that can be split into independent deployment units | Co-Creation - MVP/PoC using layered delivery | . | Squad Model | . | | . Vertically Integrated Distributed Squad . | Workloads that have huge dependency between modules | Co-Creation with Clients and squads across multiple locations | . | Squad Model | . | | . ",
    "url": "http://localhost:4000/githubPage/dynamic-delivery#leveraging-the-power-of-co-creation",
    "relUrl": "/dynamic-delivery#leveraging-the-power-of-co-creation"
  },"62": {
    "doc": "Dynamic Delivery",
    "title": "Ceremonies &amp; Collaboration",
    "content": "For Location Independent Distributed Squads, . | Ceremonies should happen in overlapping time zone | Tools to enable location independent collaboration | Client Product Owner is required for Backlog Grooming, Prioritization, Playback and formal acceptance of user stories by end of sprint | . | Dynamic Collaboration | . | | . ",
    "url": "http://localhost:4000/githubPage/dynamic-delivery#ceremonies--collaboration",
    "relUrl": "/dynamic-delivery#ceremonies--collaboration"
  },"63": {
    "doc": "Dynamic Delivery",
    "title": "Challenges of Distributed Agile",
    "content": ". | Distributed Agile promotes Federated Innovation and taps into Specialized Talents available in various development centers | ‘Follow-the-Sun’ approach contributes towards ‘Faster Time to Market’ | . Communication . | Use visuals in communication / user story | Reduce team dependency in assigning work items | Use of Technology for team synergy – WebEx | . Accessibility . | Plan work time overlap across locations | Add handover as an agenda in Daily Meeting | Come up with Proxy roles for location | . Squad Structure . | Enable horizontal communication of peers | Squad size limited to around 8 resources | Virtual task-board to ease collaboration and sharing | . Delivery . | Parallel Scrums for distributing the workloads | Assign entire logical work to squads in same time zone | Integration Sprint | . ",
    "url": "http://localhost:4000/githubPage/dynamic-delivery#challenges-of-distributed-agile",
    "relUrl": "/dynamic-delivery#challenges-of-distributed-agile"
  },"64": {
    "doc": "Enterprise Agility",
    "title": "Enterprise Agility",
    "content": "Chapter Layout . | Objective | Key Questions to Ask | IBM’s Approach | Sighed Benefits  | Risks | Client Stories | . ",
    "url": "http://localhost:4000/githubPage/enterprise-agility#enterprise-agility",
    "relUrl": "/enterprise-agility#enterprise-agility"
  },"65": {
    "doc": "Enterprise Agility",
    "title": "Enterprise Agility",
    "content": " ",
    "url": "http://localhost:4000/githubPage/enterprise-agility",
    "relUrl": "/enterprise-agility"
  },"66": {
    "doc": "Event Driven",
    "title": "Event Driven Architecture",
    "content": ". Table of Contents . | Introduction | EDA Overview | EDA Characteristics | Typical EDA Use Cases | IBM’s PoV on EDA . | Architectural Blueprint for EDA-Microservices Systems | Architectural Concerns &amp; Complexities | Key Architectural Considerations | Architectural Patterns | Technology Stack | Event Modeling | Event Processing Topology | Deployment Topology | Exception Handling Strategy | Event Backbone Capabilities and Constraints | Security | Observability | Fault Tolerance and Response | References | . | . ",
    "url": "http://localhost:4000/githubPage/event-driven#event-driven-architecture",
    "relUrl": "/event-driven#event-driven-architecture"
  },"67": {
    "doc": "Event Driven",
    "title": "Introduction",
    "content": "Today’s IT systems are generating, collecting, and processing more data than ever before. And, they are dealing with highly complex processes (that are being automated) and integrations between systems and devices that cut across typical organizational boundaries. At the same time, IT systems are expected to be developed more quickly and cheaply, while also being highly available, scalable, and resilient. To achieve these aims, developers are adopting architectural styles and programming paradigms, such as micoservices, event-driven architecture, DevOps, and more. New tools and frameworks are being built to help developers deliver on these expectations. Developers are combining Event Driven Architecture (EDA) and Microservices architectural styles to build systems that are extremely scalable, available, fault tolerant, concurrent, and easy to develop and maintain. By combining these two architectural styles, developers can build distributed, highly scalable, available, fault-tolerant, and extensible systems. These systems can consume, process, aggregate, or correlate extremely large amounts of events or information in real-time. Developers can easily extend and enhance these systems by using industry-standard open-source frameworks and cloud platforms. ",
    "url": "http://localhost:4000/githubPage/event-driven#introduction",
    "relUrl": "/event-driven#introduction"
  },"68": {
    "doc": "Event Driven",
    "title": "EDA Overview",
    "content": "Event Driven Architecture (EDA) has existed for a long time. Cloud, microservices, and serverless programming paradigms and sophisticated development frameworks are increasing the applicability of EDA in solving mission critical business problems in real time. Technologies and Platforms, such as Kafka, IBM Cloud Pak for Integration, and Lightbend, and development frameworks such as Spring Cloud Stream, Quarkus, and Camel, all provide first class support to EDA development. EDA is also extended for ‘streaming data processing’ which is a requirement for developing real-time artificial intelligence or machine learning solutions. ",
    "url": "http://localhost:4000/githubPage/event-driven#eda-overview",
    "relUrl": "/event-driven#eda-overview"
  },"69": {
    "doc": "Event Driven",
    "title": "EDA Characteristics",
    "content": ". | Loose coupling between components/services | Ability to scale individual components | Processing components can be developed independent of each other | High cloud affinity | Asynchronous nature. As well as ability to throttle workload | Fault Tolerance and better resiliency | Ability to build processing pipelines | Availability of sophisticated event brokers reduce code complexity | A rich palate of proven Enterprise Integration Patterns | . ",
    "url": "http://localhost:4000/githubPage/event-driven#eda-characteristics",
    "relUrl": "/event-driven#eda-characteristics"
  },"70": {
    "doc": "Event Driven",
    "title": "Typical EDA Use Cases",
    "content": "Typical EDA use cases span across all the use cases where real time processing is required. In addition, rapid digitization of business processes systems are generating events which might drive processes in other systems. i.e. systems need to communicate with each other. With the advent of new technologies, use cases that were traditionally developed as batch jobs can now be done in near real-time using stream data processing. Following categories of use cases are best fit for EDA: . | IoT use cases - where there are IoT devices sending events which need to be correlated and processed | Real time stream data processing for computing aggregations, building data stores/in-memory data grids, etc. These data stores and computed aggregations can be used for near real-time MI reporting. | Systems integration for processing offline/asynchronous business processes | Real time data analytics | Monitoring and Alerting (not just systems monitoring, but monitoring of critical business processes - e.g. fraud detection, risk, etc.) | . EDA is extremely powerful when combined with Microservices and Cloud. ",
    "url": "http://localhost:4000/githubPage/event-driven#typical-eda-use-cases",
    "relUrl": "/event-driven#typical-eda-use-cases"
  },"71": {
    "doc": "Event Driven",
    "title": "IBM’s PoV on EDA",
    "content": ". Architectural Blueprint for EDA-Microservices Systems . The following figure is an architectural diagram of an EDA-Microservices-based enterprise system. Some microservices components and types are shown separately for better clarity of the architecture. | Event Driven MicroServices Architecture Overview | . | | . The EDA and Microservices specific components in this blueprint are: . | Event backbone. The event backbone is primarily responsible for transmission, routing, and serialization of events. It can provide APIs for processing event streams. The event backbone offers support for multiple serialization formats and has a major influence on architectural qualities such as fault tolerance, elastic scalability, throughput, and so on. Events can also be stored to create event stores. An event store is a key architectural pattern for recovery and resiliency. | Services Layer. The services layer consists of microservices, integration, and data and analytics services. These services expose their functionality through a variety of interfaces, including REST API, UI, or as EDA event producers and consumers. The services layer also contains services that are specific to EDA and that address cross-cutting concerns, such as orchestration services, streaming data processing services, and so on. | Data Layer. The data layer typically consists of two sublayers. In this blueprint, individual databases owned by microservices are not shown. | Caching layer, which provides distributed and in-memory data caches or grids to improve performance and support patterns such as CQRS. It is horizontally scalable and may also have some level of replication and persistence for resiliency. | Big data layer, which is comprised of data warehouses, ODS, data marts, and AI/ML model processing. | . | Microservices Chassis. The microservices chassis provides the necessary technical and cross-cutting services that are required by different layers of the system. It provides development and runtime capabilities. By using a microservices chassis, you can reduce design and development complexity and operating costs, while you improve time to market, quality of deliverables, and manageability of a huge number of microservices. | Deployment Platform: Elastic, cost optimized, secure, and easy to use cloud platforms should be used. Developers should use as many PaaS services as possible to reduce maintenance and management overheads. The architecture should also provision for hybrid cloud setup, so platforms such as Red Hat OpenShift should be considered. | . Architectural Concerns &amp; Complexities . EDA also introduces some major architectural concerns. Some of these concerns include: . | A large number of distributed and independently deployed components or services, which introduces these issues: . | Design and implementation complexity. Understanding and debugging of such systems is difficult. Event processing workflows are not intuitive and need to be documented. | Multiple points of failure. Increased complexity in testing, debugging, and exception handling. | The release process, deployment, and system monitoring gets complicated and requires high level of automation. | From a development perspective, consistency in implementation, conformance to design, and implementation standards is desired. However, there are multiple development squads. This could result in inconsistent implementation and quality issues. Therefore, the development of a reference architecture that outlines the use of architectural patterns, development frameworks, development of reusable services or utilities, and setting up a robust and effective governance model is essential. | . | Asynchronous event processing is difficult compared to synchronous processing due to requirements related to event ordering or sequencing, callbacks, and exception handling. | Losing information or events is not desirable (obviously). So, the requirements for extremely highly available, scalable, and fault-tolerant systems are especially important, which makes designing and deployment of the systems quite complex. Event producers and consumers have to be designed to withstand failures, have the ability to replay failed events, and have deduplication capabilities. | Lack of support for distributed transactions. This issue means that developers must create custom and complex rollback and recovery implementations spanning across multiple distributed systems. | Maintaining data consistency. Due to the distributed nature and multiple systems of record, maintaining data consistency is complex. In most of the cases, it is eventual consistency due to lack of atomic transactions across multiple distributed systems. | Event consumers and producers have to consider properties that are specific to products that are used for event brokers, data caches, and so on. For example, delivery guarantee influences the design of producers and consumers. | . Key Architectural Considerations . Architectural considerations influence the architecture of a system. They act as guide rails to make architectural decisions. They have a major influence on non-functional characteristics of the system. The following architectural considerations are extremely important for event-driven, microservices-based systems: . | Architectural patterns | Technology stack | Event modeling | Processing topology | Deployment topology | Exception handling | Leveraging event backbone capabilities | Security | Observability | Fault tolerance and response | . Architectural Patterns . Choosing architectural and integration patterns is a critical architectural consideration for event-driven, microservices-based systems. They provide proven and tested solutions for many desired architectural qualities. The following architectural patterns are extremely useful in developing event-driven, microservices-based systems: . | Pipes &amp; Filters | Staged Event Driven Architecture (SEDA) | Event Sourcing | Command Query Responsibility Segregation (CQRS) | SAGA | Stream Processing | Microservices Chassis | Dead Letter Queues (DLQ) | . Additionally, many Enterprise Integration Patterns and Microservices patterns provide the building blocks for event-driven microservices-based systems. Patterns need to be chosen based on requirements and architectural qualities that are desired from the system. Technology Stack . The components such as event brokers, data caches or grids, microservices frameworks, security mechanisms, distributed databases, monitoring systems, and alerting systems form the technology backbone of event-driven, microservices-based systems. This backbone provides support for key architectural qualities (performance, availability, reliability, operating cost, fault-tolerance, and so on) and simplifies development. It also influences several design and development decisions. When choosing your technology stack, consider these characteristics: . | Horizontal Scalability of individual components. Scaling should not compromise availability. That is, the addition of nodes should not require downtime. | High Availability of individual components. The selected product or framework should support clustering with capability to have members across different availability zones or regions, support rolling upgrades, support data replication, and should be fault-tolerant which means the cluster should re-balance itself in case of loss of nodes. | Cloud Affinity, which means it should be easy to deploy on cloud. In fact, if they are available as services on a PaaS platform, its even better because it reduces management and maintenance overhead. Support for containerization is a must. | Low operating cost, which means it should be able to run on commodity hardware and should be frugal in terms of CPU, memory, and storage. | Configurability and tuning of the behavior and non-functional characteristics without downtime. | Manageability. | Vendor lock-in should be avoided. Choose products that are based on open standards or are open source products. When choosing an open source product, consider how widely adopted the product is, whether it has a thriving developer community, and the license should be open and not very restrictive (such as the Apache License V2.0). | For event brokers and development frameworks, they should have support for: . | Multiple serialization formats (JSON, AVRO, Protobuf, etc.) | Exception handling and dead letter queues (DLQs) | Stream processing (including support for aggregations, joins, and windowing) | Partitioning and preserving the order of events | . | Reactive programming support is nice to have. | Polyglot programming support is nice to have in Event backbone. | . Following table lists down the popular choices for different components: . | Component Type | Choices | . | Event Backbone | Apache Kafka, integration platforms such as IBM Cloud Pak for Integration, Lightbend, AWS Eventbridge + Kinesis | . | Microservices development frameworks | Spring frameworks such as Spring Boot, Spring Cloud Stream, Quarkus, Apache Camel | . | Data Caches/Grids | Apache Ignite, Redis, Ehcache, Elasticsearch, Hazelcast | . | Observability | Prometheus + Grafana, ELK, StatsD + Graphite , Sysdig, AppDynamics, Datadog | . Event Modeling . Event modeling consists of defining event types, event hierarchy, event metadata, and payload schemas. Carefully consider these event modeling characteristics: . | Event types. In an enterprise system there are multiple business domains each consuming and producing different types of events. One of the key aspects of modeling is identifying events types and events. Use domain driven design and practices such as event storming and event sources, to identify and classify events. Event types can be hierarchical in nature which help in having a layered approach to event processing. Define event types and events to cover all business requirements and map them to different business processes or workflows. Granularity of event types is of key importance to avoid tight coupling between components. Event types are key to defining routing rules. | Event schema. Event schema is comprised of event metadata (such as type, time, source system, and so on) and payload (that is, information) that is used for processing by event processors. Event type is typically used for routing. Event metadata is typically used for correlating and ordering events, but it can be used for audit and authorization purposes as well. Payloads influence the sizing of queues, topics and event stores, network performance, (de)serialization performance, and resource utilization. Avoid duplicating content. You can always just regenerate the state by replaying the events whenever required. | Versioning. Requirements and implementation evolve over time and they often will impact the event model. Changes to the event model can potentially impact too many microservices. Changing all impacted services simultaneously is not practical. Therefore, the event model should have support for multiple versions and be backward compatible so that microservices can change at a time convenient to them. It is also a good idea to add new attributes to the payload instead of changing the existing attributes (deprecate instead of change). Versioning is dependent on serialization format. | Serialization format. There are multiple serialization formats that can be used to encode the event and its payload, such as JSON, protobuf, or Apache Avro. Important considerations here are schema evolution support, (de)serialization performance and serialized size. It is very easy to develop and debug JSON because the event message is human readable, but JSON is not performant and could increase the event storage requirement. Whereas Avro or Protobuf reduce the size of the payload, are fast, and support schema evolution, they require additional design and development effort. | Partitioning. The partitioning of events is important to increase concurrency, scalability, and availability. Partitioning is also key to the ordering of messages. From an architecture perspective, selecting a partitioning key is important. Having a very coarse-grained key will impact scalability and concurrency. Having a very fine-grained key might not help in preserving order of events. In event brokers such as Kafka, partitioning bounds the scalability of event consumers. | Ordering. Some events might need to be ordered (at least for the given entity) based on their arrival time. For example, account transactions for a given account have to be processed sequentially. It is important to identify events that require ordering. Ordering should be used only where it is essential, since it has an impact on performance and throughput. In Apache Kafka, ordering of events is directly related to partitioning. | Event durability Durability means how long should the event be available on the queues or topics. For example, should you delete the event as soon as it is consumed. Delete events older than the configured retention period. Delete events which have explicit markers (such as tombstones in Kafka). Based on the requirements, one of these should be chosen and configured. While using timebased retention, consider how long the events should be available for replay if required. If the event store pattern is being used, then an additional question about number of versions of the same event or payload that need to be maintained has to be thought about. Event brokers such as Kafka provide various configuration options that can be set at the topic level to specify the durability of events. | . Event Processing Topology . In EDA, processing topology refers to the organization of producers, consumers, enterprise integration patterns, and topics and queues to provide event processing capability. They are basically event processing pipelines where parts of functional logic (processors) are joined together using enterprise integration patterns and queues and topics. Processing topology is a combination of the SEDA, EIP, and Pipes &amp; Filter patterns. For complex event processing, multiple processing topologies can be connected to each other. Another key concept in processing topology is orchestration vs. choreography. Orchestration refers to having a central orchestrator that orchestrates the processing workflow by calling different components. Whenever a strict control is required over processing, orchestration is chosen, such as for payments processing. Orchestration is typically used where the SAGA pattern is employed. Orchestration has a trade-off with performance and availability (as the orchestrator could become the single point of failure). Choreography refers to a completely de-centralized way of processing. That is, events are published and interested components subscribe to topics. There is no central component to control the processing flow. Choreography is complex to implement and maintain. Consider these guidelines for creating processing topologies: . | Processing stages (processors) should be connected using persistent queues and topics. | Configure partitioning keys and message retention policies at each queue or topic. | Granularity of processing is important. If the processors are too fine grained, then there is a chance of tight coupling between processors. Ideally, each processor should be logically independent of each other. | Microservices can be used for implementing processors. This allows for loose coupling, segregation of responsibilities, and ease of development. | Processing concurrency should be configurable at processor level. | Use proven Enterprise Integration Patterns (EIPs). Choose development frameworks that provide built-in support for EIP such as Apache Camel or Spring-cloud-stream. | Build modular and hierarchical processing topologies such that complex event processing is achieved by assembling simple processing pipelines. This helps in making the implementation modular and easy to update. | If processors have a state (that changes with events), consider having stores to back the states for increased fault-tolerance and recoverability. | . Architectural practices such as Process event streams and Event managed state can be used to design the processing topology. It is also good to have a detailed understanding of event broker capabilities while defining the processing topology. For instance Kafka streams provides first class support for defining event stream processing topologies. Kakfa also provides automatic support for state stores when performing aggregation and join operations on event streams. The following figure depicts a blueprint of a processing topology: . | Event Processing Pipeline | . | | . And this following figure depicts a simplified order processing toplogy for online shopping. The router has the ability to dynamically route events to multiple topics. Also note that event processors will also have ‘event filters’ to control consumption and production of events based on context. | Order Processing Topology | . | | . Deployment Topology . In an EDA-microservices architecture, there are numerous components to deploy. A deployment topology should be chosen such that architectural requirements related to scalability, availability, resiliency, security, and cost are met. However, there are tradeoffs to be made between redundancy, performance, and cost. Deployment to cloud makes the architecture even more performant, resilient, and cost efficient. Capabilities that are provided by cloud deployments (such as high availability setup in Kubernetes) should be exploited. Consider these key principles to consider for your deployment topology: . | Each deployed component should be independently scalable and deployed as a cluster to increase concurrency and resiliency. | Ensure that each cluster spans across multiple availability zones. This setup gives more resiliency in case of data-center failures. An added advantage of this is, instead of having a passive DR, an active-active deployment across different availability zones or regions can be done. | Replication factor determines the number of replicas of an event or information. Without replication, failure of individual instances (even though clustered) would result in data loss. This is especially required for event brokers and databases. However, replication comes at compute and storage cost. Replication should be set based on factors such as availability zones, data regions, number of nodes, and so on. | In the case of Kafka, the number of topic partitions places an upper bound on concurrency of consumers. | Throttling of workloads. Configure thread pools and the number of instances of consumers and producers to throttle throughput. Depending on the volume and throughput of the downstream processors, these parameters need to be adjusted accordingly. | Data compression. If the payload size is big and CPU availability is high, then compression can be used to compress the events before transmission. However, compression is a tradeoff between network utilization and CPU utilization. | Data encryption. Based on security standards in the organization, configure TLS, authentication, and authorization between the event broker and producers and consumers (and for your databases). Please note that enabling TLS can increase CPU utilization. | . Additionally, it is important to have support for automated deployments, automated failover, rolling upgrades or blue-green deployments, and the externalization of the configuration to make the environment of the deployment artifacts independent. Exception Handling Strategy . In EDA, having a comprehensive and consistent exception handling strategy is important to improve resiliency. Exception handling strategy consists of all or some of the following: . | Logging the exception | Retrying the event for specified number of time and at specified retry intervals | Moving the event to a dead letter queue (or stopping the processing of events), if all retries are exhausted | Raising alerts | In some cases, generating an event | Correcting the cause of exception and replaying the event | . Exceptions can be of two types: business exceptions and system exceptions. Business exceptions are raised when validations or a business condition fails. System exceptions are a broad category of failures due to unavailability of components (database, event broker, or other microservices) or due to resource issues (such as OutOfMemory errors), network or transport related issues (such as payload serialization or de-serialization errors), or unexpected code failure (such as NullPointerException or ClassCastException). There is significant variation in how you handle the different types of exceptions. Some of the exception handling mechanisms are listed below: . | Expected business exceptions are typically handled in the code. Handling could involve logging the exception, updating entities and their state, generating exception events, or consuming the exception and moving on. | Exceptions due to invalid payloads (including serialization or de-serialization issues) will not be solved with retries. Such events are referred as poision pills in Kafka (because it blocks subsequent messages of that partition). Intervention might be required for such events. It is advisable to move them to a dead letter queue (DLQ). The DLQ consumers should allow correction and replay of events. | System exceptions due to unavailability of components are temporary in nature. Hence, multiple retries should be configured. Another key configuration parameter is backoff multiplier. It is used to have exponentially increasing time interval between consecutive retries. Different frameworks have different strategies if the failure persists after retries. For instance, Camel would move the event to a DLQ. Kafka streams would stop the processing. It is advisable to use the default behavior of the frameworks in such scenarios. | Resource issues (such as OutOfMemory errors) are typically at the component level and would result in the unavailability of a component. The risk of losing events is minimal here due to the fault tolerant nature of the event broker. Also, when deployed in a Kubernetes environment, new pods are started to replace failed pods. | The SAGA pattern is used where data consistency is very important and processing involves multiple microservices. Use the SAGA pattern for those events where data consistency requirements are very strict. | Recovery and replay should be thought about from the beginning and not applied as an afterthought (it becomes extremely complex later). Recovery and replay components are typically custom developed and vary based on event processing. The simplest replay component might just pick up the failed event and republish it on the input topic. | . Your development framework should support having a consistent exception handling strategy across all microservices. It should provide a set of predefined exception classes for business exceptions and provide a generic exception handler that can be customized using configuration but enforces architectural decisions related to exception handling. Most development frameworks do provide such support. However, they need to be configured correctly or extended to provide the required features. Event Backbone Capabilities and Constraints . Different event backbone products or platforms provide support for architectural qualities differently. At the same time, they impose constraints on design and architecture. While defining the architecture, their capabilities and constraints should be considered to effectively address the non-functional requirements. For example, the following are few important capabilities and constraints for Kafka. | Kafka provides support for event ordering based on partition keys. It also ensures that there is a single consumer (thread) listening on a partition. This makes it very easy to order events just by selecting an appropriate partition key. For example, OrderId, when used as an partition key, will ensure that all events related to a particular order will be processed in the order of their arrival. | Kafka supports idempotence for producers. This means Kafka ensures that an event is produced exactly once by a producer. Developers don’t need to worry about it. | Kafka provides at least once delivery guarantee. This means consumers should be able to handle duplicate messages. Developers need to be aware of the guarantees provided by their event brokers. | Another important aspect for Kafka is an offset-commit strategy for consumers, which means whether events should be automatically or manually acknowledged. If auto-commit is enabled, events that produce an error might get lost (if exceptions are consumed) or the consumer might see duplicate messages. Manual commits can be used to counter this, but it requires additional code. Frameworks such as spring-cloud-stream that work seamlessly with Kafka, provide the choice of not auto-committing in case of errors or moving the failed events to a DLQ in addition to manual/auto-commit. This is an important aspect that needs to be thought through during design. | Kafka Streams provides the ability to process event streams and easily perform various advanced and complex operations on event streams such as aggregations and joins. This makes it is very easy to perform analytics in real time. For example, computing real-time statistics of events grouped by various dimensions requires very minimal coding. These are stateful operations and maintain a state. Kafka also provides automatic fault-tolerance through state-stores. | . Security . Developers must consider these aspects of security in EDA-microservices architectures: . | Transport level security | Authenticated &amp; authorized access to event production and consumption | Audit trails for event processing | Data security (such as authorized access and encrypted storage) | Eliminating vulnerabilities in the code | Perimeter security devices and patterns | . Observability . Observability includes monitoring, logging, tracing, and alerting. Each component of the system should be observable to avoid failures and also to quickly recover from failures. Most of the EDA products and development frameworks provide support for observability by publishing metrics that can be exported into industry-standard observability tools such as Prometheus and Grafana, ELK, StatsD and Graphite, Splunk, or AppDynamics. For example, Apache Kafka provides detailed metrics that can be exported and integrated with most of these tools. Also, cloud platforms that offer managed services for an event backbone (IBM Event Streams) provide first class support for observability. Microservices development frameworks such as Spring or Camel provide good support for code instrumentation for monitoring. From an EDA perspective, instrumenting the code of producers and consumers for publishing metrics, publishing event broker metrics, and correlating these through a metrics dashboard is essential because the number of distributed components in EDA is high. Some of the key metrics from an EDA perspective are rate of incoming and outgoing messages, lag in consumption, network latency, queue and topic sizes, and so on. For monitoring microservices, refer to my article on Monitor Spring Boot microservices for a detailed tutorial on instrumenting and monitoring microservices. Fault Tolerance and Response . To provide adequate fault tolerance, the architecture needs to provide redundancy, exception handling, and elastic scaling (scaling up when thresholds are breached and scaling down when load returns to normal). With EDA and cloud, most of these can be easily achieved. Event backbones cater to fault-tolerance by supporting the clustering and replication of queues and topics. Producers and consumers can have multiple instances deployed. When deployed as containers on a Kubernetes platform, elastic scaling can be easily achieved through auto-scaling (using horizontal pod auto-scalers) but exception handling has to be designed for producers and consumers. Although EDA-based systems provide for resiliency through staged architecture, quick failure response and recovery is critical to avoid delays and consistency issues. To achieve this quick recovery, you need: . * Automation for starting and stopping instances and restarting failed instances, which can be easily configured in Kubernetes-based platforms, such as Red Hat OpenShift * Raising alerts and incidents as and when failures occur * A well-defined incident management process * Availability of logs and the ability to correlate logs across multiple components through tracing. Tracing needs to be enabled in microservices. Development frameworks such as spring-sleuth can be used for this. For log aggregation, tools such as ELK or Splunk can be used. This would help the team identify the root cause and resolve the issue quickly. References . | Architectural considerations for event-driven microservices-based systems | IBM Architecture Centre - Event Driven Architecture | . ",
    "url": "http://localhost:4000/githubPage/event-driven#ibms-pov-on-eda",
    "relUrl": "/event-driven#ibms-pov-on-eda"
  },"72": {
    "doc": "Event Driven",
    "title": "Event Driven",
    "content": " ",
    "url": "http://localhost:4000/githubPage/event-driven",
    "relUrl": "/event-driven"
  },"73": {
    "doc": "Garage",
    "title": "Garage",
    "content": ". Table of Contents . | Objective | IBM’s Approach | . ",
    "url": "http://localhost:4000/githubPage/garage",
    "relUrl": "/garage"
  },"74": {
    "doc": "Garage",
    "title": "Objective",
    "content": "This section is used to highlight a Collaborative Delivery Model that is both organic &amp; immersive and provides value for our clients. | Garage | . | | . ",
    "url": "http://localhost:4000/githubPage/garage#objective",
    "relUrl": "/garage#objective"
  },"75": {
    "doc": "Garage",
    "title": "IBM’s Approach",
    "content": "We offer a unique way of engaging through co-creation, joint squads, and the IBM Garage Method for Cloud to deliver organizational change and a new culture of speed and velocity that’s sustainable for the digital future. | We think the best way to deliver co-creation to the clients is through joint squads | There should be SMEs from both IBM and from the client to ensure that skills are being transferred throughout the process | They’re working together in a very agile, very iterative way to prioritize what’s getting developed | . There are three distinct ways of delivery build applications on cloud: . | Envision: Quick engagement similar to a PoC that does high level architecture and defines the MVP | MVP: Actually building an MVP deployed to production – also contains elements of design and analyse | Scale: Full engagement with multiple teams | . | IBM Approach | . | | . |   | Envision | MVP | Scale | . | Outcomes | - Architecture and Blueprint - MVP Definition - Journey Map - User Story Backlog - Journey Maps | - Blueprint &amp; E2E Architecture - MVP Deployed to Production - 12-30 Microservices - 135-270 story points with varying complexity | - 2-5 Microservices per Month - Deployment across geos, LOBs - Co-Creation and Skills Transfer - Organizational Change - New technology adoption and skillset(AI, Analytics, Automation, Blockchain) | . | Key Activities | - IBM Garage Design Thinking Workshops - Cloud Advisory Assessment - Experience Led Development | - Set up Dev &amp; Test Environments - Joint Development Squads - Coexistence, E2E Testing and Integration | - Organizational Change Management - Employee Engagement - Digital Change - Full Dev &amp; Test | . | Duration | 3 Month Week | 3-6 Months | 6+ Months | . | Deployment Model | Multi-cloud, On-prem | Multi-cloud, On-prem | Multi-cloud, On-prem | . ",
    "url": "http://localhost:4000/githubPage/garage#ibms-approach",
    "relUrl": "/garage#ibms-approach"
  },"76": {
    "doc": "Governance",
    "title": "Manage and control without inhibiting agility &amp; flexibility",
    "content": ". Table of Contents . | Overview | Structure | Active Oversight Responsibilities | Talent &amp; Culture | Infrastructure | . ",
    "url": "http://localhost:4000/githubPage/governance#manage-and-control-without-inhibiting-agility--flexibility",
    "relUrl": "/governance#manage-and-control-without-inhibiting-agility--flexibility"
  },"77": {
    "doc": "Governance",
    "title": "Overview",
    "content": "The modern Cloud Strategy Operating Model Governance model ensures that all four of its IT elements (i.e. Cloud Centre of Excellence, Application Engineering &amp; Operations, Platform Engineering &amp; Operations, and Integrated Service Management) play an integral part in the successful delivery and realisation of strategic initiatives, enabled through IT cloud capabilities and services. | Governance | . | | . This document describles the necessary forums that are required to determine how: . | Digital products and services are being adopted and used via standardised frameworks and services. | Operations of capabilities and services are performing. | . The Governance model will span across three accountability levels i.e. Strategy, Management and Operations — and will include board level decisions in Exec Committees through to day-to-day management decisions, which will in turn drive Operational delivery. An accountability level characterises the scope and intent of the activity and decision-making. This cuts across all four IT Elements of a modern operating model. | Governance Pillars | . | | . Structured within a hierarchical model, each governing Board or Committee adds distinct value to the Enterprise: . | Governance Structure | . | | . But how does this work in practice? . | Governance Boards | . | | . “Good” governance model is comprised of four main component: . | Structure | Active Oversight Responsibilities | Talent &amp; Culture | Infrastructure | . ",
    "url": "http://localhost:4000/githubPage/governance#overview",
    "relUrl": "/governance#overview"
  },"78": {
    "doc": "Governance",
    "title": "Structure",
    "content": ". | Outlines board and management committee structures, mandates, membership, and charters | Establishes design of governance framework | Delineates organizational structure, reporting lines, and relationships | Highlights roles and independence of control and support functions from business owners | . ",
    "url": "http://localhost:4000/githubPage/governance#structure",
    "relUrl": "/governance#structure"
  },"79": {
    "doc": "Governance",
    "title": "Active Oversight Responsibilities",
    "content": ". | Outlines the type of committees (board and management) and associated responsibilities | Specifies functional accountabilities for day-to-day management of business practices across the enterprise | Delineates board and management policies supporting delegation of authority (decision rights) including reporting, escalation, and veto rights | . ",
    "url": "http://localhost:4000/githubPage/governance#active-oversight-responsibilities",
    "relUrl": "/governance#active-oversight-responsibilities"
  },"80": {
    "doc": "Governance",
    "title": "Talent &amp; Culture",
    "content": ". | Aligns governance with operating and business principles | Articulates core beliefs and foundation for culture | Highlights characteristics of risk culture | Outlines leadership success, assessment, and development responsibilities | Aligns performance management, approach, measures and responsibilities to compensation and incentive plans | . ",
    "url": "http://localhost:4000/githubPage/governance#talent--culture",
    "relUrl": "/governance#talent--culture"
  },"81": {
    "doc": "Governance",
    "title": "Infrastructure",
    "content": ". | Establishes design and content of policy manuals and associated procedures | Outlines type and frequency of internal reporting and communications | Defines scorecards, measures, and metrics to track performance | Aligns technology and governance requirements | . ",
    "url": "http://localhost:4000/githubPage/governance#infrastructure",
    "relUrl": "/governance#infrastructure"
  },"82": {
    "doc": "Governance",
    "title": "Governance",
    "content": " ",
    "url": "http://localhost:4000/githubPage/governance",
    "relUrl": "/governance"
  },"83": {
    "doc": "Tooling",
    "title": "Multicloud / Hybrid Cloud",
    "content": " ",
    "url": "http://localhost:4000/githubPage/hybrid-cloud#multicloud--hybrid-cloud",
    "relUrl": "/hybrid-cloud#multicloud--hybrid-cloud"
  },"84": {
    "doc": "Tooling",
    "title": "Chapter Layout",
    "content": ". | Objective | Key Questions to Ask | IBM’s Approach | Sighed Benefits  | Risks | Client Stories | . ",
    "url": "http://localhost:4000/githubPage/hybrid-cloud#chapter-layout",
    "relUrl": "/hybrid-cloud#chapter-layout"
  },"85": {
    "doc": "Tooling",
    "title": "Tooling",
    "content": " ",
    "url": "http://localhost:4000/githubPage/hybrid-cloud",
    "relUrl": "/hybrid-cloud"
  },"86": {
    "doc": "Architecture",
    "title": "Architecture",
    "content": ". Table of Contents . | Event Driven Architecture | . ",
    "url": "http://localhost:4000/githubPage/arch",
    "relUrl": "/arch"
  },"87": {
    "doc": "Architecture",
    "title": "1. Event Driven Architecture",
    "content": " ",
    "url": "http://localhost:4000/githubPage/arch#1-event-driven-architecture",
    "relUrl": "/arch#1-event-driven-architecture"
  },"88": {
    "doc": "Architecture",
    "title": "Introduction",
    "content": "Today’s IT systems are generating, collecting, and processing more data than ever before. And, they are dealing with highly complex processes (that are being automated) and integrations between systems and devices that cut across typical organizational boundaries. At the same time, IT systems are expected to be developed more quickly and cheaply, while also being highly available, scalable, and resilient. To achieve these aims, developers are adopting architectural styles and programming paradigms, such as micoservices, event-driven architecture, DevOps, and more. New tools and frameworks are being built to help developers deliver on these expectations. Developers are combining Event Driven Architecture (EDA) and Microservices architectural styles to build systems that are extremely scalable, available, fault tolerant, concurrent, and easy to develop and maintain. By combining these two architectural styles, developers can build distributed, highly scalable, available, fault-tolerant, and extensible systems. These systems can consume, process, aggregate, or correlate extremely large amounts of events or information in real-time. Developers can easily extend and enhance these systems by using industry-standard open-source frameworks and cloud platforms. ",
    "url": "http://localhost:4000/githubPage/arch#introduction",
    "relUrl": "/arch#introduction"
  },"89": {
    "doc": "Architecture",
    "title": "EDA Overview",
    "content": "Event Driven Architecture (EDA) has existed for a long time. Cloud, microservices, and serverless programming paradigms and sophisticated development frameworks are increasing the applicability of EDA in solving mission critical business problems in real time. Technologies and Platforms, such as Kafka, IBM Cloud Pak for Integration, and Lightbend, and development frameworks such as Spring Cloud Stream, Quarkus, and Camel, all provide first class support to EDA development. EDA is also extended for ‘streaming data processing’ which is a requirement for developing real-time artificial intelligence or machine learning solutions. ",
    "url": "http://localhost:4000/githubPage/arch#eda-overview",
    "relUrl": "/arch#eda-overview"
  },"90": {
    "doc": "Architecture",
    "title": "EDA Characteristics",
    "content": ". | Loose coupling between components/services | Ability to scale individual components | Processing components can be developed independent of each other | High cloud affinity | Asynchronous nature. As well as ability to throttle workload | Fault Tolerance and better resiliency | Ability to build processing pipelines | Availability of sophisticated event brokers reduce code complexity | A rich palate of proven Enterprise Integration Patterns | . ",
    "url": "http://localhost:4000/githubPage/arch#eda-characteristics",
    "relUrl": "/arch#eda-characteristics"
  },"91": {
    "doc": "Architecture",
    "title": "Typical EDA Use Cases",
    "content": "Typical EDA use cases span across all the use cases where real time processing is required. In addition, rapid digitization of business processes systems are generating events which might drive processes in other systems. i.e. systems need to communicate with each other. With the advent of new technologies, use cases that were traditionally developed as batch jobs can now be done in near real-time using stream data processing. Following categories of use cases are best fit for EDA: . | IoT use cases - where there are IoT devices sending events which need to be correlated and processed | Real time stream data processing for computing aggregations, building data stores/in-memory data grids, etc. These data stores and computed aggregations can be used for near real-time MI reporting. | Systems integration for processing offline/asynchronous business processes | Real time data analytics | Monitoring and Alerting (not just systems monitoring, but monitoring of critical business processes - e.g. fraud detection, risk, etc.) | . EDA is extremely powerful when combined with Microservices and Cloud. ",
    "url": "http://localhost:4000/githubPage/arch#typical-eda-use-cases",
    "relUrl": "/arch#typical-eda-use-cases"
  },"92": {
    "doc": "Architecture",
    "title": "IBM’s PoV on EDA",
    "content": "Architectural Blueprint for EDA-Microservices Systems . The following figure is an architectural diagram of an EDA-Microservices-based enterprise system. Some microservices components and types are shown separately for better clarity of the architecture. | Event Driven MicroServices Architecture Overview | . | | . The EDA and Microservices specific components in this blueprint are: . | Event backbone. The event backbone is primarily responsible for transmission, routing, and serialization of events. It can provide APIs for processing event streams. The event backbone offers support for multiple serialization formats and has a major influence on architectural qualities such as fault tolerance, elastic scalability, throughput, and so on. Events can also be stored to create event stores. An event store is a key architectural pattern for recovery and resiliency. | Services Layer. The services layer consists of microservices, integration, and data and analytics services. These services expose their functionality through a variety of interfaces, including REST API, UI, or as EDA event producers and consumers. The services layer also contains services that are specific to EDA and that address cross-cutting concerns, such as orchestration services, streaming data processing services, and so on. | Data Layer. The data layer typically consists of two sublayers. In this blueprint, individual databases owned by microservices are not shown. | Caching layer, which provides distributed and in-memory data caches or grids to improve performance and support patterns such as CQRS. It is horizontally scalable and may also have some level of replication and persistence for resiliency. | Big data layer, which is comprised of data warehouses, ODS, data marts, and AI/ML model processing. | . | Microservices Chassis. The microservices chassis provides the necessary technical and cross-cutting services that are required by different layers of the system. It provides development and runtime capabilities. By using a microservices chassis, you can reduce design and development complexity and operating costs, while you improve time to market, quality of deliverables, and manageability of a huge number of microservices. | Deployment Platform: Elastic, cost optimized, secure, and easy to use cloud platforms should be used. Developers should use as many PaaS services as possible to reduce maintenance and management overheads. The architecture should also provision for hybrid cloud setup, so platforms such as Red Hat OpenShift should be considered. | . Architectural Concerns &amp; Complexities . EDA also introduces some major architectural concerns. Some of these concerns include: . | A large number of distributed and independently deployed components or services, which introduces these issues: . | Design and implementation complexity. Understanding and debugging of such systems is difficult. Event processing workflows are not intuitive and need to be documented. | Multiple points of failure. Increased complexity in testing, debugging, and exception handling. | The release process, deployment, and system monitoring gets complicated and requires high level of automation. | From a development perspective, consistency in implementation, conformance to design, and implementation standards is desired. However, there are multiple development squads. This could result in inconsistent implementation and quality issues. Therefore, the development of a reference architecture that outlines the use of architectural patterns, development frameworks, development of reusable services or utilities, and setting up a robust and effective governance model is essential. | . | Asynchronous event processing is difficult compared to synchronous processing due to requirements related to event ordering or sequencing, callbacks, and exception handling. | Losing information or events is not desirable (obviously). So, the requirements for extremely highly available, scalable, and fault-tolerant systems are especially important, which makes designing and deployment of the systems quite complex. Event producers and consumers have to be designed to withstand failures, have the ability to replay failed events, and have deduplication capabilities. | Lack of support for distributed transactions. This issue means that developers must create custom and complex rollback and recovery implementations spanning across multiple distributed systems. | Maintaining data consistency. Due to the distributed nature and multiple systems of record, maintaining data consistency is complex. In most of the cases, it is eventual consistency due to lack of atomic transactions across multiple distributed systems. | Event consumers and producers have to consider properties that are specific to products that are used for event brokers, data caches, and so on. For example, delivery guarantee influences the design of producers and consumers. | . Key Architectural Considerations . Architectural considerations influence the architecture of a system. They act as guide rails to make architectural decisions. They have a major influence on non-functional characteristics of the system. The following architectural considerations are extremely important for event-driven, microservices-based systems: . | Architectural patterns | Technology stack | Event modeling | Processing topology | Deployment topology | Exception handling | Leveraging event backbone capabilities | Security | Observability | Fault tolerance and response | . Architectural Patterns . Choosing architectural and integration patterns is a critical architectural consideration for event-driven, microservices-based systems. They provide proven and tested solutions for many desired architectural qualities. The following architectural patterns are extremely useful in developing event-driven, microservices-based systems: . | Pipes &amp; Filters | Staged Event Driven Architecture (SEDA) | Event Sourcing | Command Query Responsibility Segregation (CQRS) | SAGA | Stream Processing | Microservices Chassis | Dead Letter Queues (DLQ) | . Additionally, many Enterprise Integration Patterns and Microservices patterns provide the building blocks for event-driven microservices-based systems. Patterns need to be chosen based on requirements and architectural qualities that are desired from the system. Technology Stack . The components such as event brokers, data caches or grids, microservices frameworks, security mechanisms, distributed databases, monitoring systems, and alerting systems form the technology backbone of event-driven, microservices-based systems. This backbone provides support for key architectural qualities (performance, availability, reliability, operating cost, fault-tolerance, and so on) and simplifies development. It also influences several design and development decisions. When choosing your technology stack, consider these characteristics: . | Horizontal Scalability of individual components. Scaling should not compromise availability. That is, the addition of nodes should not require downtime. | High Availability of individual components. The selected product or framework should support clustering with capability to have members across different availability zones or regions, support rolling upgrades, support data replication, and should be fault-tolerant which means the cluster should re-balance itself in case of loss of nodes. | Cloud Affinity, which means it should be easy to deploy on cloud. In fact, if they are available as services on a PaaS platform, its even better because it reduces management and maintenance overhead. Support for containerization is a must. | Low operating cost, which means it should be able to run on commodity hardware and should be frugal in terms of CPU, memory, and storage. | Configurability and tuning of the behavior and non-functional characteristics without downtime. | Manageability. | Vendor lock-in should be avoided. Choose products that are based on open standards or are open source products. When choosing an open source product, consider how widely adopted the product is, whether it has a thriving developer community, and the license should be open and not very restrictive (such as the Apache License V2.0). | For event brokers and development frameworks, they should have support for: . | Multiple serialization formats (JSON, AVRO, Protobuf, etc.) | Exception handling and dead letter queues (DLQs) | Stream processing (including support for aggregations, joins, and windowing) | Partitioning and preserving the order of events | . | Reactive programming support is nice to have. | Polyglot programming support is nice to have in Event backbone. | . Following table lists down the popular choices for different components: . | Component Type | Choices | . | Event Backbone | Apache Kafka, integration platforms such as IBM Cloud Pak for Integration, Lightbend, AWS Eventbridge + Kinesis | . | Microservices development frameworks | Spring frameworks such as Spring Boot, Spring Cloud Stream, Quarkus, Apache Camel | . | Data Caches/Grids | Apache Ignite, Redis, Ehcache, Elasticsearch, Hazelcast | . | Observability | Prometheus + Grafana, ELK, StatsD + Graphite, Sysdig, AppDynamics, Datadog | . Event Modeling . Event modeling consists of defining event types, event hierarchy, event metadata, and payload schemas. Carefully consider these event modeling characteristics: . | Event types. In an enterprise system there are multiple business domains each consuming and producing different types of events. One of the key aspects of modeling is identifying events types and events. Use domain driven design and practices such as event storming and event sources, to identify and classify events. Event types can be hierarchical in nature which help in having a layered approach to event processing. Define event types and events to cover all business requirements and map them to different business processes or workflows. Granularity of event types is of key importance to avoid tight coupling between components. Event types are key to defining routing rules. | Event schema. Event schema is comprised of event metadata (such as type, time, source system, and so on) and payload (that is, information) that is used for processing by event processors. Event type is typically used for routing. Event metadata is typically used for correlating and ordering events, but it can be used for audit and authorization purposes as well. Payloads influence the sizing of queues, topics and event stores, network performance, (de)serialization performance, and resource utilization. Avoid duplicating content. You can always just regenerate the state by replaying the events whenever required. | Versioning. Requirements and implementation evolve over time and they often will impact the event model. Changes to the event model can potentially impact too many microservices. Changing all impacted services simultaneously is not practical. Therefore, the event model should have support for multiple versions and be backward compatible so that microservices can change at a time convenient to them. It is also a good idea to add new attributes to the payload instead of changing the existing attributes (deprecate instead of change). Versioning is dependent on serialization format. | Serialization format. There are multiple serialization formats that can be used to encode the event and its payload, such as JSON, protobuf, or Apache Avro. Important considerations here are schema evolution support, (de)serialization performance and serialized size. It is very easy to develop and debug JSON because the event message is human readable, but JSON is not performant and could increase the event storage requirement. Whereas Avro or Protobuf reduce the size of the payload, are fast, and support schema evolution, they require additional design and development effort. | Partitioning. The partitioning of events is important to increase concurrency, scalability, and availability. Partitioning is also key to the ordering of messages. From an architecture perspective, selecting a partitioning key is important. Having a very coarse-grained key will impact scalability and concurrency. Having a very fine-grained key might not help in preserving order of events. In event brokers such as Kafka, partitioning bounds the scalability of event consumers. | Ordering. Some events might need to be ordered (at least for the given entity) based on their arrival time. For example, account transactions for a given account have to be processed sequentially. It is important to identify events that require ordering. Ordering should be used only where it is essential, since it has an impact on performance and throughput. In Apache Kafka, ordering of events is directly related to partitioning. | Event durability Durability means how long should the event be available on the queues or topics. For example, should you delete the event as soon as it is consumed. Delete events older than the configured retention period. Delete events which have explicit markers (such as tombstones in Kafka). Based on the requirements, one of these should be chosen and configured. While using timebased retention, consider how long the events should be available for replay if required. If the event store pattern is being used, then an additional question about number of versions of the same event or payload that need to be maintained has to be thought about. Event brokers such as Kafka provide various configuration options that can be set at the topic level to specify the durability of events. | . Event Processing Topology . In EDA, processing topology refers to the organization of producers, consumers, enterprise integration patterns, and topics and queues to provide event processing capability. They are basically event processing pipelines where parts of functional logic (processors) are joined together using enterprise integration patterns and queues and topics. Processing topology is a combination of the SEDA, EIP, and Pipes &amp; Filter patterns. For complex event processing, multiple processing topologies can be connected to each other. Another key concept in processing topology is orchestration vs. choreography. Orchestration refers to having a central orchestrator that orchestrates the processing workflow by calling different components. Whenever a strict control is required over processing, orchestration is chosen, such as for payments processing. Orchestration is typically used where the SAGA pattern is employed. Orchestration has a trade-off with performance and availability (as the orchestrator could become the single point of failure). Choreography refers to a completely de-centralized way of processing. That is, events are published and interested components subscribe to topics. There is no central component to control the processing flow. Choreography is complex to implement and maintain. Consider these guidelines for creating processing topologies: . | Processing stages (processors) should be connected using persistent queues and topics. | Configure partitioning keys and message retention policies at each queue or topic. | Granularity of processing is important. If the processors are too fine grained, then there is a chance of tight coupling between processors. Ideally, each processor should be logically independent of each other. | Microservices can be used for implementing processors. This allows for loose coupling, segregation of responsibilities, and ease of development. | Processing concurrency should be configurable at processor level. | Use proven Enterprise Integration Patterns (EIPs). Choose development frameworks that provide built-in support for EIP such as Apache Camel or Spring-cloud-stream. | Build modular and hierarchical processing topologies such that complex event processing is achieved by assembling simple processing pipelines. This helps in making the implementation modular and easy to update. | If processors have a state (that changes with events), consider having stores to back the states for increased fault-tolerance and recoverability. | . Architectural practices such as Process event streams and Event managed state can be used to design the processing topology. It is also good to have a detailed understanding of event broker capabilities while defining the processing topology. For instance Kafka streams provides first class support for defining event stream processing topologies. Kakfa also provides automatic support for state stores when performing aggregation and join operations on event streams. The following figure depicts a blueprint of a processing topology: . | Event Processing Pipeline | . | | . And this following figure depicts a simplified order processing toplogy for online shopping. The router has the ability to dynamically route events to multiple topics. Also note that event processors will also have ‘event filters’ to control consumption and production of events based on context. | Order Processing Topology | . | | . Deployment Topology . In an EDA-microservices architecture, there are numerous components to deploy. A deployment topology should be chosen such that architectural requirements related to scalability, availability, resiliency, security, and cost are met. However, there are tradeoffs to be made between redundancy, performance, and cost. Deployment to cloud makes the architecture even more performant, resilient, and cost efficient. Capabilities that are provided by cloud deployments (such as high availability setup in Kubernetes) should be exploited. Consider these key principles to consider for your deployment topology: . | Each deployed component should be independently scalable and deployed as a cluster to increase concurrency and resiliency. | Ensure that each cluster spans across multiple availability zones. This setup gives more resiliency in case of data-center failures. An added advantage of this is, instead of having a passive DR, an active-active deployment across different availability zones or regions can be done. | Replication factor determines the number of replicas of an event or information. Without replication, failure of individual instances (even though clustered) would result in data loss. This is especially required for event brokers and databases. However, replication comes at compute and storage cost. Replication should be set based on factors such as availability zones, data regions, number of nodes, and so on. | In the case of Kafka, the number of topic partitions places an upper bound on concurrency of consumers. | Throttling of workloads. Configure thread pools and the number of instances of consumers and producers to throttle throughput. Depending on the volume and throughput of the downstream processors, these parameters need to be adjusted accordingly. | Data compression. If the payload size is big and CPU availability is high, then compression can be used to compress the events before transmission. However, compression is a tradeoff between network utilization and CPU utilization. | Data encryption. Based on security standards in the organization, configure TLS, authentication, and authorization between the event broker and producers and consumers (and for your databases). Please note that enabling TLS can increase CPU utilization. | . Additionally, it is important to have support for automated deployments, automated failover, rolling upgrades or blue-green deployments, and the externalization of the configuration to make the environment of the deployment artifacts independent. Exception Handling Strategy . In EDA, having a comprehensive and consistent exception handling strategy is important to improve resiliency. Exception handling strategy consists of all or some of the following: . | Logging the exception | Retrying the event for specified number of time and at specified retry intervals | Moving the event to a dead letter queue (or stopping the processing of events), if all retries are exhausted | Raising alerts | In some cases, generating an event | Correcting the cause of exception and replaying the event | . Exceptions can be of two types: business exceptions and system exceptions. Business exceptions are raised when validations or a business condition fails. System exceptions are a broad category of failures due to unavailability of components (database, event broker, or other microservices) or due to resource issues (such as OutOfMemory errors), network or transport related issues (such as payload serialization or de-serialization errors), or unexpected code failure (such as NullPointerException or ClassCastException). There is significant variation in how you handle the different types of exceptions. Some of the exception handling mechanisms are listed below: . | Expected business exceptions are typically handled in the code. Handling could involve logging the exception, updating entities and their state, generating exception events, or consuming the exception and moving on. | Exceptions due to invalid payloads (including serialization or de-serialization issues) will not be solved with retries. Such events are referred as poision pills in Kafka (because it blocks subsequent messages of that partition). Intervention might be required for such events. It is advisable to move them to a dead letter queue (DLQ). The DLQ consumers should allow correction and replay of events. | System exceptions due to unavailability of components are temporary in nature. Hence, multiple retries should be configured. Another key configuration parameter is backoff multiplier. It is used to have exponentially increasing time interval between consecutive retries. Different frameworks have different strategies if the failure persists after retries. For instance, Camel would move the event to a DLQ. Kafka streams would stop the processing. It is advisable to use the default behavior of the frameworks in such scenarios. | Resource issues (such as OutOfMemory errors) are typically at the component level and would result in the unavailability of a component. The risk of losing events is minimal here due to the fault tolerant nature of the event broker. Also, when deployed in a Kubernetes environment, new pods are started to replace failed pods. | The SAGA pattern is used where data consistency is very important and processing involves multiple microservices. Use the SAGA pattern for those events where data consistency requirements are very strict. | Recovery and replay should be thought about from the beginning and not applied as an afterthought (it becomes extremely complex later). Recovery and replay components are typically custom developed and vary based on event processing. The simplest replay component might just pick up the failed event and republish it on the input topic. | . Your development framework should support having a consistent exception handling strategy across all microservices. It should provide a set of predefined exception classes for business exceptions and provide a generic exception handler that can be customized using configuration but enforces architectural decisions related to exception handling. Most development frameworks do provide such support. However, they need to be configured correctly or extended to provide the required features. Event Backbone Capabilities and Constraints . Different event backbone products or platforms provide support for architectural qualities differently. At the same time, they impose constraints on design and architecture. While defining the architecture, their capabilities and constraints should be considered to effectively address the non-functional requirements. For example, the following are few important capabilities and constraints for Kafka. | Kafka provides support for event ordering based on partition keys. It also ensures that there is a single consumer (thread) listening on a partition. This makes it very easy to order events just by selecting an appropriate partition key. For example, OrderId, when used as an partition key, will ensure that all events related to a particular order will be processed in the order of their arrival. | Kafka supports idempotence for producers. This means Kafka ensures that an event is produced exactly once by a producer. Developers don’t need to worry about it. | Kafka provides at least once delivery guarantee. This means consumers should be able to handle duplicate messages. Developers need to be aware of the guarantees provided by their event brokers. | Another important aspect for Kafka is an offset-commit strategy for consumers, which means whether events should be automatically or manually acknowledged. If auto-commit is enabled, events that produce an error might get lost (if exceptions are consumed) or the consumer might see duplicate messages. Manual commits can be used to counter this, but it requires additional code. Frameworks such as spring-cloud-stream that work seamlessly with Kafka, provide the choice of not auto-committing in case of errors or moving the failed events to a DLQ in addition to manual/auto-commit. This is an important aspect that needs to be thought through during design. | Kafka Streams provides the ability to process event streams and easily perform various advanced and complex operations on event streams such as aggregations and joins. This makes it is very easy to perform analytics in real time. For example, computing real-time statistics of events grouped by various dimensions requires very minimal coding. These are stateful operations and maintain a state. Kafka also provides automatic fault-tolerance through state-stores. | . Security . Developers must consider these aspects of security in EDA-microservices architectures: . | Transport level security | Authenticated &amp; authorized access to event production and consumption | Audit trails for event processing | Data security (such as authorized access and encrypted storage) | Eliminating vulnerabilities in the code | Perimeter security devices and patterns | . Observability . Observability includes monitoring, logging, tracing, and alerting. Each component of the system should be observable to avoid failures and also to quickly recover from failures. Most of the EDA products and development frameworks provide support for observability by publishing metrics that can be exported into industry-standard observability tools such as Prometheus and Grafana, ELK, StatsD and Graphite, Splunk, or AppDynamics. For example, Apache Kafka provides detailed metrics that can be exported and integrated with most of these tools. Also, cloud platforms that offer managed services for an event backbone (IBM Event Streams) provide first class support for observability. Microservices development frameworks such as Spring or Camel provide good support for code instrumentation for monitoring. From an EDA perspective, instrumenting the code of producers and consumers for publishing metrics, publishing event broker metrics, and correlating these through a metrics dashboard is essential because the number of distributed components in EDA is high. Some of the key metrics from an EDA perspective are rate of incoming and outgoing messages, lag in consumption, network latency, queue and topic sizes, and so on. For monitoring microservices, refer to my article on Monitor Spring Boot microservices for a detailed tutorial on instrumenting and monitoring microservices. Fault Tolerance and Response . To provide adequate fault tolerance, the architecture needs to provide redundancy, exception handling, and elastic scaling (scaling up when thresholds are breached and scaling down when load returns to normal). With EDA and cloud, most of these can be easily achieved. Event backbones cater to fault-tolerance by supporting the clustering and replication of queues and topics. Producers and consumers can have multiple instances deployed. When deployed as containers on a Kubernetes platform, elastic scaling can be easily achieved through auto-scaling (using horizontal pod auto-scalers) but exception handling has to be designed for producers and consumers. Although EDA-based systems provide for resiliency through staged architecture, quick failure response and recovery is critical to avoid delays and consistency issues. To achieve this quick recovery, you need: . * Automation for starting and stopping instances and restarting failed instances, which can be easily configured in Kubernetes-based platforms, such as Red Hat OpenShift * Raising alerts and incidents as and when failures occur * A well-defined incident management process * Availability of logs and the ability to correlate logs across multiple components through tracing. Tracing needs to be enabled in microservices. Development frameworks such as spring-sleuth can be used for this. For log aggregation, tools such as ELK or Splunk can be used. This would help the team identify the root cause and resolve the issue quickly. References . | Architectural considerations for event-driven microservices-based systems | IBM Architecture Centre - Event Driven Architecture | . ",
    "url": "http://localhost:4000/githubPage/arch#ibms-pov-on-eda",
    "relUrl": "/arch#ibms-pov-on-eda"
  },"93": {
    "doc": "Target Operating Model",
    "title": "Target Operating Model",
    "content": ". How you operate contributes more to the ability to innovate and being agile for an enterprise than any technology you can adopt as an enterprise. Operating Models for Future Application Development are about locking in the value of your investments in cloud and exploiting them for growth. Cloud is more than a set of technologies, therefore successfully adopting and operating multicloud requires operating model transformation: . | Competitive disruption is driving new and changed business models enabled by technology that IT is asked to integrate, secure, and operate. | IT’s mission has been redefined to become a digital integrator across multiple business and IT platforms - a mission that requires a multicloud operating model. | Cloud technology cannot be separated from the organisational, cultural, and process transformation needed to use those technologies to drive desired business outcomes. | . Table of Contents . | Overview | What Do We Want To Be And What Do We Want To Achieve? | Capabilities Required &amp; Transition | Services Portfolio | Organization &amp; Functions | Manage and Control without Inhibiting Agility &amp; Flexibility | Financial Models | Digital Platforms &amp; Supply Chains | Innovation &amp; Agility in Delivery Approach | Agile Metrics, Processes &amp; Toolchain | . ",
    "url": "http://localhost:4000/githubPage/target-opt-model",
    "relUrl": "/target-opt-model"
  },"94": {
    "doc": "Target Operating Model",
    "title": "1. Overview",
    "content": " ",
    "url": "http://localhost:4000/githubPage/target-opt-model#1-overview",
    "relUrl": "/target-opt-model#1-overview"
  },"95": {
    "doc": "Target Operating Model",
    "title": "A Design Thinking, Persona-driven Approach to Operating Model",
    "content": "To capture the value of these investments, enterprises need to change how they operate with clients, employees, partners, and regulatory authorities to provide digital products in a safe, sustainable, consistent, quality and profitable fashion. Our Cloud Strategy Operating model examines how to look at the changes you will need to make in organizing for and executing new work to deliver Digital Products and Services in a much changed operating environment. Each operating model dimension must be aligned with multicloud. A multicloud target operating model is: . | Powered by cloud platforms that are full-stack operated with clear governance and a powerful IT ecosystem | Supported by skilled people, collaborative enterprise culture, and automated processes. | . | Target Operating Model Dimensions | . | | . Traditional business &amp; IT operating models are broken and require urgent transformation to meet demands of multicloud world. | Dimensions | Existing Way of Operating | Agile Cloud Way of Operating | . | Organisation | - Waterfall Product development – From business Teams handover requirements to IT dev to Ops | - Agile iterative product development - Business team roles become Product Owners working continuously with the DevOps teams | . | Governance | - Centralized governance boards focused on new project approvals, change control | - Offering management driving innovation funnel from IT with agile architecture embedded into development teams | . | Financial Model | - Corporation funds IT projects part of long term portfolios | - Corporation funding product and services lines with flexible usage based | . | Workforce Model | - Staff performance - position and salary driven by size of teams, size of budgets, duration- Skills and role/responsibilities of Technical professionals are legacy based often narrow in responsibility or Skill | - Staff position and salary aligned to knowledge and value creation- DevOps Team model to encourage technical capabilities collaboration- Site reliability engineering will require deep technical experience across platform technologies | . | Leadership &amp; Culture | - A formal demand vs supply relationship with the business- Risk averse, adopt tried and tested | - Co creation with the business and the partners- Empowerment for experimentation, learning, continuous improvement | . | Partnering &amp; Sourcing | - Out source / task 3rd party suppliers for dev, maintenance | - Co creation with suppliers, delivery lines, rebalance internal vs outsource | . | Digital Technology Platforms | - Teams build and develop applications and features to a monolithic architecture style, with staged releases | - Teams build with Microservices architecture approach, Containers and continuous feature releases- Agile engineering approach for co creation | . | KPIs &amp; SLAs | - Requirements and Solutions are business activity focussed | - Requirements and Solutions are Value Stream Outcome focussed, and on cloud Platform Service &amp; solutions | . | New ways or working | - Application Teams develop applications and handing over to Operations to run and maintain - Operations team control stacks and technology implementations often limiting choice for efficiencies | - Application teams become full stack Service Product lifecycle teams, providing support, and automation for “Less Ops” service management- Ops team take on the Multicloud management higher up the stack, rely on self directed DevOps teams owning product performance | . ",
    "url": "http://localhost:4000/githubPage/target-opt-model#a-design-thinking-persona-driven-approach-to-operating-model",
    "relUrl": "/target-opt-model#a-design-thinking-persona-driven-approach-to-operating-model"
  },"96": {
    "doc": "Target Operating Model",
    "title": "How Are Enterprises Organising Themselves?",
    "content": "This section provides a peek into how organizational structures are forming around new era of digital enterprise and application development. Modern IT functions are typically supporting the development of cloud enabled, digital products in several ways: . | 4 Pillars | . | | . | Center of Excellence - Drives adoption and usage of cloud services in a controlled, secure and consistent way . | Advise on cloud adoption to lines of business | Develop guardrails and standards for cloud consumption | Curate a catalogue of cloud services approved for use within the enterprise | Provide education and talent development | . | Platform Provider - Creates a platform of business &amp; technology services to support digital transformation . | Incubate the new technology platforms to be consumed by Digital/LoB teams | Act as a broker and integrator across hybrid multi cloud environments | Provide operational platform support | Enable DevSecOps toolchain and capabilities | . | Digital Product Co-creator - Partners with lines of business to compete successfully in the marketplace . | Operate on product lines towards new digital propositions | Operate agile end to end teams | Drive design thinking for new engineering practices | Experiment and scale fast | Full end to end service accountability | . | Integrated Service Management - Centralised command centre to protect live service and drive service excellence across the enterprise . | Level 1/1.5 support function | Command centre proactively monitoring services, enabled by analytics and automation | Provides real-time reporting on service performance | Drives service excellence across the enterprise for operations including Site Reliability best practices | . | . These elements interact together to provide an integrate eco-system to create and operate digital products. Within each of these elements, there are various functions that support product development and operations. | Target Operating Model - 4 Pillars | . | | . Center of Excellence . The role of the Centre of Excellence is to drive cloud adoption through governance, to build communities and advise on consumption and usage to meet business requirements: . | Sets guardrails and policies to provide choice and agility, without comprising security and compliance | Sets technical architecture and guidelines for development of products and services | Delivers education to product teams with line of business and cloud/infrastructure platform teams | Promote asset reuse, best practices and recommendations | Manages communities to encourage collaboration | Provide advice on sourcing and vendor selection | Deliver professional services to support and augment product teams | . Platform Provider . The role of Platform Provider (Platform Engineering &amp; Operations) is to develop and operate cloud, technology and infrastructure products for use across the enterprise: . | Develops and operates enterprise wide services technical services to be consumed by lines of business IT or value streams | Develops services on public and private cloud | Manage end-to-end lifecycle of platform services | Manage performance and capacity of platform services | Develop and operate shared tooling services for consumption across the enterprise | Providers data platform services for consumption across the enterprise | Develops and operates integration services including API publishing and management | Manages and maintains the core infrastructure including network, physical infrastructure and data centres | . Digital Product Co-creator . The role of Digital Product co-creator (Application Engineering &amp; Operations) is to develop and operate business and application services to deliver customer and business outcomes: . | Develops and operates business and application services | Focused on delivering business outcomes and creating new customer/consumer experiences | Help re-defining business process and workflows to deliver better outcomes | Work in cross-functional, agile teams working in conjunction with business users | Manages the end-to-end lifecycle of business and application services | Have overall end-to-end service responsibility | Consume and utilise products and services developed and operated by platform teams | Optimise business service/application performance and service levels | . Integrated Service Management . The role of Integrated Service Management is to provide centralised first line support, Operations Command Centre and driving service management excellence across the organisation: . | Provides level 1/1.5 support capability across applications and platforms | Operations Command Centre providing centralised monitoring and enabled by analytics and automation | Drive service excellence across the organisation for incident, problem and change management | Managing configuration and asset management across the organization to have a fully integrated view of all assets within the IT estate | Provide centralised real-time reporting and dashboarding capability on service performance | . ",
    "url": "http://localhost:4000/githubPage/target-opt-model#how-are-enterprises-organising-themselves",
    "relUrl": "/target-opt-model#how-are-enterprises-organising-themselves"
  },"97": {
    "doc": "Target Operating Model",
    "title": "What Is Required To Be Done?",
    "content": "Enterprises need to take a holistic look at the operating model by doing which, many of the key questions IT are faced with can be addressed: . | Target Operating Model Questions | . | | . Back to top . ",
    "url": "http://localhost:4000/githubPage/target-opt-model#what-is-required-to-be-done",
    "relUrl": "/target-opt-model#what-is-required-to-be-done"
  },"98": {
    "doc": "Target Operating Model",
    "title": "2. What Do We Want To Be And What Do We Want To Achieve?",
    "content": "| Target Operating Model Vision | . | | . Defining the vision for your organization, the services it will provide and the capabilities needed to deliver these, shaping how all other dimensions of the operating model need to be designed and the priority in which they need to be addressed. Defining from the outset a clear vision, mission &amp; set of principles is critical as it sets the direction, tone and shape of all other operating dimensions. ",
    "url": "http://localhost:4000/githubPage/target-opt-model#2-what-do-we-want-to-be-and-what-do-we-want-to-achieve",
    "relUrl": "/target-opt-model#2-what-do-we-want-to-be-and-what-do-we-want-to-achieve"
  },"99": {
    "doc": "Target Operating Model",
    "title": "Constructing a Vision, Mission and Principles",
    "content": "| Target Operating Model Vision | . | | . The two top layers of the detail are the ways in which vision and mission statements align to clients overarching strategy. Vision describes what we aspire to achieve e.g. Creating the very best digital products, enabled by flexible platforms and the best talent. Mission describles what we do and who we do it for. These mission statements are subsequently met through key transformation levers which describe how the vision and mission be achieved. The foundational layer introduces the key principles that guide the direction of transformation and underpin the way in which digital products are created and cloud is adopted within IT and adopted across the enterprise. ",
    "url": "http://localhost:4000/githubPage/target-opt-model#constructing-a-vision-mission-and-principles",
    "relUrl": "/target-opt-model#constructing-a-vision-mission-and-principles"
  },"100": {
    "doc": "Target Operating Model",
    "title": "Guiding Principles",
    "content": "The guiding principles set the direction and guardrails for operating model transformation. A strong principle is supported by a clear rationale and associated implications. | Target Operating Model Principles | . | | . A Guiding Principle is a high level statement of intent or purpose intended to guide and govern the design &amp; operation of the operating model. The Rationale highlights the business benefits of adopting the principle. These are directly related to the mission statements. Implications are tasks that must happen in order to successfully implement the principle to realise the associated benefits. Here is a set of principles that are emerging in transformative enterprises: . | Business Oriented - In everything we do, have what we are trying to achieve as a business and what delivers value at heart. | Innovation - Constantly innovate in the products we create, the way we work, looking for inspiration in how we can transform our business by embracing the latest technology. | Agile culture - Create an agile culture where everyone is looking to deliver value through every action we take and making that available to others. | Developer First - Put the developer experience at the forefront of what we do and enable them to create business value and digital products that delight our customers. | Self-Service - Create products and platforms that can be easy accessed, understood and consumed, in a self-service way wherever possible. | Sharing - Share for the benefit of your colleagues, teams and the enterprise through assets, practices, experiences, skills and support. | . Principles will have an impact on the design and elaboration across a range of operating model dimensions, such as the 2nd example above i.e. Innovation. | Principle . | Constantly innovate in the products we create, the way we work, looking for inspiration in how we can transform our business by embracing the latest technology. | . | Rationale . | Our industry is underthreat from new start-up market entrants and therefore we need to make sure we innovate what we do to ensure we are constantly providing the best customer experience. | We need to transform the way we think and work as an organistion to ensure we are always improving what we do to be more effective and efficient, focusing on what adds value. | . | Implications . | Ways of Working: Provide Design Thinking and User Journey training to everyone in the organisation | Workforce Model: Allow headroom to our colleague to tackle ‘pet projects’ | Financial Model: Provide funding for ‘sandpitting’ of new technology to every employee | Processes: Create a mechanism to showcase potential initiatives to receive innovation funding | Leadership &amp; Culture: Develop a culture where people are encouraged to think differently and beyond the everyday | Partnering &amp; Sourcing: Establish a regular forum for our partners to showcase their latest thinking and technology | Financial Model: Provide funding for colleauges to attend conferences and technology showcases | Governance: Develop and innovation forum as part of our governance structure | Locations &amp; Facilities: Create innovation spaces where people can collaborate outside of normal teams | . | . ",
    "url": "http://localhost:4000/githubPage/target-opt-model#guiding-principles",
    "relUrl": "/target-opt-model#guiding-principles"
  },"101": {
    "doc": "Target Operating Model",
    "title": "How Vision, Mission &amp; Goals effect the organizations in the enterprise",
    "content": "Over the course of the operating model transformation, each area of the organisation may elaborate their own set of principles in-line with the overall vision &amp; mission. | . Back to top . ",
    "url": "http://localhost:4000/githubPage/target-opt-model#how-vision-mission--goals-effect-the-organizations-in-the-enterprise",
    "relUrl": "/target-opt-model#how-vision-mission--goals-effect-the-organizations-in-the-enterprise"
  },"102": {
    "doc": "Target Operating Model",
    "title": "3. Capabilities Required &amp; Transition",
    "content": ". ",
    "url": "http://localhost:4000/githubPage/target-opt-model#3-capabilities-required--transition",
    "relUrl": "/target-opt-model#3-capabilities-required--transition"
  },"103": {
    "doc": "Target Operating Model",
    "title": "We can only see a short distance ahead, but we can see plenty there that needs to be done.",
    "content": "- Alan Turing . ",
    "url": "http://localhost:4000/githubPage/target-opt-model",
    "relUrl": "/target-opt-model"
  },"104": {
    "doc": "Target Operating Model",
    "title": "No one is less ready for tomorrow than the person who holds the most rigid beliefs about what tomorrow will contain.",
    "content": "– Wacker, Taylor &amp; Means . ",
    "url": "http://localhost:4000/githubPage/target-opt-model",
    "relUrl": "/target-opt-model"
  },"105": {
    "doc": "Target Operating Model",
    "title": "What Capabilities Are Required to Deliver Digital Products &amp; Pervices and Platform Services?",
    "content": "A broad range of capabilities are required to ideate, develop, support and control digital products and services throughout their lifecycle. | Engagement &amp; Demand: the capabilities needed to translate business strategy, needs and requests into products and platforms and ensure the right skills are available across the organisation. | Product Lifecycle Management: the capabilities needed to architect, develop and release both application and platform/technology products and services. | Fulfillment &amp; Provisioning: the capabilities required to make products and services easily consumable and rapidly available for use when needed. | Service Operations: the capabilities required to support and operate products and services and ensure they are performant, resilient and meet business and user service levels. | Governance &amp; Control: the capabilities required to support and operate products and services, and ensure they are performant, resilient and meet business and user service levels. | . Many of these capabilities cut across the organisation, but have a ‘centre of gravity’ of where the direction and approach for these is set. | Many of these capabilities will be required across various areas of the organisation and will play a role in developing and supporting digital products in an effective, flexible and agile way. | However, there is typically a ‘centre of gravity’ within the organization that helps set the direction, standards and process for how that should be done across the organisation. | Where this ‘centre of gravity’ resides can vary depending on the maturity of the organisation and the level of autonomy in which parts of the organisation are given. | . | Capabilities | . | | . ",
    "url": "http://localhost:4000/githubPage/target-opt-model#what-capabilities-are-required-to-deliver-digital-products--pervices-and-platform-services",
    "relUrl": "/target-opt-model#what-capabilities-are-required-to-deliver-digital-products--pervices-and-platform-services"
  },"106": {
    "doc": "Target Operating Model",
    "title": "Capability Evolution",
    "content": "The key question to ask is how these capabilities are changing for leading digital, platform and cloud enabled organisations? Some capabilities are reinventing themselves and some new capabilities are found to be necessary for optimization that is happening in the operating model of digital enterprises. Engagement &amp; Demand . | Aspects | Traditional | With Digital, Platforms &amp; Multicloud | Change Implication | . | Demand | Demand is planned, typically with tight controls &amp; constraints | Variable Demand can be readily supported with less constraint, integrated Bus DevOps |   | . | Digital Technology &amp; Services Strategy | Strategy focus is asset &amp; technology-centric. Adoption driven by vendor features on a cyclical basis | Continuous strategy focus is Business Value-Centric (Time to Value, User Experience). Greater focus on Technology adoption timing, experimentation pilots. End to end service execution can be displayed against strategic goals via dashboards with analytics. | Design Thinking applied to Technology Adoptions. Enterprise Architecture re-focus on Bus Architecture &amp; Business Service Components. Sense &amp; respond to Technology maturity changes | . | Demand Management | Focus is typically on IT and project driven demand, IT requirements or major business events. CAPEX as a constraint | Forecast in a different way, focus shifts to business usage behaviors &amp; needs for services. Use automation to detect patterns and capacity need changes. OPEX consumption estimating | Business Partner roles can now focus on strategic demand &amp; value outcomes. Estimate based on Pay-per-usage. Service Catalogs required | . | Product Introduction &amp; Roadmap | Focus is typically on IT and project driven IT requirements, staged releases | Agile based approach, with significant potential for frequent release cycles. But SoR integration mast be managed | New Methods to guide business product owners. New ways of working with IT teams, to plan for change. | . | Data Science and Analytics | Data is managed as a storage cost and constraint. Data not treated as a strategic asset | Data as a Service (DaaS) greatly increases the speed and the capacity to add data services with speed, agility, and with cost impacts transparent. Big Data management disciplines for Cognitive AI and Analytics consumption | Enterprise Architecture teams need to update approaches, patterns and controls to drive DaaS adoption and establish Data platform services | . | Customer Service Assistance | Traditional Helpdesk &amp; support staff, IT help pages | Helpdesk teams need to be able to manage incidents across cloud providers and IT teams. Greater focus on self-support and use of Virtual Assistance. Business Relationship / Account Management disciplines | New procedures, help scripts etc. Virtual Assistance and Robotic Process Automation for high frequency topics | . | Adoption, Communities &amp; Communication | Created in an ad-hoc way usually within organisational silos | Communities and tribes that cut across the organisation focusing on developing core skills, capabilities and collaboration needed to develop digital products and services | Develop clear communities with specialty and focus areas, create a culture of cross organisation working, incentivizing giveback to the organisation | . | Talent &amp; Knowledge Strategy | Traditional model | Skills are evolving rapidly, as cloud platforms and solutions evolve. Heavy reliance on external talent bases | Centre of Excellence approach and Joint Venture &amp; Collabration models to jump start capability change | . Product Lifecycle Management . | Aspects | Traditional | With Digital, Platforms &amp; Multicloud | Change Implication | . | Product Lifecycle Management | Release cycles are staged and somewhat constrained for the business | Cloud Solutions enable multi-speed IT and increased parallel changes |   | . | Architecture and Design | Complex multi-layer architecture planning skills required | Cloud platforms provide a foundation architecture, focus is on re-using existing legacy, and integrating new solutions. Shift to End User experiences | Legacy modernisation skills required. Design Thinking Methods. Ensure regulatory &amp; policy compliance is embedded in the process | . | Product Planning | Long cycle planning, with features needs to be added in elongated release cycles | Clear product ownership driving a priortised backlog that is aligned to changing business needs | More flexible budgeting and planning, with shorter cycle roadmaps dor development | . | Product Engineering &amp; Development | Build and Unit Test monolithic application code | DevOps with Microservices and container based approach. | New development methods &amp; DevOps discipline | . | Configuration &amp; Version Management | Typically a pain point across AD &amp; infrastructure, and rarely well automated | Stronger focus on Config &amp; Version management across environments, enabled through technology. Multi-concurrent versions can be enabled AB Testing | Re-training of staff, tool changes and new working disciplines. | . | Product Testing, Packing &amp; Deployment | Build and Test in dedicated environments – traditional staged approach | DevOps approach: Test with containers and in Production partitions. A/B Testing of features. Service integration experience testing focus (eg: single-sign on). Operations testing for risk | Streamlining the process &amp; automation to avoid quality being a bottleneck. Ensure regulatory &amp; policy compliance is embedded in the process | . | Release &amp; Change Management | Release cycles are staged and somewhat constrained for the business | Business gain potentially radically variable and fast feature deployment cycles. Automation to drive change control compliance - DevOps | Release &amp; deployment management process changes. Controls change. Empowerment of ADM teams - DevOps | . Fulfillment &amp; Provisioning . | Aspects | Traditional | With Digital, Platforms &amp; Multicloud | Change Implication | . | Service Access &amp; Orchestration | Mixed models | Self Servicing, catalog based approach.Single-sign-on globally | Need for single sign-on &amp; catalog capability across multi-clouds. Corp Policy to manage authorised cloud services, security access rights and self-service clip levels. | . | Platform Environment Provisioning | Developers face long wait times to secure access to IT environments | Developers can self-serve IT environments, improving quality | New policies and processes for developers, and for IT environment managers | . | Data Provisioning | In-house online install catalog | Application Store &amp; Continuous updatesContainer based approach - Cloud native applications and DB’s are provisioned in to the cloud environment mirroring dev and test environments reducing errors | Current personal supporting provisioning services will have greatly reduced demand. | . | API &amp; Integration Management | Application &amp; Data Integration requires centralised IT teams, intensive effort &amp; bottlenecks in lifecycle | Open approach to Application &amp; Data Integration, decoupled, fewer bottlenecks | Application Development strategy change. API Lifecycle management processes. | . | Capacity Management | Capacity pre-planned and constrained by In-house or Supplier Technology Stacks | Capacity practices alter - depending on the cloud model (Eg: public vs private, on-premise). Dynamic capacity and self-service needs to be controlled to avoid run-away costs, and minimise wastage | New capacity planning and management practices according to IT deployment model. Active lifecycle approach required to reduce ‘Zombie’ accounts and Inactive capacity | . | Asset &amp; Information Management | Assets and Information controlled centrally or dedicated provider. | Asset &amp; Information sit across Cloud Providers - focus of IT is to regulate, audit &amp; optimise. | Change to policies and processes. Use of technology (APIs) to automate data collection. Cost management disciplines for data archive and retrievals | . | Talent Acquisition, Management and Training | Training Courses &amp; CertificationsPower User support mechanisms | Continuous Just In Time &amp; experiential learning.Agile communities | Garage Days &amp; Hackathons.Center of Excellence modelsSocial collaboration enablers | . Service Operations . | Aspects | Traditional | With Digital, Platforms &amp; Multicloud | Change Implication | . | Service Operations | Focus on prioritising issues and problems, housekeeping and maintenance | Focus on continuous operation, prevention and containment, continuous tech refresh |   | . | Customer Service Level Management | Standardised SLA models, controlled in-house | SLAs vary across cloud providers. Service integration through Operating Level agreements &amp; automation | Policies need to be established, particularly for mission critical business processes, to avoid business performance issues | . | Platform Performance Management | Performance is internally controlled, but also constrained by pre-build technology stacks | Performance shifts to cloud providers and platform integrators, with varying characteristics. Network latency as a typical constraint. | IT need to adapt monitoring across multicloud providers with a focus on latency issues and new technology standards (eg: containers) | . | Availability &amp; Service Reliability Engineering | IT Support staff model – System Administrators and DBAs. IT manages availability to SLAs, with a focus on scripts, maintenance windows and recover | Prevention based practices – Site Reliability Engineers. Automation to continuously detect, prevent and recover. | Site Reliability Engineering skills and IT Self-Healing script automation. | . | Services &amp; API Event Monitoring | IT incidents handled through predictable supplier based models | Strong focus on prevention, root-case analysis and recovery - DevOps Site reliability Engineering, event management, and skills at Helpdesk level. Increased complexity | Processes need to robust for resolving problems &amp; issues in timely manner. Strong root cause skills | . | Incident &amp; Problem Management | ITIL based support approach | Emphasis on early detection and prevention, auto-recovery | Embed Site Reliability Engineering. Auto-recovery, Event Analytics. Integrate with legacy operations | . | Continuous Improvement | Done through lessons learnt at the end of major programmes or initiatives | Ingrained as part of the fabric of working, with a culture that seeks out improvement. SRE capability constantly seeking to automate and reduce TOIL | Clear metrics to measure improvements, culture and incentives to delver improvements | . | Security &amp; Business Continuity | Security enablers to prevent vulnerability (eg; virus scan). Manual fall backs for continuity | Continuous threat posture, with security operating across boundaries, detecting trends and weakness ahead of major events. Dynamic continuity changes &amp; Robust fallback | Automation &amp; AI to pre-empt the risk. Platform standards to minimize variation vulnerability | . Governance &amp; Control . | Aspects | Traditional | With Digital, Platforms &amp; Multicloud | Change Implication | . | Policies &amp; Standards | IT Policies are Asset and technology Centric, for internal IT teams. Policies and standards are relatively static | Cloud IT Policies &amp; standards are utilized by the Business, suppliers and external users, enforced by central governing teams. Standards are platform and As-A-Service centric. Standards continuously evolve with new Technology | Policies &amp; Standards to enable greater Self-Service, by external parties (Eg: APIs), and for platform standardization. Focus on continuous updates | . | Financial Planning &amp; Cloud Economics | Fixed period planning cycles (annual, per project), with significant investments for asset refresh | The IT service framework now prices IT Services incrementally from the IT vendors to get accurate Operational Expense forecasting. Consumption driven planning with Business Product Owners | IT budget planning processes change. Project estimating &amp; planning process changes. Budget allocation &amp; chargeback models change, aligning to Business Product Owners - need to be handled carefully. Handling business case viability challenges | . | Partnering &amp; Sourcing | IT supplier based models. Value assessed on asset basis. Long procurement cycles | “Vendors” are replaced with partners who are tied in seamlessly to the essential delivery of IT services. Value assessed on business outcomes. Rapid proof of value procurement cycles | Procurement and sourcing business rules need to be modified to reflect the new opex nature of cloud operations. | . | Portfolio Management | PO is driving mixed model of CAPEX programmes &amp; project releases. Traditional PPM | Project staffs will plan projects incrementally and measure value for incremental progress. ROI metrics will be easier to acquire and justify. | Training of PMO staff’s in agile management techniques is required. Governance changes required | . | Information &amp; Security control | Internal controls are managed against assets and | Security polices and controls are proven for each cloud service fielded. For industries with regulatory concerns, security works ahead of ahead of new imitative to show the impact of cloud tools and procedures. | Security Organizations need extensive education/exposure to how cloud operations should be tailored for security polices and control, | . | Risk, Audit and Compliance | Control is managed centrally, and controls are typically transparent | Cloud Service Provider contracts assure compliance. Understanding the compliance and regulations for using public cloud providers requires clients working with their providers to validate their deployment patterns. | Proactively work with compliance authorities to implement new policies and control’s - but may stretch the talent and experience of incumbent personnel, such as for GDPR issues. | . ",
    "url": "http://localhost:4000/githubPage/target-opt-model#capability-evolution",
    "relUrl": "/target-opt-model#capability-evolution"
  },"107": {
    "doc": "Target Operating Model",
    "title": "How Do We Transition?",
    "content": "Assessing the maturity of these capabilities helps determine the areas for focus of operating model change and sets a baseline for measuring improvement. Undertaking a maturity assessment across these capabilities provides a number of benefits to defining the future operating model transformation . | Bring visibility to current practices adoption | Identifying and prioritise gaps in existing capabilities | Expose pain points | Provide a future view of ambitions and goals with to help focus capability transformation | Structure the transformation priorities | Target quick wins and early proof points | . Additionally, this maturity assessment case act as a ‘yard stick’ to measure progress of the operating model transformation, alongside other KPIs that help support the measurement of each capability. | TOM Capability Maturity | . | | . Back to top . ",
    "url": "http://localhost:4000/githubPage/target-opt-model#how-do-we-transition",
    "relUrl": "/target-opt-model#how-do-we-transition"
  },"108": {
    "doc": "Target Operating Model",
    "title": "4. Services Portfolio",
    "content": " ",
    "url": "http://localhost:4000/githubPage/target-opt-model#4-services-portfolio",
    "relUrl": "/target-opt-model#4-services-portfolio"
  },"109": {
    "doc": "Target Operating Model",
    "title": "What Services Does The Next Generation IT Organisation Offer?",
    "content": "The service portfolio outlines what is offered, developed and supported across the organisation using a common taxonomy, language and service definition. It covers all products and services across the organisation in a clear and transparent way. Why is it important? . | Provides clarity on what products and services are offered, develop and managed within the organisation and by suppliers, with identified internal product owners | Clearly defines the role between and application and platform engineering teams and establishes clear ownership | Creates clear line of sight between business services and underlying technology simplifying supportability and operations | Helps avoid duplication of functionality and accelerates development through patterns that are architected and built with the necessary security and compliance approval | Make services more consumable by describing them in a common way, make costs transparent, presenting in a curated catalogue and automating access and provisioning where possible | Helps manage the lifecycle of services driving investment decisions, architectural choices and service strategy | . | Target Operating Model - Service Portfolio | . | | . ",
    "url": "http://localhost:4000/githubPage/target-opt-model#what-services-does-the-next-generation-it-organisation-offer",
    "relUrl": "/target-opt-model#what-services-does-the-next-generation-it-organisation-offer"
  },"110": {
    "doc": "Target Operating Model",
    "title": "Key Principles for Establishing Service Porfolios",
    "content": "| Principle | Description | . | Single &amp; Hybrid Offering | - The same offering must be delivered everywhere.- Leverage capabilities from Private and Public Cloud Providers and facilitate Exit Strategies. | . | Technology Agnostic | - Focus on the abstraction and the interface defined by the service and the value which gets delivered rather than particular implementations that might be supporting it. | . | Standardized &amp; Prescriptive | - Consumer applications and services rely on the set of standardized set of Cloud Platform Services that have been published for each customer.- Each service might require a number of other mandatory service components (either by policy or as part of a bundle) in order to meet other key objectives and requirements. | . | Managed &amp; Platform-centric | - As opposed to Workload centric offering, a Platform-centric offering provides Cloud Services that presents building blocks that can be used by consumer applications and services.- A managed Cloud Service Portfolio must ensure and deliver: - Value that contributes to fulfill the vision and the strategy - Enterprise grade SLAs. | . | Differentiated | - A Single, Managed, Secure and Hybrid Multicloud offering presents a distinctive value chain that goes way beyond the one obtained by just doing a pass-through of technology proposals from different CSPs. | . | Compelling &amp; Competitive | - The Service Offering must be compelling and competitive so that customers are willing to adopt it - The CCC must make sure the solution proposed as a Service Offering is cost effective and cost efficient. | . | Agile | - The Service Definition process must follow a regular cadence of small, frequent and incremental iterations. - Prioritization efforts must take into consideration a positive feedback loop with customers and prospects. | . Back to top . ",
    "url": "http://localhost:4000/githubPage/target-opt-model#key-principles-for-establishing-service-porfolios",
    "relUrl": "/target-opt-model#key-principles-for-establishing-service-porfolios"
  },"111": {
    "doc": "Target Operating Model",
    "title": "5. Organization &amp; Functions",
    "content": ". | 4 Pillars | . | | . You have earlier seen how enterprises are organising themselves in the digital and multicloud world and the four kind of organizations thus forming. Within each of these elements, there are various functions that support product development and operations: . | 4 Pillars Function | . | | . ",
    "url": "http://localhost:4000/githubPage/target-opt-model#5-organization--functions",
    "relUrl": "/target-opt-model#5-organization--functions"
  },"112": {
    "doc": "Target Operating Model",
    "title": "Centre of Excellence",
    "content": "Strategy, Architecture &amp; Demand . Purpose . | Provides overall direction by being responsible for aligning both cloud and IT strategy with the broader enterprise strategy. | Owns and iterates a “to-be” cloud vision, and provides governance to ensure projects, services and products are operated in a standardised and strategical aligned method. | Defines and maintains the Enterprise’s cloud strategy, policy and architectural references (including integration, infrastructure, software &amp; licenses and disaster recovery). | Collects new requirements for functionality to be implemented. | Acts as a consultant and/or advisor for Lines of Business expected to consume its products and services, consulting on delivery and best practices associated with cloud technologies. | . Core Activities . Responsible and Accountable for: . | Continuously reviewing and communicating cloud strategy against business strategy objectives | Defining and maintaining cloud reference architecture, patterns, principles and standards for the Cloud Platform | Managing cloud architecture governance on behalf of the Centre of Excellence | Developing and maintaining guidelines and policies for cloud integration, cloud infrastructure, configuration management, software &amp; license management and disaster recovery | Defining and maintaining target “to-be” cloud estate and transition states for cloud implementation | Understanding customer needs and validating solutions from customer perspective. | Translating business requirements into Solution Design Documents and initial product or service roadmap (implemented by AEO/PEO) | Monitoring and tracking new business requirements and technology approaches based on demand | . Consulted on activities linked to Platform Engineering Operations, Application Engineering Operations, and Integrated Service Management. Roles . | Centre of Excellence Lead — Define and agree Vision, Mission, and Goals for Cloud Strategy Operating Model — in line with Business Outcomes set by Enterprise C-Suite—and drive alignment of IT products &amp; services with consumer demands and BoB innovation | Architecture Squads [Chief Solution Architect; Cloud Platform Architect; Transformation SRE; Security Consultant; DevSecOps Engineer] — Determine cloud architecture strategy (and associated policies and standards) and data stewardship standards in tandem with enterprise architecture strategy; Defines the service integration strategy and architecture for the enterprise; Drive alignment for a full stack product or service solution that meets customer demand | . Adoption &amp; Communities . Purpose . Collects new requirements for functionality to be implemented into the Centre of Excellence, Application Engineering &amp; Operations, Platform Engineering &amp; Operations, and Integrated Service Management. Acts as a consultant and/or advisor for Lines of Business and Delivery on best practices associated with cloud technologies, application modernisation and application migration. Core Activities . Responsible and Accountable for: . Communications Strategy and Marketing . | Continuously develop and communicate the vision to Enterprise stakeholders | Determine marketing communication methods and frequency  | . Methods and Knowledge . | Develop, maintain and share relevant industry and business knowledge | Maintain and create standard development methods to share across all areas of the business | Share information on existing cloud capabilities to Enterprise | Implement, update and maintain mechanisms for cloud knowledge management | Maximise reuse by sharing of solutions across AEO and PEO teams | . Cloud Transformation/Migration Support (as needed) . | Support the migration to Cloud of the legacy IT estate (where appropriate) by defining the approach and principles appropriate for an application centric migration | Perform cloud affinity assessment &amp; workload analysis for each service/application proposed to move to the Cloud | Run cloud affinity and workload analysis for all departments/sectors/businesses within the organisation | Create cloud transformation roadmap based on affinity, workload and difficulty ratings. | Provide estimates for modernisation of applications | . Roles . | Centre of Excellence Lead – Leads cloud within the Enterprise | Cloud Ambassador - Acts as a consumer advocate for cloud services | Cloud Engagement Manager - Interfaces with the Lines of Business to understand and interpret business needs | . Education, Talent, and Skills . Purpose . Manages resourcing, skills assessments, upskilling &amp; development and the arrival of expertise at the right time. Leverage existing talent within the Enterprise, procure talent outside the Enterprise, and upskill individuals with skills that best align to business and IT needs. Core Activities . Responsible and Accountable for - . | Analysing current team skills and  skill gaps | Leveraging partner organisations to augment roles within Organisation as required | Establishing re-skilling initiatives | Establishing initiatives, processes and policies to capture and retain cloud talent | Establishing cloud enablement education programs, workshops and seminars | Mapping cloud education and training to roles and responsibilities | Establishing cloud re-skilling programs | Conducting training of user communities and IT support resources | Educating relevant stakeholders in their responsibility for compliance | Organising and managing community events | Integrating external community partner services into portal | Setting up partner external community relationship | . Roles . | People and Talent Lead – Create business case for talent (upskilling of existing talent, acquisition of new talent, cross-skilling of talent, etc) to align with evolving needs of business delivered by the IT organisation; Own relationship between HR and the IT organisation to ensure both are tightly coupled | Cloud Education Specialist – Drives cloud operational efficiency by building cloud training offerings | Talent Acquisition Manager – Coordinate and drive acquisition of talent in line with business skillset requirements | . Security, Risk &amp; Compliance . Purpose . Provides confidence to Enterprise that IT products and services meet the regulatory and compliance requirements as outlined by geographical regions, industry bodies and Enterprise policy. Outline, review, iterate and communicate controls in line with the agreed organisational risk and compliance framework. Provide SecOps function with confirmation that cloud Platform and associated cloud products and services comply with Enterprise security rules and standards. Set, manage, and report on all security standards whilst assessing new and existing projects and tooling for compliance with regulation. Core Activities . Responsible and Accountable for: . | Defining application, network, data, identity and access management, and system security guidelines and policies (in line with best practice for cloud products &amp; services) | Understanding regulatory and confidentiality requirements and policies and add into products and services backlogs | Documenting the internal and external context of each security, privacy and data protection risk, and the evaluation criteria | Reviewing and monitoring physical and logical data security measures | Reviewing and monitoring application security controls | Monitoring network intrusion detection data | Creating and maintaining the policies and supporting controls for cloud under enterprise risk management framework | Managing, maintaining and reporting on compliance policies, standards and processes (following predefined cadence) | Determining and understanding the relevant regulatory and audit requirements | Embedding ownership and responsibility for IT-related risks within the business at an appropriate senior level | Identifying and evaluating risks related to information security, cloud project management, cloud sourcing, etc. | Identifying and prioritising projects to enhance cloud compliance and to remediate identified issues/risks | . Roles . | Cloud Auditor - Ensures Cloud environment compliance is being performed according to the agreed and documented processes | Cloud Service Security &amp; Risk Manager – Implements security and compliance policy | . ",
    "url": "http://localhost:4000/githubPage/target-opt-model#centre-of-excellence",
    "relUrl": "/target-opt-model#centre-of-excellence"
  },"113": {
    "doc": "Target Operating Model",
    "title": "Platform Engineering &amp; Operations",
    "content": "Brokerage &amp; Orchestration . Purpose . Ensures the commercial and/or contractual arrangements associated with the third parties (e.g., ATOS, cloud vendors, etc.) that are part of the provision and implementation of cloud-related products and services are strategically planned and managed such that they deliver maximum value, low risk and are financially beneficial to the Enterprise. Core Activities . | Defining and implementing a category strategy for procuring cloud services (i.e. vendor management strategy) | Defining and running the partner qualification, selection and management approach | Managing active/ongoing contracts | Procuring/purchasing cloud services &amp; products | Providing financial guidance for funding requests, procurement, and business case development for cloud services | Developing and operating the financial model and process for cloud products and services | Publishing &amp; maintaining agreed cloud service pricing models | Providing a centralised billing capability and aggregating and reconciling bill for each cloud customer | Establishing CSOM business accounting and funding mechanisms | Creating and maintaining cloud budgets | Providing financial management of cloud assets | Developing and managing accounting controls and standards | Establishing and operating financial reporting framework including the monitoring of all costs and variances | Negotiating relationships between providers and cloud consumers | Developing standard cloud customer contracts and relationship management approaches | . Roles . | Cloud Pricer - Manages cost of Cloud services offering | Cloud Service Provider Management - Negotiates relationships between cloud providers and cloud consumers. Interfaces with external service suppliers to support purchasing and contract management | Cloud Financial Manager - Manages and enforces all aspects of CSP cost and consumer pricing | Cloud Marketplace Manager –Manages the marketplace evolution and ensures access to the services for end users | . API &amp; Integration Management . Purpose . Publish, promote and oversee application programming interfaces in a secure, scalable environment, including end user support resources that define and document the API. Create business agility to drive rapid business reconfiguration (i.e. to revamp customer experience, address regulatory challenges (GDPR), etc.). Provide ease of use for APIs. Track and manage APIs in use. Build a framework for enforcing agreements on API use and security access control. Core Activities . | Planning, design, implementation, testing, publication, operation, consumption, maintenance, versioning and retirement of APIs | Elaborating channel IT design requirements | Delivering a developer portal through which to target, market to and govern communities of developers who embed APIs | Conducting runtime management | Enforcing relevant API security policies and requests and also guarantees authorization and security.  | Estimating APIs’ value | Using analytics to understand patterns of API usage [i.e. monitor API usage, load, transaction logs, historical data and other metrics that better inform the status as well as the success of the APIs available]  | Informing developers when an API is scheduled to be retired or deprecated, and what alternative APIs might be offered (frequently with a new pricing plan) | . Roles . | Cloud Service Integration Architect—Defines the service integration strategy and architecture for the enterprise | Cloud Service Composer &amp; Developer—Creates/bundles products and services for end-user consumption | Cloud Service Integration Engineer—Technical focal point for cloud service integration within the IT ecosystem | . Infrastructure Management . Purpose . Maintain legacy estate (data centres, hardware, networks, mainframe, etc.) to ensure access and performance are uncompromised while exploring areas for transformation in alignment with business transformation goals and strategic priorities. Core Activities . | Managing physical infrastructure platform according to internal service level agreements | Managing physical capacity expansion in-line with growth forecasts | Managing physical assets usage optimisation and currency | Managing to policy compliance | Conducting server configuration, server commissioning and decommissioning, server and backup management, server management remote and onsite, desktop management, database management, and network management LAN/WAN | Understanding implications/constraints of enterprise application and infrastructure architectures | Determining integration requirements between IT services and solutions | Identify opportunities for shared capabilities at a service/solution implementation level | Developing and maintaining service and solution architecture | . Roles . | Infrastructure Manager—Manage existing core infrastructure to maintain performance and availability | . Platform Product Engineering and Management . Purpose . Design, develop, test and deploy cloud platform to best align with evolving marketplace demands for cloud products in coordination with Application Development Squads. Ensure platform is built in line with security, risk, and compliance policies and standards and adheres to standards around capacity, availability, and reliability. Perform proactive and reactive management of platform products. Core Activities . | Defining platform architecture specific to the applications (in consultation with App Dev Squads) | Designing development tools environment changes specific to the applications (in consultation with App Dev Squads) | Configuring DevOps environments, tools &amp; automation frameworks for the development of platform | Leveraging DevOps &amp; Automation to build &amp; deploy the platforms needed for common services | Ensuring reliability is built into common platform | Proactively managing and continuously improving the platform | Developing and deploying automation and other best practices to address opportunities | Performing reactive management of the platform(s) to the required SLAs and contractual agreements | . Roles . | Platform Build Squads [Cloud Platform Architect; Platform Security and Quality Assurance; Platform SREs; Platform Specialists; Platform Developer (patterns/automation)] — Build and manage platform services in Agile manner | Shared Platform Squad — DevOps and Tooling — jointly design the development tools environment changes specific to the application group, leveraging common services | . ",
    "url": "http://localhost:4000/githubPage/target-opt-model#platform-engineering--operations",
    "relUrl": "/target-opt-model#platform-engineering--operations"
  },"114": {
    "doc": "Target Operating Model",
    "title": "Application Engineering &amp; Ops",
    "content": "Application Product Engineering Squads . Purpose . Innovate, design, develop, test and deploy an application (in Agile teams) to align with evolving marketplace demands for cloud products. Coordinate with Lines of Business, in alignment with a particular Value Stream (e.g.) to define and manage information throughout the product lifecycle. Leverage cutting edge technologies and ways of working to remain tightly coupled with delivering products that delight customers. Core Activities . | Understanding consumer demands and trends to ensure product development properly addresses needs in the marketplace | Translating consumer needs into requirements and own lifecycle for application (in consultation with Platform Squads, as applicable) | Developing common services to meet the functional requirements, following design created by architects | Ensuring non-functional requirements are met and Build to Manage principles are followed | Leveraging DevOps &amp; Automation to build &amp; deploy products | Proactively managing and continuously improving the applications | Developing and deploy automation and other best practices to address opportunities | Maintaining the functional elements of applications | Maintaining non-functional elements (in consultation with Platform SREs) | . Roles . | Application Development Squads [Application Developers; Application Security and Quality Assurance; Application SREs]—Build and manage applications in Agile manner | User Researcher—Research marketplace demands to ensure aliment with existing product portfolio; Identify areas for new products based on evolutions in the marketplace | User Experience Designer—Design products based on insights from UX in consultation with App Dev Squads | DevOps Transformation Lead Engineer—Coordinates DevOps practices definition and adoption | . ",
    "url": "http://localhost:4000/githubPage/target-opt-model#application-engineering--ops",
    "relUrl": "/target-opt-model#application-engineering--ops"
  },"115": {
    "doc": "Target Operating Model",
    "title": "Integrated Service Management",
    "content": "Customer Assistance . Purpose . Own and manage relationship with consumers of cloud products and services to ensure consistent, high quality interactions. Monitor quality using OKRs and report results to stakeholders to visibility into quality on regular basis. Core Activities . | Owning and managing relationship with consumers of cloud products and services | Managing day to day interactions with customers (internal and external) | Ensuring high quality customer interactions via comprehensive monitoring and reporting | Monitoring quality using OKRs agreed by Cloud Customer Support &amp; Care resource | Liaising with Operations Command Centre on resolution of complex customer service situations | . Roles . | Cloud Customer Support &amp; Care –Assists with usage and change management as first point of contact | . Operations Command Centre . Purpose . Owns the operation of the cloud platform and ensures its alignment with standard Enterprise operating procedures. Implement and operate Enterprise cloud platform services, enabling accelerated delivery of new cloud products and services in a controlled and secure manner. Ensure the guidelines and policies for operations are clearly defined and maintained. Core Activities . | Forecasting business volumes for cloud services | Providing Cloud User and Account Management | Conducting compliance/risk monitoring and reporting (upon consulting Risk &amp; SecOps) | Monitoring and managing risks related to ongoing cloud activities based on cadence identified in the Enterprise Risk Framework (including information security, cloud project management, and cloud sourcing) | Developing and maintaining Cloud infrastructure Release and deployment guidelines and policies | Developing and maintaining Configuration management guidelines and policies | Developing and maintaining software and license management guidelines &amp; policies | Developing back up and archive strategy and supporting guidelines &amp; policies | Developing and maintaining DR management guidelines &amp; policies | Developing event management strategy, guidelines and policies | . Roles . | Cloud Operations Manager –Owns daily operational activities | First Responder –Ensures rapid response to service level breaches | Site Reliability Engineer –Executes and optimizes automated operations | Cloud Operator –Supports cloud products &amp; services | . Service Excellence . Purpose . Monitoring and managing service to ensure quality is aligned with Service Management Strategy. Action changes in service (at either a strategic or tactical level) in accordance with Customer Service quality ratings. Monitor and manage service quality to ensure quality is sufficiently high enough to meet customer expectations. Core Activities . | Developing and maintaining KPI data collection guidelines | Developing and maintaining checklist of introduction of infrastructure to production | Understanding implications/constraints of enterprise applications | Assisting cloud migration execution (provision new environment, validate compliance, prepare tools, create runbook) | Providing support for early operational problems | Defining processes for resolving service performance issues and proactively identifying improvement opportunities | . Roles . | Cloud Service Manager –Owns daily operational activities | . Back to top . ",
    "url": "http://localhost:4000/githubPage/target-opt-model#integrated-service-management-1",
    "relUrl": "/target-opt-model#integrated-service-management-1"
  },"116": {
    "doc": "Target Operating Model",
    "title": "6. Manage and Control without Inhibiting Agility &amp; Flexibility",
    "content": " ",
    "url": "http://localhost:4000/githubPage/target-opt-model#6-manage-and-control-without-inhibiting-agility--flexibility",
    "relUrl": "/target-opt-model#6-manage-and-control-without-inhibiting-agility--flexibility"
  },"117": {
    "doc": "Target Operating Model",
    "title": "Overview",
    "content": "The modern Cloud Strategy Operating Model Governance model ensures that all four of its IT elements (i.e. Cloud Centre of Excellence, Application Engineering &amp; Operations, Platform Engineering &amp; Operations, and Integrated Service Management) play an integral part in the successful delivery and realisation of strategic initiatives, enabled through IT cloud capabilities and services. | Governance | . | | . This document describles the necessary forums that are required to determine how: . | Digital products and services are being adopted and used via standardised frameworks and services. | Operations of capabilities and services are performing. | . The Governance model will span across three accountability levels i.e. Strategy, Management and Operations — and will include board level decisions in Exec Committees through to day-to-day management decisions, which will in turn drive Operational delivery. An accountability level characterises the scope and intent of the activity and decision-making. This cuts across all four IT Elements of a modern operating model. | Governance Pillars | . | | . Structured within a hierarchical model, each governing Board or Committee adds distinct value to the Enterprise: . | Governance Structure | . | | . But how does this work in practice? . | Governance Boards | . | | . “Good” governance model is comprised of four main component: . | Structure | Active Oversight Responsibilities | Talent &amp; Culture | Infrastructure | . ",
    "url": "http://localhost:4000/githubPage/target-opt-model#overview",
    "relUrl": "/target-opt-model#overview"
  },"118": {
    "doc": "Target Operating Model",
    "title": "Structure",
    "content": ". | Outlines board and management committee structures, mandates, membership, and charters | Establishes design of governance framework | Delineates organizational structure, reporting lines, and relationships | Highlights roles and independence of control and support functions from business owners | . ",
    "url": "http://localhost:4000/githubPage/target-opt-model#structure",
    "relUrl": "/target-opt-model#structure"
  },"119": {
    "doc": "Target Operating Model",
    "title": "Active Oversight Responsibilities",
    "content": ". | Outlines the type of committees (board and management) and associated responsibilities | Specifies functional accountabilities for day-to-day management of business practices across the enterprise | Delineates board and management policies supporting delegation of authority (decision rights) including reporting, escalation, and veto rights | . ",
    "url": "http://localhost:4000/githubPage/target-opt-model#active-oversight-responsibilities",
    "relUrl": "/target-opt-model#active-oversight-responsibilities"
  },"120": {
    "doc": "Target Operating Model",
    "title": "Talent &amp; Culture",
    "content": ". | Aligns governance with operating and business principles | Articulates core beliefs and foundation for culture | Highlights characteristics of risk culture | Outlines leadership success, assessment, and development responsibilities | Aligns performance management, approach, measures and responsibilities to compensation and incentive plans | . ",
    "url": "http://localhost:4000/githubPage/target-opt-model#talent--culture",
    "relUrl": "/target-opt-model#talent--culture"
  },"121": {
    "doc": "Target Operating Model",
    "title": "Infrastructure",
    "content": ". | Establishes design and content of policy manuals and associated procedures | Outlines type and frequency of internal reporting and communications | Defines scorecards, measures, and metrics to track performance | Aligns technology and governance requirements | . Back to top . ",
    "url": "http://localhost:4000/githubPage/target-opt-model#infrastructure",
    "relUrl": "/target-opt-model#infrastructure"
  },"122": {
    "doc": "Target Operating Model",
    "title": "7. Financial Models",
    "content": " ",
    "url": "http://localhost:4000/githubPage/target-opt-model#7-financial-models",
    "relUrl": "/target-opt-model#7-financial-models"
  },"123": {
    "doc": "Target Operating Model",
    "title": "Overview",
    "content": "Financial models for IT are evolving too, in line with the evolving digital enterprise and how they operate. Here are some of the key attributes of a suitable financial model to enable the enterprise to function effectively as an innovative organization and with agility: . | Asset light - Shifting from CAPEX to OPEX and dynamic accounting models | IT Spend as a % of Customer Growth &amp; Revenue Potential, ARPU | Consumption driven IT economics | Benefit &amp; Value driven, not cost constraint decisions | Committed investments to transform, start-up seed funding to innovate | Optimise to Re-Invest | . ",
    "url": "http://localhost:4000/githubPage/target-opt-model#overview-1",
    "relUrl": "/target-opt-model#overview-1"
  },"124": {
    "doc": "Target Operating Model",
    "title": "Cloud Economic Considerations",
    "content": "An enterprise can generate value for itself from cloud by a combination of optimizing its IT infrastructure &amp; operational cost and innovating its way to new revenue that it could not realize earlier. | Cloud Economics | . | | . | Efficiently Utilize Cloud Resources . | Proactively identify and eliminate unused resources | Identify and optimize idle resources | Right size resources for efficient cost and performance | . | Operational Optimization of Cloud Resources . | Leverage dynamic auto scaling for cost optimized performance | Identify opportunities for movement to lower cost instances/families | Optimize and reduce operational costs of data resources | Balance performance and costs of cloud resources | . | Financial Governance Mechanisms for Cost Management . | Structural governance: Evaluate cost implications during design | Role governance: Determine the access, provisioning and budget authority for defined cloud roles | User governance: Create “menus” of available patterns for each role with cloud provisioning authority | Quality governance: Audit effectiveness and adherence to governance across all three levels | . | Pricing, Reserving and Discounting of Cloud Resources . | Evaluate bulk pricing options for potential discounts prior to provisioning | Monitor use of cloud resources against purchased discounts | Utilize automated recommendations and alerting to identify opportunities for bulk purchases based on usage trends | . | . ",
    "url": "http://localhost:4000/githubPage/target-opt-model#cloud-economic-considerations",
    "relUrl": "/target-opt-model#cloud-economic-considerations"
  },"125": {
    "doc": "Target Operating Model",
    "title": "Value Levers",
    "content": "Different levels of value can be realized through layers of transformation: . | Costing Layers | . | | . Value is driven through an exhaustive set of levers across 5 sources and needs to be supported by a business case. These are the 5 sources: . | Business acceleration . | Get 3.5X more benefit than public only | New insights and better client experiences | Faster time to market for all your apps | Innovation in a secure, consistent way | . | Application modernization . | Speed app release from months to weeks | Modernize 66% more apps | Consistent skills and agile dev ops | Automation and less rework. | . | Infrastructure cost efficiency . | Cut infrastructure costs by 4x with less maintenance | 95% incident reduction/higher resiliency | Greater utilization | 10% infra cost savings | . | Regulatory and risk . | Reduce compliance | Spend by ~25% | Gain a single pane of control | Consistent security/compliance policies | Automation on a consistent stack | . | Strategic optionality . | Realize a more agile, flexible architecture | Avoid vendor lock in | Match workloads to the right model | Optimize cost by moving workloads | . | . Back to top . ",
    "url": "http://localhost:4000/githubPage/target-opt-model#value-levers",
    "relUrl": "/target-opt-model#value-levers"
  },"126": {
    "doc": "Target Operating Model",
    "title": "8. Digital Platforms &amp; Supply Chains",
    "content": "This section tries to answer the following questions: . | How do I create a platform architecture to enable developers and business users to create business value? | How do I best utilise and access an ecosystem of partners to add value to what we do? | How has the recent pandemic changed traditional location-based working? | . ",
    "url": "http://localhost:4000/githubPage/target-opt-model#8-digital-platforms--supply-chains",
    "relUrl": "/target-opt-model#8-digital-platforms--supply-chains"
  },"127": {
    "doc": "Target Operating Model",
    "title": "Digital Platforms",
    "content": "Leading digital organisations are creating scalable, flexible and multi-level platforms that can be easily consumed by developers, business users and partners: . | Business Platforms - Developing business platforms and services that can be re-used within the rest of the organisation (both business users and development teams) and with partners and B2B customers | Technology Platforms - Creating platforms on-top of cloud services that embed automation, controls and security that are easily consumable by developers ‘as code’ or via a self-service portal via a single pane of glass | . | Digital Platforms | . | | . The key drivers for such platforms are: . | Engineered with open interfaces, ’standard’ based and easy to connect to | Are aligned and pre-built to architectural, service, compliance and security standards | Removes a level of operations that needs to be performed by developers so they can focus on adding value and creating digital products | Adds value on-top of ‘out-of-the-box’ cloud and technology services | Provides a set of stable core components, but which can support variety and evolution | Easily extensible, enabling scalability, not just on demand but based on strategy  | Encourages and facilitates innovation and novel usage | Opens up a business capability to be accessed by partners and customers | Allows the rest of the organisation and partners to add value and capture value through the interfaces | . Business platforms are built upon a foundation of technology platforms, connected in an open and transparent way . | Each platform provides a range of services that can be easily publish and consumed to construct digital products and services | These are surfaced either via APIs, CLI, self-service catalogues or through other user interfaces and visualisation engines (i.e. codeless BPM) | These can be combined together via open API and connectors | Each platform service should also provide APIs to monitor and operate services to provide visibility of availability and performance | . | Digital Platform Functions | . | | . ",
    "url": "http://localhost:4000/githubPage/target-opt-model#digital-platforms",
    "relUrl": "/target-opt-model#digital-platforms"
  },"128": {
    "doc": "Target Operating Model",
    "title": "Partnering &amp; Sourcing",
    "content": "An effective IT Supply Chain improves and organisation flexibility, cost effectiveness and ability to build capability. What is the IT Supply Chain? . The process off managing interactions with cloud providers (platform vendors, SaaS providers, service vendors) formally by selecting them based on their ability to meet identified requirements and managing performance against the agreed upon commitments within the Service Catalogue. Some of the key focus areas in this function include: . | Migrating to as-a-service commercial models for both technology infrastructure enabled by cloud computing and a flexible workforce that delivers outcome-driven engagements | Adopting a more liquid workforce as they try to bridge the skill gaps resulting from rapid changes in technology | Implement the right mix of local and global focus given the flexibility on cloud, new ways of working and need for shorter delivery cycles with high business involvement | . What benefits does it bring? . | Benefits from IT Supply Chain | . | | . Considerations Design when designing your IT Supply Chain . | Collaborative Sourcing Relationships - Shift away from focus on deliverables and service levels. Focus on a sourcing philosophy to embrace a partnership approach that can result in collaborative processes, skill development | Managed Capacity - As enterprises continuously evolve their strategies and roadmap, they would need to pull in capabilities and services on Demand | Open collaboration tooling - The new ways of working would need collaborative working practices across microsites. Supplier base model will need to adjust to match the DevOps Lifecycle | Service Brokerage and Orchestration - Provide a centralized catalog of internal and external cloud, business platform and Open APIs to fuel rapid innovation across the enterprise | Multisourcing Strategies - Unlike the conventional tower-based sourcing, Continuous Delivery in a cloud world require flexible sourcing strategies that allow multiple vendors collaborate with clearly defined roes | Opex Centric Consumption driven planning - The IT service framework now prices IT Services incrementally from the IT vendors to get accurate Operational Expense forecasting. Consumption driven planning with Business Product Owners | Predictive Demand Planning - Use in-depth analysis and predictive forecasts to anticipate demand and initiate sourcing with sufficient lead times | Monitoring - Globally monitor and control all local / regional cloud service consumption for continuous platform optimisation. Drive usage analytics and chargebacks through automated single unified plane of control across cloud service providers | Mitigation - Managing Cloud Platform risks like Vendor-lock-in, latency, security, and availability as an integral part of the Sourcing Strategy | . ",
    "url": "http://localhost:4000/githubPage/target-opt-model#partnering--sourcing",
    "relUrl": "/target-opt-model#partnering--sourcing"
  },"129": {
    "doc": "Target Operating Model",
    "title": "Dynamic Delivery",
    "content": "Pre-pandemic, delivery models were optimised with delivery across client site, nearshore and offshore delivery centres. To balance cost and value, enterprises had Co-located garages for experience and innovation complimented by distributed Agile practices. With the pandemic, this was challenges as business continuity came to the fore. Enterprises had to deal with Dispersed teams &amp; Individual silos, lack of certainty and change in priority of projects as well as security concerns - truly hindering innovation. Shift 1 . Enterprises initially focussed on recovering from this situation and getting back into being a resilient organization. This shift was focused on Employee safety, making existing team members virtually equidistant, bringing certainty to Program delivery, increasing portability of tasks and restoring Innovation. | Resilient | . | | . This shifted focus on 100% Cloudified Delivery Methods by adopting to Virtual Garage models and updated Security procedures and getting their teams skilled in virtual execution. Shift 2 . Having realized the longetivity of the situation and the ability to change, enterprises are evolving further to generate value in spite of the handicap. Delivery across sites and individuals are being optimized for skill and cost (hybrid and variable), Automation has been taken up a notch in the Cloudified Delivery Platforms. | New Normal | . | | . The focus is now on building the ability in the enterprise to absorb shocks from external unforeseen events and be able to carry out any task, at any time, from anywhere. This, in turn, as offered an improved value and cost profile compared to pre-pandemic model, . Back to top . ",
    "url": "http://localhost:4000/githubPage/target-opt-model#dynamic-delivery",
    "relUrl": "/target-opt-model#dynamic-delivery"
  },"130": {
    "doc": "Target Operating Model",
    "title": "9. Innovation &amp; Agility in Delivery Approach",
    "content": " ",
    "url": "http://localhost:4000/githubPage/target-opt-model#9-innovation--agility-in-delivery-approach",
    "relUrl": "/target-opt-model#9-innovation--agility-in-delivery-approach"
  },"131": {
    "doc": "Target Operating Model",
    "title": "New Ways of Working",
    "content": "To develop the future IT operating model, we recommend taking an agile, user centric approach. Take a persona lead approach, define user journeys to develop an operating model that is built for the people who will demand, consume and operate services. Elaborate and build out each operating model dimension in an agile and iterative way, informed by the user centric journeys and feedback on previous releases of the operating model. | Delivery Approach | . | | . Operating model dimensions are elaborated in an iterative way and based around key personas, the operating model is elaborated around those who will demand, consume and operate services. | Iterative Drivers | . | | . Back to top . ",
    "url": "http://localhost:4000/githubPage/target-opt-model#new-ways-of-working",
    "relUrl": "/target-opt-model#new-ways-of-working"
  },"132": {
    "doc": "Target Operating Model",
    "title": "10. Agile Metrics, Processes &amp; Toolchain",
    "content": " ",
    "url": "http://localhost:4000/githubPage/target-opt-model#10-agile-metrics-processes--toolchain",
    "relUrl": "/target-opt-model#10-agile-metrics-processes--toolchain"
  },"133": {
    "doc": "Target Operating Model",
    "title": "Performance - Measuring KPIs and SLAs",
    "content": "How can I measure performance within my organisation in a way that proactively informs future decision-making while offering line of sight into accountability and decision ownership? . Metrics and KPIs need to change from retrospectively assessing performance to proactively driving action. Historically, performance metrics and KPIs were static measures that looked backward to assess progress and lacked traceability to an organisation’s strategic objectives. | Metrics | . | | . However, performance metrics must evolve to meet the demands of today’s modern IT estate. Instead of measuring past activity, metrics should be designed and implemented in a way that helps drive the organisation’s overall objectives and appropriate action, where possible, in a pre-emptive or predictive fashion. To do so, leading practice suggests organisations should use the industry recognised OKR (Objective &amp; Key Results) approach for measuring and improving performance. OKR . OKR is an Effective Methodology for Measuring Performance, OKR, known as Objective and Key Results, is a methodology is a system for setting, cascading and communicating goals throughout your organization. OKRs bring focus because they make it easy for companies, departments, and teams to recognize what the current priorities are, and to align them to employees personal objectives. Benefits of an OKR: . | Focus: OKR creates clarity and focus. Everyone has fewer defined objectives, which forces prioritisation at all levels. | Fast paced: OKRs are short term goals—typically 3 months in length. This gives employees more chances to assess and improve work. | Better results: using short term goals creates a culture where problems are solved quickly and where every contribution matters. This results in better and more consistent results | . An OKR consists of two things: . Objective - memorable qualitative description of what you want to achieve. Objectives should be short, inspirational and engaging. Objectives should motivate and challenge the team. Key Results - set of metrics that measure progress towards objective. They should not be a task. Each objective has 2-5 key results. For example, Objective #1: We will improve release velocity of the sprint Q3-2019 for version X.Y of product B as measured by …. Key Results: - Reduce bugs found during development process by 20% - Improve Unit testing coverage from 50% to 70% - increase sprint capacity from 85 item (Sprint Planning ) to 100 SP - Individual developer contributes to 20% more code review by end of every sprint . OKRs will vary across the organisation based on their priorities mapping reference OKRs to each CSOM component, such as: . | Center of Excellence Sample Objective: Drive adoption and usage of cloud services in consistent way; Sample Key Results: Enhanced skills on cloud technologies, Increased brand perception by customers | Platform Engineering &amp; Operations Sample Objective: Incubate new technology platforms; Sample Key Results: Faster time to provision infrastructure, Increased Automation with DevSecOps toolchain enabled | Integrated Service Management Sample Objective: Protect live service; Sample Key Results: Increase Process Efficiency, Proactively monitor services, Provide real-time reporting on service performance | Application Engineering &amp; Operations Sample Objective: Accelerate the development team speed; Sample Key Results: Reduced time to delivery, Change, release, incident and problem tightly coupled | . | OKR Pillars | . | | . ",
    "url": "http://localhost:4000/githubPage/target-opt-model#performance---measuring-kpis-and-slas",
    "relUrl": "/target-opt-model#performance---measuring-kpis-and-slas"
  },"134": {
    "doc": "Target Operating Model",
    "title": "IT Processes",
    "content": "How do I adapt my processes to drive agility &amp; speed, minimising friction without sacrificing control? . Key process underpin the capabilities required within the organisation - . Engagement &amp; Demand: the capabilities needed to translate business strategy, needs and requests into products and platforms and ensure the right skills are available across the organisation . Product Lifecycle Management: the capabilities needed to architect, develop and release both application and platform/technology products and services . Fulfillment &amp; Provisioning: the capabilities required to make products and services easily consumable and rapidly available for use when needed . Service Operations: the capabilities required to support and operate products and services and ensure they are performant, resilient and meet business and user service levels . Governance &amp; Control: the capabilities required to support and operate products and services and ensure they are performant, resilient and meet business and user service levels . | IT Processes | . | | . Example Process: Engagement &amp; Demand &gt; Demand Management . Its the process of understanding the patterns of the business’ behaviors and relate those patterns to the impact on the supply of cloud services, synchronizing the consumption (demand) with the capacity (supply) of IT resources, as documented in the Service Catalogue. Objectives . | Minimize the mismatch between demand and supply | Minimize the mismatch between demand and the service catalog | Maximize reuse of the services in the service catalog | Identify the need for new or modified services and capabilities | Identify investment implications | Minimize the lead time to service the demand | . Outcomes . | Demand and capacity forecast model | Demand Servicing Lead time | Ability to service both cloud and legacy assets | Predictability of budget estimates | . Activities . | Define Demand Patterns | Forecast Demand and Provision | Manage Demand Provision Control | Evaluate Demand Management Performance | . Cloud Adoption Considerations . | Multi-cloud service catalogs | Cloud consumption/demand policies | Trained BRMs to represent the CIO | Private cloud capacity constraints and AD resources | Demand for IT technology services to support requests from AD teams | Demand implications between cloud and legacy environments | Auto detect service usage patterns (e.g. seasonal spikes) | Continual refresh of 30-60-90 day demand forecasts | . Process View . | Demand Management Process | . | | . | Process Activities | Enterprise Business | Centre of Excellence | Application Eng &amp; Ops | Platform Eng &amp; Ops | 3rd Party Suppliers | Comments | . | Investigate demand patterns &amp; forecasts | C | A/R | R | I | I | Business IT to operate a pull function | . | Define demand patterns &amp; forecasts | A/R | C | C |   |   |   | . | Value &amp; classify business demands |   | A/R | C | I |   |   | . | Consolidate business demands &amp; forecasts |   | A/R | A/R |   |   | CSP together with Enterprise IT will consolidate demand across its customers. | . | Manage Demand provision control |   | C | A/R | A/R | I |   | . | IT Demand Strategy &amp; Plan |   | A/R | C | C | I | Business IT own development, and any subsequent revision of the IT Demand Strategy and Plan | . | Platform Infrastructure Demand Strategy &amp; Plan |   | C | C | A/R | I |   | . | Application and Services planning | I | C | A/R | C | I |   | . | Track utilization | I | I | A/R | A/R |   |   | . ",
    "url": "http://localhost:4000/githubPage/target-opt-model#it-processes",
    "relUrl": "/target-opt-model#it-processes"
  },"135": {
    "doc": "Target Operating Model",
    "title": "IT Toolchain",
    "content": "Toolchain is a set of tool integrations that support development, deployment, and operations tasks. In the landscape of fragmented tools and processes, it is important for the enterprise to integrate tools and practices from multiple vendors and open source projects. For an agile and innovative team, a IT tool chain is required which offers right tools for the job while keeping some standardization, allow for automating and redefining processes with AI and removes bottlenecks &amp; delays in the production line, enables collaboration across eco-system &amp; integration across team silos and embeds digital processes &amp; methods and auto-checks for compliance &amp; quality. | Toolchain | . | | . Key focus and principles of the toolchains: . | Best of Breed | Meet business demands, Objectives and maximize flow in IT Value Streams | Fully Secured and Complaint to standards | Improve Visibility and Aid Automation | Reduce Integration Challenges | Supports Continuous Integration and Delivery | . | Tooling | . | | . What are the tools required at the minimum? . | Planning and Collaboration: These are the tools that help accelerate the plan and provide transparency to stakeholders and collaboration opportunities (Slack) | Source Control: Manage the source code across all properties and assets | Tracking and escalation issues: To maintain a catalog when they troubleshoot problems or resolving issues and followed by a surge in responsiveness (Jira) | Development (CI/CD): Collaborative tools that provide dashboards to stakeholders and developers, providing transparency and the opportunity to work together during the CI/CD process | API/Integration | Security: Integrating security tools, processes and policies to the DevOps toolchain | Configuration management: A convenient way to standardize configurations across assets (Chef and Puppet) | Automation and Testing: Collaborative tools to enable continuous testing and test automation | Brokerage and Orchestration | Operations and Monitoring: Tools for monitoring provide essential information about the functioning of the software and the presence of security issues and threats. | Cloud Management | Observability and Analysis | . | Periodic Table of DevOps Tools | . | | . ",
    "url": "http://localhost:4000/githubPage/target-opt-model#it-toolchain",
    "relUrl": "/target-opt-model#it-toolchain"
  },"136": {
    "doc": "Low-Code No-Code",
    "title": "Low-Code No-Code",
    "content": " ",
    "url": "http://localhost:4000/githubPage/lowcode-nocode",
    "relUrl": "/lowcode-nocode"
  },"137": {
    "doc": "Low-Code No-Code",
    "title": "What is Low-Code No-Code",
    "content": "Visual software deployment platform for creating business applications with a click or drag and drop of components and piecing them together. Key benefits of Low-Code No-Code development platforms are: . | Quickly create and deploy applications | Improved business process agility and automation | Standard design and accessibility principles | Easy to build and test | A simple submission(data) transformed into end-end service | No complex deployments, hosting, integrations required | Business perspective development platforms | Frontend, backend, data, process flows, and architecture all embedded into the platform | Provides holistic view of the entire applications | . | Low-Code No-Code | . | | . | Model components should be immutable,any new changes should be new components, extensive testing is required, need support for flexible auth requirements. | Avoid vendor lock-in by using open source-based components, no frequent updates to components, cultural changes and training needed. | Set appropriate security constraints for B2E, B2C and B2B applications, customization is difficult, data compliance policies revisit needed. | . ",
    "url": "http://localhost:4000/githubPage/lowcode-nocode#what-is-low-code-no-code",
    "relUrl": "/lowcode-nocode#what-is-low-code-no-code"
  },"138": {
    "doc": "Low-Code No-Code",
    "title": "IBM Approach",
    "content": "Prebuilt assets to accelerate Low-Code Cloud Native Development. Client Scenario . Build an X-Press connect API marketplace for business and third party Partners with following features: . | Partners to self-subscribe and map APIs on the platform | Automatic code generation, build and deploy to non-production environments | Certify and move to production in order to consume the APIs | . IBM Solution . | IBM Solution | . | | . Digital Scenario Builder (DSB) helps choreograph microservices into a digital scenario flow . | Automated Code generation leads to 25 % better code quality, reducing cost of maintenance | 25% cheaper with enhanced productivity using core language resources and scaling abilities | . | Digital Scenario Builder | . | | . Digital Microservices Builder . | Generate microservices code using API definitions that enables to rapidly deploy new functionality &amp; accelerate go-to-market of new products &amp; services | Accelerated to get Microservices program to be 20% faster | . | Digital Microservices Builder | . | | . DevOps Commander and Deployment Automation Tool . | Automate design, build, deployment and operation based on best practices, Microservice API enabled tool, UCDz, Jenkins, Jira, REXX Scripts, Shell Scripts | . | DevOps Commander and Deployment Automation Tool | . | | . ",
    "url": "http://localhost:4000/githubPage/lowcode-nocode#ibm-approach",
    "relUrl": "/lowcode-nocode#ibm-approach"
  },"139": {
    "doc": "Delivery Model",
    "title": "Delivery Model",
    "content": ". Table of Contents . | Garage | Dynamic Delivery | . ",
    "url": "http://localhost:4000/githubPage/delivery",
    "relUrl": "/delivery"
  },"140": {
    "doc": "Delivery Model",
    "title": "1. Garage",
    "content": " ",
    "url": "http://localhost:4000/githubPage/delivery#1-garage",
    "relUrl": "/delivery#1-garage"
  },"141": {
    "doc": "Delivery Model",
    "title": "Objective",
    "content": "This section is used to highlight a Collaborative Delivery Model that is both organic &amp; immersive and provides value for our clients. | Garage | . | | . ",
    "url": "http://localhost:4000/githubPage/delivery#objective",
    "relUrl": "/delivery#objective"
  },"142": {
    "doc": "Delivery Model",
    "title": "IBM’s Approach",
    "content": "We offer a unique way of engaging through co-creation, joint squads, and the IBM Garage Method for Cloud to deliver organizational change and a new culture of speed and velocity that’s sustainable for the digital future. | We think the best way to deliver co-creation to the clients is through joint squads | There should be SMEs from both IBM and from the client to ensure that skills are being transferred throughout the process | They’re working together in a very agile, very iterative way to prioritize what’s getting developed | . There are three distinct ways of delivery build applications on cloud: . | Envision: Quick engagement similar to a PoC that does high level architecture and defines the MVP | MVP: Actually building an MVP deployed to production – also contains elements of design and analyse | Scale: Full engagement with multiple teams | . | IBM Approach | . | | . |   | Envision | MVP | Scale | . | Outcomes | - Architecture and Blueprint - MVP Definition - Journey Map - User Story Backlog - Journey Maps | - Blueprint &amp; E2E Architecture - MVP Deployed to Production - 12-30 Microservices - 135-270 story points with varying complexity | - 2-5 Microservices per Month - Deployment across geos, LOBs - Co-Creation and Skills Transfer - Organizational Change - New technology adoption and skillset(AI, Analytics, Automation, Blockchain) | . | Key Activities | - IBM Garage Design Thinking Workshops - Cloud Advisory Assessment - Experience Led Development | - Set up Dev &amp; Test Environments - Joint Development Squads - Coexistence, E2E Testing and Integration | - Organizational Change Management - Employee Engagement - Digital Change - Full Dev &amp; Test | . | Duration | 3 Month Week | 3-6 Months | 6+ Months | . | Deployment Model | Multi-cloud, On-prem | Multi-cloud, On-prem | Multi-cloud, On-prem | . Back to top . ",
    "url": "http://localhost:4000/githubPage/delivery#ibms-approach",
    "relUrl": "/delivery#ibms-approach"
  },"143": {
    "doc": "Delivery Model",
    "title": "2. Dynamic Delivery",
    "content": " ",
    "url": "http://localhost:4000/githubPage/delivery#2-dynamic-delivery",
    "relUrl": "/delivery#2-dynamic-delivery"
  },"144": {
    "doc": "Delivery Model",
    "title": "Leveraging the Power of Co-Creation",
    "content": "Globally Distributed Flexible Squad Composition . A program / account can have various flavors of these squad compositions, based on the workload &amp; requirement. Some programs can start onsite heavy, and may move towards more offshore depending on the workload shifts and productivity. Co-located Squad . | Workloads that can be split into independent deployment units | Co-Creation - MVP/PoC using layered delivery | . | Squad Model | . | | . Vertically Integrated Distributed Squad . | Workloads that have huge dependency between modules | Co-Creation with Clients and squads across multiple locations | . | Squad Model | . | | . ",
    "url": "http://localhost:4000/githubPage/delivery#leveraging-the-power-of-co-creation",
    "relUrl": "/delivery#leveraging-the-power-of-co-creation"
  },"145": {
    "doc": "Delivery Model",
    "title": "Ceremonies &amp; Collaboration",
    "content": "For Location Independent Distributed Squads, . | Ceremonies should happen in overlapping time zone | Tools to enable location independent collaboration | Client Product Owner is required for Backlog Grooming, Prioritization, Playback and formal acceptance of user stories by end of sprint | . | Dynamic Collaboration | . | | . ",
    "url": "http://localhost:4000/githubPage/delivery#ceremonies--collaboration",
    "relUrl": "/delivery#ceremonies--collaboration"
  },"146": {
    "doc": "Delivery Model",
    "title": "Challenges of Distributed Agile",
    "content": ". | Distributed Agile promotes Federated Innovation and taps into Specialized Talents available in various development centers | ‘Follow-the-Sun’ approach contributes towards ‘Faster Time to Market’ | . Communication . | Use visuals in communication / user story | Reduce team dependency in assigning work items | Use of Technology for team synergy – WebEx | . Accessibility . | Plan work time overlap across locations | Add handover as an agenda in Daily Meeting | Come up with Proxy roles for location | . Squad Structure . | Enable horizontal communication of peers | Squad size limited to around 8 resources | Virtual task-board to ease collaboration and sharing | . Delivery . | Parallel Scrums for distributing the workloads | Assign entire logical work to squads in same time zone | Integration Sprint | . ",
    "url": "http://localhost:4000/githubPage/delivery#challenges-of-distributed-agile",
    "relUrl": "/delivery#challenges-of-distributed-agile"
  },"147": {
    "doc": "Home",
    "title": "Future of Application Development",
    "content": ". Organizations are being disrupted from traditional competitors and new players never seen in their markets. To combat the disruption, organization recognize they need to pivot, act quickly to market shifts, create new markets and lower the cost of operations to compete. A key element in an organization’s transformation strategy is technology. Leveraging emerging technologies can often be the difference between being disrupted or being the disruptor. Today’s top organizations are leveraging emerging technology to innovate and differentiate. Key to their success is the ability to reimagine existing customer facing applications and build new applications that support new ways of delivering goods and services. Central to their digital transformation journey is Application Development. | Digital Transformation | . | | . The following sections provide IBM’s vision on the Future of Application Development and the key elements to consider when modernizing your existing applications or building new capabilities. | Key drivers of the evolution of application development, what our clients are doing and how this is happening. | Target Operating Model: What this evolution means for the enterprise. Modern enterprise workflows demand agility at all layers of business platforms, and they all need to securely deliver technology solutions faster and at scale. Driven by intense competition, the requirements for business applications are constantly changing a lot, so, companies need to release updates daily or even multiple times per day at all layers of IT landscape. | New age models of application development and how IBM has been changing to enable enterprises: . | Architecture is becoming more reactive and event-driven than just APIs. | Delivery Model: High Speed Garages and Dynamic Delivery models to enable enterprises deliver new applications. | DevSecOps Principles: Key disciplines of DevSecOps - DataOps &amp; Data Pipelines, Tooling, Continous Monitoring and Site Reliability Engineering for Continuous Improvements. | Software Engineering Practices: Key practices which are enabling this evolution. | New Age Development with Hybrid Integration: How Hybrid Integration is becoming the center stage of application development. | Low-Code No-Code: a new form of application development platforms evolving with lower barrier to entry for coding. | . | . ",
    "url": "http://localhost:4000/githubPage/#future-of-application-development",
    "relUrl": "/#future-of-application-development"
  },"148": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "http://localhost:4000/githubPage/",
    "relUrl": "/"
  },"149": {
    "doc": "DevSecOps Principles",
    "title": "DevSecOps Principles",
    "content": "Modern enterprise workflows demand agility at all layers of business platforms, and they all need to securely deliver technology solutions faster and at scale. Driven by intense competition, the requirements for business applications are constantly changing so much; therefore, companies need to release updates daily or even multiple times per day at all layers of IT landscape. | DevSecOps Drivers | . | | . Table of Contents . | DataOps &amp; Data Pipelines | DevSecOps Tooling | Monitoring | Continuous Improvement with Site Reliability Engineering (SRE) | . ",
    "url": "http://localhost:4000/githubPage/devsecops-principles",
    "relUrl": "/devsecops-principles"
  },"150": {
    "doc": "DevSecOps Principles",
    "title": "1. DataOps &amp; Data Pipelines",
    "content": " ",
    "url": "http://localhost:4000/githubPage/devsecops-principles#1-dataops--data-pipelines",
    "relUrl": "/devsecops-principles#1-dataops--data-pipelines"
  },"151": {
    "doc": "DevSecOps Principles",
    "title": "Data Ops",
    "content": "| DataOps | . | | . Data Ops lies at the conjunction of data engineering, data science and data analytics. Drivers for DataOps . Problems . | Data Scientists spend a lot of time working on the data for various phases such as data filtration, cleansing, deduplication, profiling and transformation | As part of developing analytical models, the data scientists are required to test the models against datasets. With each iteration, the data needs to be refreshed and the models are required to be retrained. The metadata also changes and this requires to be monitored, giving rise to a need for KPIs for data | . Drivers . | Data must be delivered at the speed of analytics to meet the demand | Accelerate Data preparation and Data Enrichment for Data Scientists | Deliver DataOps as a Service for Data Platforms and Solutions – as an extension of DevOps &amp; DevSecOps | Data Scientists should be able to consume data quickly with up to date datasets which are consistent throughout the life cycle of data without compromising on integrity | . ",
    "url": "http://localhost:4000/githubPage/devsecops-principles#data-ops",
    "relUrl": "/devsecops-principles#data-ops"
  },"152": {
    "doc": "DevSecOps Principles",
    "title": "Data Pipelines",
    "content": "Data has a workflow in a Data Platform between the point of source through the various stages of transformation to the point of consumption. | Data Pipeline | . | | . | This includes Ingestion, Integration, Storage, Processing, Testing, Serving, Machine Learning &amp; analytics. | . | . ",
    "url": "http://localhost:4000/githubPage/devsecops-principles#data-pipelines",
    "relUrl": "/devsecops-principles#data-pipelines"
  },"153": {
    "doc": "DevSecOps Principles",
    "title": "DevOps vs DataOps",
    "content": "| DevOps | DataOps | . | Development + Operations | Data + Operations | . | Develop Software | Deliver Data | . | Application Developers | Data Scientists &amp; Data Analysts | . | Application Design | Data Engineering | . | Integration of Developers, Testers, Operators and Users | Integration of Data practitioners | . | Application Development Life Cycle | Data Life Cycle Management | . ",
    "url": "http://localhost:4000/githubPage/devsecops-principles#devops-vs-dataops",
    "relUrl": "/devsecops-principles#devops-vs-dataops"
  },"154": {
    "doc": "DevSecOps Principles",
    "title": "DataOps-as-a-Service",
    "content": "| DataOps-as-a-Service | . | | . ",
    "url": "http://localhost:4000/githubPage/devsecops-principles#dataops-as-a-service",
    "relUrl": "/devsecops-principles#dataops-as-a-service"
  },"155": {
    "doc": "DevSecOps Principles",
    "title": "DataOps Ecosystem",
    "content": "| DataOps Ecosystem | . | | . Back to top . ",
    "url": "http://localhost:4000/githubPage/devsecops-principles#dataops-ecosystem",
    "relUrl": "/devsecops-principles#dataops-ecosystem"
  },"156": {
    "doc": "DevSecOps Principles",
    "title": "2. DevSecOps Tooling",
    "content": " ",
    "url": "http://localhost:4000/githubPage/devsecops-principles#2-devsecops-tooling",
    "relUrl": "/devsecops-principles#2-devsecops-tooling"
  },"157": {
    "doc": "DevSecOps Principles",
    "title": "Background",
    "content": "Security and compliance have become top priorities in today’s volatile and uncertain business environment and must be integrated into software lifecycle to maintain velocity. | DevSecOps | . | | . Enterprises are under pressure to implement DevSecOps to modernize and be agile, deliver faster, acquire industry best practices, but they face serious implementation challenges. Managing Tool Sprawl . Individual business units choose from a plethora of DevSecOps tools in the market, usually without an enterprise level view resulting in tool sprawl . Centralized Management . Without a management layer and GitOps, pipeline definitions become too decentralized and creates pipeline sprawl. Centralized access control with pattern-based approach is necessary for every organization. Toolchain monitoring and management . Ongoing management of the toolchain instances and application delivery pipelines require significant efforts. Optimization with continuous feedback . The need for continuous feedback and optimization to evaluate the impact of each release and gauge end user satisfaction is essential to deliver the right business outcomes. ",
    "url": "http://localhost:4000/githubPage/devsecops-principles#background",
    "relUrl": "/devsecops-principles#background"
  },"158": {
    "doc": "DevSecOps Principles",
    "title": "IBM Approach",
    "content": ". | Help Clients to adopt a sustainable DevSecOps model, building upon any transformation they have already undertaken | Understand and build upon existing target model and transformation plan on different fronts: people, process, technologies as required | Build upon any foundational steps taken for creating a DevSecOps culture and roll-out across the client organization | . | IBM Approach | . | | . Solution Design Points . | Assessment of client’s current SDLC state/development process and pain points | Build upon existing definition of target DevSecOps processes, toolchain and implementation plan, filling gaps where required | Provide DevSecOps framework artifacts and reusable materials for the client organization customization and roll-out | . Key Outcomes and Benefits . | Defined the target DevSecOps framework | Maximise shift left approach in DevSecOps environment | Defined optimization points for security activities within the SDLC | . Back to top . ",
    "url": "http://localhost:4000/githubPage/devsecops-principles#ibm-approach",
    "relUrl": "/devsecops-principles#ibm-approach"
  },"159": {
    "doc": "DevSecOps Principles",
    "title": "3. Monitoring",
    "content": " ",
    "url": "http://localhost:4000/githubPage/devsecops-principles#3-monitoring",
    "relUrl": "/devsecops-principles#3-monitoring"
  },"160": {
    "doc": "DevSecOps Principles",
    "title": "Overview",
    "content": "Having an integrated multicloud services management delivers: . | Ultimate flexibility, freedom of choice and avoidance of vendor lock-in | Simplified and delayered cloud and application management functions | Consistency in monitoring, optimized data/workload placement, improved service management quality | Reduced risk, enhanced resiliency, and improved reliability in your entire cloud operations | Increased transparency, faster innovation and reduced cost through simplified governance | . | Integrated Service Management | . | | . ",
    "url": "http://localhost:4000/githubPage/devsecops-principles#overview",
    "relUrl": "/devsecops-principles#overview"
  },"161": {
    "doc": "DevSecOps Principles",
    "title": "Cloud Management",
    "content": "Manage traditional and cloud environments seamlessly across hybrid multicloud using our enterprise grade cloud management platform. IBM delivers the only enterprise grade solution for managing hybrid multicloud which provides: . | A unified operations layer that enables infrastructure and applications to be monitored and maintained including legacy infrastructure, private cloud, public cloud and container environments | A robust management platform with built in capabilities including inventory, orchestration, operations, and service management across cloud infrastructures. | An optimized digital self-service user experience to consume, deploy, operate, and govern across all clouds and data centers through a single pane of glass | . | Cloud Management | . | | . ",
    "url": "http://localhost:4000/githubPage/devsecops-principles#cloud-management",
    "relUrl": "/devsecops-principles#cloud-management"
  },"162": {
    "doc": "DevSecOps Principles",
    "title": "Security Operations Center (SOC)",
    "content": "The SOC proactively monitors, avoids outages, quickly troubleshoots and continuously improves through: . | Single point of contact &amp; pane of glass view into Hybrid multicloud provider landscape which delivers application run/operate/service delivery with cost savings of up to 20% | Integrated business services operations and management using proven SRE practices and AI-infused automation | Embedded Hybrid IT Security assures data protection, resiliency, and compliance | . | Security Operations Center (SOC) | . | | . ",
    "url": "http://localhost:4000/githubPage/devsecops-principles#security-operations-center-soc",
    "relUrl": "/devsecops-principles#security-operations-center-soc"
  },"163": {
    "doc": "DevSecOps Principles",
    "title": "Services Control Tower",
    "content": "Services Control Tower enables you to manage the Cloud Operating Model with meaningful insights to redefine business strategies. The IBM Services Control Tower summarizes key metrics and allows you to manage and simplify digital, cognitive and cloud chaos, translating cloud complexity into tangible enterprise business strategies. It provides: . | End-to-end visibility to manage operations | Produce early warnings for issues with automatic resolutions driving quicker response times | Reduce overall operating costs but increase business flexibility | Achieve the promise of the cloud operating benefits | . | Services Control Tower | . | | . ",
    "url": "http://localhost:4000/githubPage/devsecops-principles#services-control-tower",
    "relUrl": "/devsecops-principles#services-control-tower"
  },"164": {
    "doc": "DevSecOps Principles",
    "title": "Integrated DevOps",
    "content": "Adopt an integrated DevOps platform for hybrid multicloud – offered as a shared, dedicated or customized model. Embrace DevOps across the enterprise with new ways of operating and the tools required to accelerate software delivery, improve quality and enhance customer experience . | The IBM DevOps Commander enables tool chain orchestration with customized, real time builds of your DevOps environment, almost 70% faster than traditional methods. | Standardized, integrated DevOps tool chain services available across leading cloud platforms | Build a customized DevOps tool chain and integrate already deployed DevOps tools in less than an hour | Incorporates market leading and open source software engineering tools | Best in class security practices integrated into the software engineering lifecycle | Secure testing integration creates a robust DevSecOps environment | . ",
    "url": "http://localhost:4000/githubPage/devsecops-principles#integrated-devops",
    "relUrl": "/devsecops-principles#integrated-devops"
  },"165": {
    "doc": "DevSecOps Principles",
    "title": "Security Cloud Framework",
    "content": "Traditional security controls and infrastructure operational practices are changing to data and workload-centric cloud security policies, technologies, and practices. We approach this challenge by aligning security controls with the realities of your hybrid multicloud IT estate to: . | Enforce Application security via DevSecOps practices | Cloud Platform security | Identification and planning for enterprise / regulatory specific controls / compliance / audits | Data and Storage security and privacy | Security threat monitoring | Security incident management | Business risk and compliance services | . | Security Framework | . | | . Back to top . ",
    "url": "http://localhost:4000/githubPage/devsecops-principles#security-cloud-framework",
    "relUrl": "/devsecops-principles#security-cloud-framework"
  },"166": {
    "doc": "DevSecOps Principles",
    "title": "4. Continuous Improvement with Site Reliability Engineering (SRE)",
    "content": " ",
    "url": "http://localhost:4000/githubPage/devsecops-principles#4-continuous-improvement-with-site-reliability-engineering-sre",
    "relUrl": "/devsecops-principles#4-continuous-improvement-with-site-reliability-engineering-sre"
  },"167": {
    "doc": "DevSecOps Principles",
    "title": "What is SRE?",
    "content": "Site Reliability Engineering (SRE) is an DevSecOps discipline that aims at developing automation in operational aspects of running IT application. This includes infrastructure provisioning, responding to faults and events, managing performance, etc. It was conceptualized in Google in 2003 for their systems and has been adopted by the industry over a period as an essential set of practices. SRE is an engineering approach to deliberately introduce reliability into site operations. Fundamentally, it’s what happens when you ask a software engineer to design an operations function. SRE fundamentally changes the approach to systems management. It recognizes that throwing bodies at the problem is not a long term approach. Introducing ever and more complex layers of management does not yield a winning strategy. SRE and Application Development . SRE is an important discipline for application development in the current and future. It has evolved into a product development paradigm - an ongoing process of incrementally delivering new capabilities, continuous focus in required on maintaining and improving the resilience of the application. ",
    "url": "http://localhost:4000/githubPage/devsecops-principles#what-is-sre",
    "relUrl": "/devsecops-principles#what-is-sre"
  },"168": {
    "doc": "DevSecOps Principles",
    "title": "SRE Practices",
    "content": "| SRE Practices | . | | . Key elements of an Engineering approach to Operations . | Understand your system System thinking! You can get still things done without understanding, but if you know what’s happening, you can innovate. | Ask great questions Asking a really good question is like telepathically moving information from someone else’s brain to yours. | Read the code Even difficult codebases, even in languages you don’t know. It can be easier than you expect to go look at the code and see what’s really happening. | Debug like a wizard To be better at debugging. | Write down a design Understand what all can go wrong. Use design docs, project briefs, etc. They help you catch misconceptions early and get good ideas. | Understand the big picture Better technical decisions. Either be sure that you know why the thing’s important, or go do something else. | . Principles and Tenets of SRE . An effect SRE practice has the following principles: . | System Thinking | Data-Driven Decisions | Engineering Rigid | Embracing Risk | Eliminating Toil | Remove Technical Debt | Simplicity | Collaboration | Shared Responsibility | Trust &amp; Transparency | . If you take these principles and try to apply them to operating a system you will recognize some common features: . | Ops to scale with load through Automation, but don’t stop at Automation | Cap operational load: 50% time spent on toil - 50% on engineering projects (improvements) | Excess Ops work overflows to the Dev Team, share 5% of Ops work with Dev Team | Have an SLA / SLO for the service, measure against the SLA / SLO | Error budget to control velocity. Effective self-regulation of features vs. stability | Observability, including the Golden Signals: Latency, Traffic, Errors, Saturation, Requests | Actionable symptom-based alerts, from the user perspective. (Automated) runbooks to govern actions. | Blameless post-mortem for every event | Common staffing pool for SRE and Dev | . Tenets of SRE . | Scale Ops sub-linearly with load | Cap Operational load at 50% | Handle Overflow | SLA/SLO/SLI | ORP &amp; Error Budget | Golden Signals | Symptom-based Alerting | Blameless Postmortems | Staffing Pool | . ",
    "url": "http://localhost:4000/githubPage/devsecops-principles#sre-practices",
    "relUrl": "/devsecops-principles#sre-practices"
  },"169": {
    "doc": "DevSecOps Principles",
    "title": "AIOps for SRE",
    "content": "Capture the most value from your cloud operations with Site Reliability Engineers (SRE) that are central to the ‘operations-centric’ design of our cloud management solutions. SRE principles enable enterprises to optimize their hybrid multicloud management and deliver: . | High resiliency and elastic architecture | Reduction in errors and service requests | Optimized saturation ensuring resources are not constrained | Reduced latency by identifying hotspots and providing feedback to the development squads to automate the fixes | Engineering automation into the operations process | . Infusion of AIOps techniques in operations can allow systems to continuously improve and become more reliable. | AIOps PoV | . | | . Shift Left . Automation with AI techniques enable us to shift incident resolution closer to the source of problems – towards zero touch application management. Shift left resulting in improved service resilience, efficiency &amp; user experience through proactive, automated &amp; cognitive application maintenance. | AIOps Shift Left | . | | . Automated Operations . Embrace automated operations and intelligent management utilizing AI Ops to enhance performance across your enterprise. | Automated Operations | . | | . Automate manual tasks involved in incident diagnosis, incident resolution, service request fulfillment, change implementation, scheduled / preventive maintenance &amp; more to shift incident resolution closer to the source of problems. AIOps in Integrated Service Management Toolchain . | Integrated Service Management Toolchain | . | | . ",
    "url": "http://localhost:4000/githubPage/devsecops-principles#aiops-for-sre",
    "relUrl": "/devsecops-principles#aiops-for-sre"
  },"170": {
    "doc": "DevSecOps Principles",
    "title": "IBM SRE Approach",
    "content": "IBM SRE approach brings measurable benefits through balance between Development &amp; Operations. | IBM Approach | . | | . Key components of IBM’s SRE approach are: . | Identify and Design for the real reliability need . | Baseline SLOs (instead of 100% of 3 sec response time, target for 90%) | Review architecture for significant use cases | Define IaaC framework | Enhance and Configure custom code quality checks | . | Use the error budget to release new features . | Architect for Blue Green / Canary deployments with minimum or no downtime | Arrive at error budgets and approve new features to be deployed. (When the availability targets are met) | . | Observe everything . | Implement Build to manage with heartbeat APIs when functional APIs are secured | Design and Implement Dashboards for various KPIs | Introduce distributed tracing to pinpoint the transaction failures | . | Automate everything which is repeatable . | Identify toil and implement automated runbook | Create manual runbooks for toils which cannot be automated for L1.5 | . | Consistent CI/CD . | Implement touchless CI/CD with quality gates and automated (e.g. code coverage) regression tests | Introduce Performance tests with Jmeter in CI/CD | . | Ease of Operations . | SREs jump in as first responders for Sev 1 issues | Introduce blameless port-mortem | Chat Ops with Bots for creating automated ticketing and resolution | Document RCAs and raise enhancement tickets for Dev Team | . | . IBM has infused SRE into its Integrated Operations Reference Architecture too: . | Infused SRE in Integrated Operations Reference Architecture | . | | . ",
    "url": "http://localhost:4000/githubPage/devsecops-principles#ibm-sre-approach",
    "relUrl": "/devsecops-principles#ibm-sre-approach"
  },"171": {
    "doc": "Software Engineering Practices",
    "title": "Software Engineering Practices",
    "content": ". Table of Contents . | Chaos Engineering | . ",
    "url": "http://localhost:4000/githubPage/sw-engineering",
    "relUrl": "/sw-engineering"
  },"172": {
    "doc": "Software Engineering Practices",
    "title": "1. Chaos Engineering",
    "content": "Chaos Engineering is an evolution to business continuity and service resiliency planning. It is the discipline of experimenting on a service to build confidence in its ability to withstand failure in production. Chaos Engineering aims to introduce failures within a system to identify and fix failures proactively before they impact business operations. | Chaos Engineering | . | | . ",
    "url": "http://localhost:4000/githubPage/sw-engineering#1-chaos-engineering",
    "relUrl": "/sw-engineering#1-chaos-engineering"
  },"173": {
    "doc": "Software Engineering Practices",
    "title": "Background: The Challenge",
    "content": "Like all things, an IT system lives as part of a bigger ecosystem and other elements of the ecosystem have a bearing on the health and operation of the system. While all enterprise want their systems to be robust and design for systems to be available in general, hardly enough effort is spent to make their systems resilient to such faults in the environment - the infrastructure, the connectivity, other systems or the application itself. While some of these faults can be thought of and considered during designing the application, . | Not all can be thought through. In some cases, doing so may offset of the value of building the system itself. | Manifestations of such faults vary as well as these fault themselves respond differently to the environment they exist in. | . Considering external faults as random as they can be as chaos, engineering your system to respond to &amp; survive such faults is Chaos Engineering. It is a set of practices largely focused on hypothesizing and experimenting to verify the hypotheis and using this cycle as a tool to uncover and address faults in design, deployment, tooling and operation. | Scientific Method | . | | . ",
    "url": "http://localhost:4000/githubPage/sw-engineering#background-the-challenge",
    "relUrl": "/sw-engineering#background-the-challenge"
  },"174": {
    "doc": "Software Engineering Practices",
    "title": "Chaos Engineering &amp; Application Development",
    "content": "Chaos engineering practices are important in context of application development in a number of ways and help development teams build capabilities in the application to tackle the following kind of failures by providing fault injection methods across a range of application development activities: . | Infrastructure &amp; Network Failures - Cloud native applications run on a set of resources typically managed by external service providers. An outage to any of the underlying resource can take down the application. This is also true for network failure events. These outages can range from a storage device failure, a corrupt instance to outage of an availability zone. Ability to sustain such an event is essential. While some of these scenarios can be thought of, reproducing such events is next to impossible requiring massive coordination to create mostly unseen situations. By providing capability to reproduce such events, chaos engineering tools provide a way to build for and test for such outages. | Application Failures - Application Development is done at a much faster pace with agile practices trading off deep requirement and impact analysis in the processes. In such an environment, a testing method that can create scenarios beyond the specified acceptance criteria and requirements can add great value to the resiliency of the product. | External Application Failures - Integration is the core of modern applications and being resilient to faults in another systems an app is integrated with becomes utmost important. Faults that can be in form of errors or reliability or performance of the other system and can occur at any point for any duration. Ability to induce such faults into an app under development can greatly help developers identify and cater for such faults. | . Apart from capabilities to reproduce and address failure scenarios, Chaos engineering tools can also help manage a running application. Using Chaos Engineering practices, an application team can effectively manage the utilization, security and performance of an application. ",
    "url": "http://localhost:4000/githubPage/sw-engineering#chaos-engineering--application-development",
    "relUrl": "/sw-engineering#chaos-engineering--application-development"
  },"175": {
    "doc": "Software Engineering Practices",
    "title": "Evolution",
    "content": "Point of views and guiding principles for Chaos Engineering have started to take shape, e.g. Principles of Chaos. ",
    "url": "http://localhost:4000/githubPage/sw-engineering#evolution",
    "relUrl": "/sw-engineering#evolution"
  },"176": {
    "doc": "Software Engineering Practices",
    "title": "IBM Approach",
    "content": "Chaos Engineering is the discipline of experimenting on a software system - often in production - in order to build confidence in the system’s capability to withstand turbulent and unexpected conditions. Although the name suggests chaos, the experiments are well designed and planned: at the beginning, failure points are identified and based on this a hypothesis is formulated. A corresponding experiment is built (node failure, packet drops, slow links) and then run against the system. The result of the experiment is measured and compared against the hypothesis to either proof or disproof it. These experiments are typically performed in so-called “game days”, to test the resilience of an application in a controlled environment. However, experiments may also be performed in production environments to assure the robustness of the application. The goal of testing in production is to be able to detect problems that cannot be surfaced in pre-production testing. IBM has identified the following Principles for Chaos Engineering. Each of these 8 principles is support by a concise statement describing the What, Why, and How of that principle. | Chaos Engineering Principles | . | | . Strengthen Reliability Disciplines . Chaos Engineering builds on established reliability engineering practices. Organizations should leverage mature business continuity processes and system reliability patterns. Through the discipline of identifying and mitigating reliability issues, the organization becomes more resilient. Understand the System . As a precursor to planning a viable Chaos Engineering experiment, the system and its business outcomes should be well understood. A baseline should be formed by understanding the system as a whole, its emergent properties and functions, as well as its topology, architecture, dependencies, steady-state behavior, output response, and characteristics such as availability, latency and throughput. Experiment on Every Component . Treat all components, layers, services, and dependencies as subjects for potential experiments whenever possible. These experiments should uncover any gaps between expected and actual behavior and should validate fallbacks. Strive for Production . Experiment first upstream in non-production environments. Build confidence by learning about the system behavior during different scenarios and ensuring the system’s ability to gracefully withstand and recover from failures and unexpected events. As confidence is gained, transition towards production. Contain the Impact . Avoid cascading failures by containing tests scope and detaching dependencies. Different containment strategies should be applied to help limit the scope, minimize impact on business, and confirm their effectiveness. Measure, Learn, Improve . Infuse observability into all system components. Measure experiment results to understand the business impact. Learn from the collected insights to improve system reliability and guide future experiments. Increase Complexity Gradually . Increase complexity gradually while adjusting the granularity of the experiments. Expanding scope and combining tests may reveal previously unknown weaknesses. Socialize Continuously . Effective and transparent communication is critical to the success of any Chaos Engineering program. This creates a culture that recognises the benefits of introducing rigorously controlled risks to drive increased resilience. IBM uses a range of tools and platform capabilities around chaos engineering thus, enabling development teams to weed out such issues and therefore, build resilient products . | Istio (the core of OpenShift Service Mesh) provides capabilities to induce HTTP errors, response delays for applications deployed in container applications. Refer this article. | Netflix’s The Simian Army is a set of tools (called monkeys) to induce various kind of failures in a cloud environment e.g bringing down a virtual machine. | . ",
    "url": "http://localhost:4000/githubPage/sw-engineering#ibm-approach",
    "relUrl": "/sw-engineering#ibm-approach"
  },"177": {
    "doc": "Software Engineering Practices",
    "title": "IBM’s Chaos Engineering Method",
    "content": "IBM’s methodology for Chaos Engineering is an iterative journey that describes the planning, execution, scaling and learning of Chaos Experiments towards a robust and resilient service as well as organization. For each of the 10 steps, a description of the key activities is provided. This leads to a comprehensive methodology of performing Chaos Engineering in the enterprise. | Methodology | . | | . Here is the details of the phases of this method: . 1. Understand the System End-to-End . A successful Chaos Engineering experiment requires a clear understanding of the system end-to-end: its weaknesses, its failure events and even any skepticism towards the reliability of that system. This will help determine the answer to the question “what are we testing?”. 2. Gain Organizational Agreement . Chaos Engineering helps organizations make systems more resilient and reliable. This intent of enhancing resiliency through continuous failure testing may not be an easy sell, as it intentionally introduces minor short-term risks, to avoid major long-term risks by improving overall reliability. 3. Create a hypothesis and plan related experiments . Simply put, a hypothesis is a collection of ‘what ifs’. It is a prediction about the expected system behavior upon introduction of a certain system change or changes. One or more chaos experiments can be used to either prove or disprove the hypothesis. 4. Enable observability . In order to understand the impact of the experiments, it’s important to have objective measurements that can illustrate the health and behavior of the system. When it takes the manual work of experts to make subjective evaluations of system health, their time required represents an ongoing tax on each experiment. Monitoring should be holistic, it shouldn’t only cover black box monitoring such as disk space, CPU usage etc., but it should also cover white box monitoring such as queries per seconds, transactions per seconds etc. 5. Prepare your experiment . A Chaos Engineering experiment requires thoughtful preparation and planning to minimize service disruption. A risk management analysis should be done by factoring in the current availability metrics for the system against what is expected, the impact the test can bring to end user experience and most importantly, data integrity. 6. Run chaos experiments . A successful chaos experiment follows a structured approach that ensures standardized communication and data collection. Different methods can be applied to mitigate risks and help improve the chances of a positive outcome. Experiments should proceed within the framework of a predefined and communicated schedule so that stakeholders know what to expect. Frequent and thorough communication is critical to avoid unnecessary pitfalls during the execution of an experiment. Chaos experiments can be run during scheduled ‘GameDays’ and eventually as part of automated continuous testing. 7. Analyze the results to prove or disprove the hypothesis . Chaos Engineering experiments will hopefully produce meaningful and relevant results. The quality of the generated data is dependent on how comprehensively observability tools have been configured. With the results in hand, we can analyze the outcome, compare it to what was predicted and then make educated decisions on how to proceed. 8. Communicate findings and improve . Chaos Engineering experiments may uncover system defects and weaknesses that must be addressed. After the results are reviewed and analyzed, the findings need to be clearly communicated and converted into actionable plans with appropriate ownership. 9. Grow blast radius and repeat . Chaos Engineering is a practice of continuous experimenting and learning. Before expanding scope and growing complexity, a consistent baseline needs to be achieved (have confidence in a consistent baseline). Experiments should start small to allow the organization to mature (gain confidence) and gradually increase in scope and complexity. This means experiments should iteratively target larger blast radiuses and widen their scope to eventually touch the system end to end. 10. Evolve and expand to production . It is important to demonstrate safety in a non-production environment before running an experiment in production. Remember that non-production and production environments may yield different experiment outcomes and insights. Moreover, production data and traffic may not be available outside of production. It is crucial to start your production chaos testing with ‘GameDays’. As capabilities mature and consistent results are achieved, chaos experiments can then be run periodically via unattended automation and during deployments. ",
    "url": "http://localhost:4000/githubPage/sw-engineering#ibms-chaos-engineering-method",
    "relUrl": "/sw-engineering#ibms-chaos-engineering-method"
  },"178": {
    "doc": "Software Engineering Practices",
    "title": "IBM Publications",
    "content": ". | IBM’s principles of chaos engineering - Learn to improve the reliability and availability of your systems by following IBM’s principles and 10 steps for chaos engineering | Use chaos engineering to assess application reliability | Run chaos engineering experiments by using Gremlin on IBM Cloud | Get started with chaos engineering by using Gremlin on IBM Cloud | Chaos Engineering vs Voodoo | Overcoming chaos on the way to the moon | . ",
    "url": "http://localhost:4000/githubPage/sw-engineering#ibm-publications",
    "relUrl": "/sw-engineering#ibm-publications"
  },"179": {
    "doc": "Industry Models",
    "title": "Industry Models",
    "content": " ",
    "url": "http://localhost:4000/githubPage/industry-models#industry-models",
    "relUrl": "/industry-models#industry-models"
  },"180": {
    "doc": "Industry Models",
    "title": "Chapter Layout",
    "content": ". | Objective | Key Questions to Ask | IBM’s Approach | Sighed Benefits  | Risks | Client Stories | . ",
    "url": "http://localhost:4000/githubPage/industry-models#chapter-layout",
    "relUrl": "/industry-models#chapter-layout"
  },"181": {
    "doc": "Industry Models",
    "title": "Industry Models",
    "content": " ",
    "url": "http://localhost:4000/githubPage/industry-models",
    "relUrl": "/industry-models"
  },"182": {
    "doc": "APIfication",
    "title": "APIfication",
    "content": "| APIfication | . | | . | APIfication offers functionality and data as a service for other systems to use. It opens up new ways composing and exposing existing functionality. | An API hides the actual implementation and reveals only the simplified interactive model to the user. | APIFication can be done in a proactive or reactive fashion: . | Proactive: This includes defining the API interface first, based on forecasted need of the end consumer and then implementing it. | Reactive: This includes upfront implementation followed by a continuous definition of new interfaces on top of this implementation, based on the emerging needs of end consumer and then implementing it. | . | Benefits of APIFication: . | Cost reduction: APIs promote and increase service reuse. They provide a usage-based evolutionary development platform. | Increased agility: APIs offer integration with any technology stack, so they enable greater productivity for developers. | Increase innovation: By enabling others to build applications that integrate with their captive data and processes, companies see new applications that use their services in new contexts. | . | . ",
    "url": "http://localhost:4000/githubPage/integration-patterns-apification",
    "relUrl": "/integration-patterns-apification"
  },"183": {
    "doc": "Data Adapters",
    "title": "Data Adapters",
    "content": ". | Data Adapters | . | | . | Data Adapter Pattern is used in data consolidation scenarios to incrementally move the data traffic from legacy source / targets to strategic / modernized source / targets. | This pattern provides flexibility to modernize both legacy data source and data target platforms independently and incrementally. | Data Adapter is capable of consuming data from both legacy and modernized data sources, and is configurable to send all data to either legacy DB or strategic / modernized DB or both. | During the coexistence state, the data adapter can be configured to send all data to both legacy and strategic / modernized databases. | Upon successful data reconciliation, the Data Adapter can be configured to send data only to the modernized DB (target state), allowing the business to discontinue legacy databases and legacy application sources. | . Benefits . | No changes to legacy and strategic / modernized data sources &amp; targets. | Data sources and data targets can be modernized independently and incrementally. | Ability to reconcile data during coexistence elevates confidence with modernization. | Ability to fallback to legacy systems, lowers business risk. | . ",
    "url": "http://localhost:4000/githubPage/integration-patterns-data-adapters",
    "relUrl": "/integration-patterns-data-adapters"
  },"184": {
    "doc": "Event Driven Integration",
    "title": "Event Driven",
    "content": ". | Overview | Enabling Event Integration for Legacy Systems | Event Driven Data Synchronization | . ",
    "url": "http://localhost:4000/githubPage/integration-patterns-event-driven-integration#event-driven",
    "relUrl": "/integration-patterns-event-driven-integration#event-driven"
  },"185": {
    "doc": "Event Driven Integration",
    "title": "Overview",
    "content": "Event-driven architecture (EDA) is a common pattern that involves production, detection, consumption, and reaction to events. | Event-Driven Architecture (EDA) Workflow | . | | . Event Driven integrations are triggered by an event in one system and trigger a predefined corresponding event in another. An event can be anything that happens in a particular system that you’ve integrated with, such as e.g. order creation in online shopping, lead creation in a CRM, a new registration received, etc. Event driven integrations are suitable for functional integration. It is not a direct candidate option for syncing all data quickly and easily. Event driven integrations are suitable for simple point to point data transfer when certain events take place in a system and you want to trigger corresponding actions in the other system. ",
    "url": "http://localhost:4000/githubPage/integration-patterns-event-driven-integration#overview",
    "relUrl": "/integration-patterns-event-driven-integration#overview"
  },"186": {
    "doc": "Event Driven Integration",
    "title": "Enabling Event Integration for Legacy Systems",
    "content": "There are older systems which are not enabled to accept and act on events. You can choose to write event adaptors. These event adaptors intercept incoming events and pass them to the backend system in the native format. They then receive response from backend systems and put an optional outgoing event too. | EDA Adaptor | . | | . ",
    "url": "http://localhost:4000/githubPage/integration-patterns-event-driven-integration#enabling-event-integration-for-legacy-systems",
    "relUrl": "/integration-patterns-event-driven-integration#enabling-event-integration-for-legacy-systems"
  },"187": {
    "doc": "Event Driven Integration",
    "title": "Event Driven Data Synchronization",
    "content": "Data Driven integrations have a different action that triggers synchronization. This action is a change in the data that you have in place in either system. This can be a new record created or a change in an existing record and more. | EDA Integration | . | | . Any change in data will trigger a sync operation of the delta in the data to the other system(s). For example, a new entry in the order fulfillment system should trigger an update in the CRM system to reflect the customer’s order history. Even driven data integration makes easy to achieve an enterprise-class type of integration with easy setup and maintenance. One example of data synchronization is necessity to access data located in the on-premises data center from an application deployed in cloud. Data driven event based integration can come to rescue here. You can add a change data capture mechanism in front of the legacy datastore which can publish the change-sets of the data in order to synchronize your datastore in the cloud. Data driven integrations are suitable for integrations with systems where you want to keep your data in sync regardless of the event that takes place. ",
    "url": "http://localhost:4000/githubPage/integration-patterns-event-driven-integration#event-driven-data-synchronization",
    "relUrl": "/integration-patterns-event-driven-integration#event-driven-data-synchronization"
  },"188": {
    "doc": "Event Driven Integration",
    "title": "Event Driven Integration",
    "content": " ",
    "url": "http://localhost:4000/githubPage/integration-patterns-event-driven-integration",
    "relUrl": "/integration-patterns-event-driven-integration"
  },"189": {
    "doc": "Integration Patterns",
    "title": "Integration Patterns",
    "content": ". ",
    "url": "http://localhost:4000/githubPage/integration-patterns",
    "relUrl": "/integration-patterns"
  },"190": {
    "doc": "Integration Patterns",
    "title": "Integration Approaches",
    "content": ". | APIfication | Event Driven Integration | . ",
    "url": "http://localhost:4000/githubPage/integration-patterns#integration-approaches",
    "relUrl": "/integration-patterns#integration-approaches"
  },"191": {
    "doc": "Integration Patterns",
    "title": "Coexistence Patterns",
    "content": ". | Coexistence scenarios . | Data Management Patterns . | Change Data Capture | Transformation and Filtering Engine | Data Adapter | Data Virtualization | . | Transaction Management Patterns . | SAGA | TCC | . | . Request Management Patterns . | Strangler | . | . ",
    "url": "http://localhost:4000/githubPage/integration-patterns#coexistence-patterns",
    "relUrl": "/integration-patterns#coexistence-patterns"
  },"192": {
    "doc": "Role of ISVs",
    "title": "Role of ISVs",
    "content": " ",
    "url": "http://localhost:4000/githubPage/isv-role#role-of-isvs",
    "relUrl": "/isv-role#role-of-isvs"
  },"193": {
    "doc": "Role of ISVs",
    "title": "Chapter Layout",
    "content": ". | Objective | Key Questions to Ask | IBM’s Approach | Sighed Benefits  | Risks | Client Stories | . ",
    "url": "http://localhost:4000/githubPage/isv-role#chapter-layout",
    "relUrl": "/isv-role#chapter-layout"
  },"194": {
    "doc": "Role of ISVs",
    "title": "Role of ISVs",
    "content": " ",
    "url": "http://localhost:4000/githubPage/isv-role",
    "relUrl": "/isv-role"
  },"195": {
    "doc": "Just-in-time Architecture",
    "title": "Just in time architecture",
    "content": "Flexibility to change on the fly . ",
    "url": "http://localhost:4000/githubPage/archived/engineering/just-in-time/#just-in-time-architecture",
    "relUrl": "/archived/engineering/just-in-time/#just-in-time-architecture"
  },"196": {
    "doc": "Just-in-time Architecture",
    "title": "Just-in-time Architecture",
    "content": " ",
    "url": "http://localhost:4000/githubPage/archived/engineering/just-in-time/",
    "relUrl": "/archived/engineering/just-in-time/"
  },"197": {
    "doc": "Key Drivers",
    "title": "Key Drivers",
    "content": ". Rapid technology advances are driving higher expectations around speed, efficiency and security. Table of Contents . | Business Expectations | Technology Trends . | Distributed Cloud | Data Fabrics | Hyperautomation | Multiexperience | Hybrid Integration | Edge &amp; IoT | Application Composition | Cognitive Augmentation | Technology velocity and security | . | Market Analysis Forecast | . ",
    "url": "http://localhost:4000/githubPage/drivers",
    "relUrl": "/drivers"
  },"198": {
    "doc": "Key Drivers",
    "title": "Business Expectations",
    "content": "Infuse technology to drive speed to market, create new revenue streams and enhance customer experience. | Business Expectation | . | | . | IoT allows to draw from multiple data sources in real time. Optimizing productivity with predictive maintenance, end-to-end connected vehicle and monitoring courier package status through Industrial IoT. | 5G and Edge enabling compute and storage options. Virtualization of Telecom Network Function and enhancing Immersive Virtual Reality Experience through 5G and Edge technologies. | Analytics enable to make sense of data and identify new opportunities with access to unique data sets. Increasing hyper-personalization, Fraud Detection and unified view of customers, products and channels through real-time data capture Analytics. | . ",
    "url": "http://localhost:4000/githubPage/drivers#business-expectations",
    "relUrl": "/drivers#business-expectations"
  },"199": {
    "doc": "Key Drivers",
    "title": "Technology Trends",
    "content": "Technology evolutions drive innovation through Data, Integration, AI and Automation. Distributed Cloud . Allows data centers to be located anywhere. This solves both technical issues like latency and also regulatory challenges like data sovereignty. It also offers the benefits of a public cloud service alongside the benefits of a private, local cloud. Data Fabrics . A core component of digitization is treating data as an asset. To be successful with digitization, businesses must use data management and governance techniques to free data from supporting IT components. Hyperautomation . Application of advanced technologies, including AI and ML, to increasingly automate processes and augment humans. It refers to the sophistication of the automation (i.e., discover, analyze, design, automate, measure, monitor, reassess). Multiexperience . Immersive experiences that use AR, VR, mixed reality, multichannel human-machine interfaces and sensing technologies. The combination of these technologies can be used for a simple AR overlay or a fully immersive VR experience. Hybrid Integration . Unified, cloud agnostic integration platform that enables internal, external, edge, IoT, SaaS, market place and partner ecosystem integration using traditional and advanced integration technologies, e.g. event, messages, data transfer and streams. Edge &amp; IoT . Information processing and content collection and delivery are placed closer to the sources of the information moving key applications and services closer to the people and devices that use them.. Application Composition . Composable business functions, services and workflows from traditional applications, enabled by APIs, microservices and containers. Cognitive Augmentation . Enhances a human’s ability to think and make better decisions, for example, exploiting information and applications to enhance learning or new experiences. Technology velocity and security . Technologies evolve rapidly, enabled by open source and cloud ecosystems and the need for increased security. ",
    "url": "http://localhost:4000/githubPage/drivers#technology-trends",
    "relUrl": "/drivers#technology-trends"
  },"200": {
    "doc": "Key Drivers",
    "title": "Market Analysis Forecast",
    "content": "Market analysis forecasts increasing adoption and growth for cloud based applications: . | Cloud applications market size is expected to grow from USD 171 billion in 2020 to USD 356 billion by 2025, at a Compound Annual Growth Rate (CAGR) of 15.8%. | By 2022, modernization of existing applications and developments of new cloud-native applications will increase the percentage of cloud-native production applications to 25%. | By 2023, 60% of the G2000 enterprise will have created their own software ecosystem. | The enterprises that build cloud based applications: . | 77% of enterprises say they develop and deploy applications faster. | 67% of enterprises say they respond to market changes and demands faster. | 86% of organizations report improvement in application performance. | . | . | Market Analysis | . | | . ",
    "url": "http://localhost:4000/githubPage/drivers#market-analysis-forecast",
    "relUrl": "/drivers#market-analysis-forecast"
  },"201": {
    "doc": "Key Drivers",
    "title": "Key Drivers of Application Development",
    "content": ". This section highlights the key drivers of the evolution of application development, what our clients are doing and how this is happening. Table of Contents . | Key Drivers | What Is Evolving? | What Are Our Clients Doing? | . ",
    "url": "http://localhost:4000/githubPage/key-drivers#key-drivers-of-application-development",
    "relUrl": "/key-drivers#key-drivers-of-application-development"
  },"202": {
    "doc": "Key Drivers",
    "title": "1. Key Drivers",
    "content": "Rapid technology advances are driving higher expectations around speed, efficiency and security. ",
    "url": "http://localhost:4000/githubPage/key-drivers#1-key-drivers",
    "relUrl": "/key-drivers#1-key-drivers"
  },"203": {
    "doc": "Key Drivers",
    "title": "1.1. Business Expectations",
    "content": "Infuse technology to drive speed to market, create new revenue streams and enhance customer experience. | Business Expectation | . | | . | IoT allows to draw from multiple data sources in real time. Optimizing productivity with predictive maintenance, end-to-end connected vehicle and monitoring courier package status through Industrial IoT. | 5G and Edge enabling compute and storage options. Virtualization of Telecom Network Function and enhancing Immersive Virtual Reality Experience through 5G and Edge technologies. | Analytics enable to make sense of data and identify new opportunities with access to unique data sets. Increasing hyper-personalization, Fraud Detection and unified view of customers, products and channels through real-time data capture Analytics. | . ",
    "url": "http://localhost:4000/githubPage/key-drivers#11-business-expectations",
    "relUrl": "/key-drivers#11-business-expectations"
  },"204": {
    "doc": "Key Drivers",
    "title": "1.2. Technology Trends",
    "content": "Technology evolutions drive innovation through Data, Integration, AI and Automation. Distributed Cloud . Allows data centers to be located anywhere. This solves both technical issues like latency and also regulatory challenges like data sovereignty. It also offers the benefits of a public cloud service alongside the benefits of a private, local cloud. Data Fabrics . A core component of digitization is treating data as an asset. To be successful with digitization, businesses must use data management and governance techniques to free data from supporting IT components. Hyperautomation . Application of advanced technologies, including AI and ML, to increasingly automate processes and augment humans. It refers to the sophistication of the automation (i.e., discover, analyze, design, automate, measure, monitor, reassess). Multiexperience . Immersive experiences that use AR, VR, mixed reality, multichannel human-machine interfaces and sensing technologies. The combination of these technologies can be used for a simple AR overlay or a fully immersive VR experience. Hybrid Integration . Unified, cloud agnostic integration platform that enables internal, external, edge, IoT, SaaS, market place and partner ecosystem integration using traditional and advanced integration technologies, e.g. event, messages, data transfer and streams. Edge &amp; IoT . Information processing and content collection and delivery are placed closer to the sources of the information moving key applications and services closer to the people and devices that use them.. Application Composition . Composable business functions, services and workflows from traditional applications, enabled by APIs, microservices and containers. Cognitive Augmentation . Enhances a human’s ability to think and make better decisions, for example, exploiting information and applications to enhance learning or new experiences. Technology Velocity and Security . Technologies evolve rapidly, enabled by open source and cloud ecosystems and the need for increased security. ",
    "url": "http://localhost:4000/githubPage/key-drivers#12-technology-trends",
    "relUrl": "/key-drivers#12-technology-trends"
  },"205": {
    "doc": "Key Drivers",
    "title": "1.3. Market Analysis Forecast",
    "content": "Market analysis forecasts increasing adoption and growth for cloud based applications: . | Cloud applications market size is expected to grow from USD 171 billion in 2020 to USD 356 billion by 2025, at a Compound Annual Growth Rate (CAGR) of 15.8%. | By 2022, modernization of existing applications and developments of new cloud-native applications will increase the percentage of cloud-native production applications to 25%. | By 2023, 60% of the G2000 enterprise will have created their own software ecosystem. | The enterprises that build cloud based applications: . | 77% of enterprises say they develop and deploy applications faster. | 67% of enterprises say they respond to market changes and demands faster. | 86% of organizations report improvement in application performance. | . | . | Market Analysis | . | | . Back to top . ",
    "url": "http://localhost:4000/githubPage/key-drivers#13-market-analysis-forecast",
    "relUrl": "/key-drivers#13-market-analysis-forecast"
  },"206": {
    "doc": "Key Drivers",
    "title": "2. What Is Evolving?",
    "content": "A lot is being re-imagined about Application Development - how its done, why its done, who does it. Enterprises are building new applications not just to automate and drive out costs but to allow them to flex &amp; to scale and to reach out to newer markets in unimagined ways. | What is evolving | . | | . ",
    "url": "http://localhost:4000/githubPage/key-drivers#2-what-is-evolving",
    "relUrl": "/key-drivers#2-what-is-evolving"
  },"207": {
    "doc": "Key Drivers",
    "title": "2.1. Shift in Focus",
    "content": "Software engineering is now seen as a core compentency they need to build not only to help them achieve their business goals but also to be competitive and innovative in their industry. Therefore IT programs are shifting from a project delivery focus to a more continuous product centric view which involves long term thinking, agile &amp; automated IT processes and long running teams. This has introduced newer organizational roles &amp; disciplines e.g. Product Management bridging the gap between business users and IT teams, Release Engineer &amp; Site Reliability Engineers bridging the gap between erstwhile IT projects and IT operations. This has also meant that enterprises are now wanted to build more skills in this space instead of outsourcing the delivery of an IT system. IBM has well understood this evolution and the need for enterprises to be engaged deeply in this process and is helping its clients take this journey using our Co-Creation model IBM Garage Method. ",
    "url": "http://localhost:4000/githubPage/key-drivers#21-shift-in-focus",
    "relUrl": "/key-drivers#21-shift-in-focus"
  },"208": {
    "doc": "Key Drivers",
    "title": "2.2. Cognitive Thinking &amp; Models",
    "content": "Cognitive technologies have transformed business processes and business models by becoming central to decision making in real time based on accumulated systemic intelligence. This has led to an entirely different way of developing applications. Here are some of the considerations of building apps using AI and Machine Learning technologies: . | No more building logic trees, build models | Training over testing | Crowd sourcing data over generating synthetic data | . This implies that organizations require a new set of capabilities. Effectiveness of machine learning models are a direct result of the quality of training data. Right set of tools and practices need to be adopted to build training data and manage it with the evolving system. ",
    "url": "http://localhost:4000/githubPage/key-drivers#22-cognitive-thinking--models",
    "relUrl": "/key-drivers#22-cognitive-thinking--models"
  },"209": {
    "doc": "Key Drivers",
    "title": "2.3. New Platform Capabilities",
    "content": "There is also evolution in application development driven by new capabilities being made available by underlying and emerging platforms. Public cloud platforms have evolved to make almost all functions available through CLI and API interfaces allowing for all administration functions to be performed in automated and autonomous manner. There is also emergence of Low Code No Code platforms enabling rapid application development by a wider range of skilled resources including the likes of Business Analysts and Product Owners. Moreover, availability of such hosting platforms on previously unsupported environments like Edge devices has triggered development of new kind of workloads which weren’t possible earlier. Back to top . ",
    "url": "http://localhost:4000/githubPage/key-drivers#23-new-platform-capabilities",
    "relUrl": "/key-drivers#23-new-platform-capabilities"
  },"210": {
    "doc": "Key Drivers",
    "title": "3. What Are Our Clients Doing?",
    "content": "This section describes some cross-cutting client stories which are examples for new age application development models. ",
    "url": "http://localhost:4000/githubPage/key-drivers#3-what-are-our-clients-doing",
    "relUrl": "/key-drivers#3-what-are-our-clients-doing"
  },"211": {
    "doc": "Key Drivers",
    "title": "3.1. BNP Paribas: Multicloud Transformation &amp; Operating Model",
    "content": "IBM worked with BNP Paribas over five months to deliver a Strategic Engagement defining its 2022 Multicloud Transformation and Operating Model, supported by a Cloud Competence Center, specifically to provide: . | Creation of new Cloud Services, in particular PaaS | Integration of brokered Public Cloud Services | Adoption of new skills and roles to drive an Agile culture | Review of security and compliance for Cloud and DevOps | Review of app portfolio for containerization and migration | Design of key assets, including Architecture Center Portal | Design of a multi-Cloud DevOps Toolchain and Practice | . This covered a multicloud &amp; legacy portfolio of 4.000+ Applications in terms of service catalog, chargeback, architecture topology, toolchain, migration, etc. Client Context . | Among the world’s Top 10 Banks based out of Europe; was lacking an accelerated approach for a holistic Cloud Transformation. | The Project dedicated to the Client-IBM JV (multi-billion contract), providing IT &amp; Cloud services to the Group’s business units. | New Group CIO demanding IT to show true Cloud capabilities comparable to leading market player to avoid increasing shadow IT. | Strategy &amp; Initiation phase spanning across the IT organization including Operating Model and CCC Creation. | . IBM Solution . JV with IBM: A holistic Multicloud Transformation &amp; Business-centric Operating Model driven through a CCC. | IBM Solution | . | | . | 3 use-case based pilot solutions in the process of rollout: DevOps Toolchain, Architecture Center Portal, IBM Cost &amp; Asset Management. | 8 new Services / Capabilities identified as 2019 backlog. Initial Cloud Affinity Assessment of 3000+ Applications performed (using BlueCAT). | 2 New organizational entities, CCC and Transformation Management Unit designed for “Crawl” phase. | A holistic 2022 Cloud Transformation Roadmap and Governance System. | . ",
    "url": "http://localhost:4000/githubPage/key-drivers#31-bnp-paribas-multicloud-transformation--operating-model",
    "relUrl": "/key-drivers#31-bnp-paribas-multicloud-transformation--operating-model"
  },"212": {
    "doc": "Key Drivers",
    "title": "Key Drivers",
    "content": " ",
    "url": "http://localhost:4000/githubPage/key-drivers",
    "relUrl": "/key-drivers"
  },"213": {
    "doc": "Monitoring",
    "title": "Monitoring",
    "content": ". Table of Contents . | Overview | Cloud Management | Security Operations Center (SOC) | Services Control Tower | Integrated DevOps | Security Cloud Framework | . ",
    "url": "http://localhost:4000/githubPage/devsecops-monitoring",
    "relUrl": "/devsecops-monitoring"
  },"214": {
    "doc": "Monitoring",
    "title": "Overview",
    "content": "Having an integrated multicloud services management delivers: . | Ultimate flexibility, freedom of choice and avoidance of vendor lock-in | Simplified and delayered cloud and application management functions | Consistency in monitoring, optimized data/workload placement, improved service management quality | Reduced risk, enhanced resiliency, and improved reliability in your entire cloud operations | Increased transparency, faster innovation and reduced cost through simplified governance | . | Integrated Service Management | . | | . ",
    "url": "http://localhost:4000/githubPage/devsecops-monitoring#overview",
    "relUrl": "/devsecops-monitoring#overview"
  },"215": {
    "doc": "Monitoring",
    "title": "Cloud Management",
    "content": "Manage traditional and cloud environments seamlessly across hybrid multicloud using our enterprise grade cloud management platform. IBM delivers the only enterprise grade solution for managing hybrid multicloud which provides: . | A unified operations layer that enables infrastructure and applications to be monitored and maintained including legacy infrastructure, private cloud, public cloud and container environments | A robust management platform with built in capabilities including inventory, orchestration, operations, and service management across cloud infrastructures. | An optimized digital self-service user experience to consume, deploy, operate, and govern across all clouds and data centers through a single pane of glass | . | Cloud Management | . | | . ",
    "url": "http://localhost:4000/githubPage/devsecops-monitoring#cloud-management",
    "relUrl": "/devsecops-monitoring#cloud-management"
  },"216": {
    "doc": "Monitoring",
    "title": "Security Operations Center (SOC)",
    "content": "The SOC proactively monitors, avoids outages, quickly troubleshoots and continuously improves through: . | Single point of contact &amp; pane of glass view into Hybrid multicloud provider landscape which delivers application run/operate/service delivery with cost savings of up to 20% | Integrated business services operations and management using proven SRE practices and AI-infused automation | Embedded Hybrid IT Security assures data protection, resiliency, and compliance | . | Security Operations Center (SOC) | . | | . ",
    "url": "http://localhost:4000/githubPage/devsecops-monitoring#security-operations-center-soc",
    "relUrl": "/devsecops-monitoring#security-operations-center-soc"
  },"217": {
    "doc": "Monitoring",
    "title": "Services Control Tower",
    "content": "Services Control Tower enables you to manage the Cloud Operating Model with meaningful insights to redefine business strategies. The IBM Services Control Tower summarizes key metrics and allows you to manage and simplify digital, cognitive and cloud chaos, translating cloud complexity into tangible enterprise business strategies. It provides: . | End-to-end visibility to manage operations | Produce early warnings for issues with automatic resolutions driving quicker response times | Reduce overall operating costs but increase business flexibility | Achieve the promise of the cloud operating benefits | . | Services Control Tower | . | | . ",
    "url": "http://localhost:4000/githubPage/devsecops-monitoring#services-control-tower",
    "relUrl": "/devsecops-monitoring#services-control-tower"
  },"218": {
    "doc": "Monitoring",
    "title": "Integrated DevOps",
    "content": "Adopt an integrated DevOps platform for hybrid multicloud – offered as a shared, dedicated or customized model. Embrace DevOps across the enterprise with new ways of operating and the tools required to accelerate software delivery, improve quality and enhance customer experience . | The IBM DevOps Commander enables tool chain orchestration with customized, real time builds of your DevOps environment, almost 70% faster than traditional methods. | Standardized, integrated DevOps tool chain services available across leading cloud platforms | Build a customized DevOps tool chain and integrate already deployed DevOps tools in less than an hour | Incorporates market leading and open source software engineering tools | Best in class security practices integrated into the software engineering lifecycle | Secure testing integration creates a robust DevSecOps environment | . ",
    "url": "http://localhost:4000/githubPage/devsecops-monitoring#integrated-devops",
    "relUrl": "/devsecops-monitoring#integrated-devops"
  },"219": {
    "doc": "Monitoring",
    "title": "Security Cloud Framework",
    "content": "Traditional security controls and infrastructure operational practices are changing to data and workload-centric cloud security policies, technologies, and practices. We approach this challenge by aligning security controls with the realities of your hybrid multicloud IT estate to: . | Enforce Application security via DevSecOps practices | Cloud Platform security | Identification and planning for enterprise / regulatory specific controls / compliance / audits | Data and Storage security and privacy | Security threat monitoring | Security incident management | Business risk and compliance services | . | Security Framework | . | | . ",
    "url": "http://localhost:4000/githubPage/devsecops-monitoring#security-cloud-framework",
    "relUrl": "/devsecops-monitoring#security-cloud-framework"
  },"220": {
    "doc": "New Age Model",
    "title": "Future Application Development – New Age Model?",
    "content": "(To be developed) . ",
    "url": "http://localhost:4000/githubPage/new-age-model#future-application-development--new-age-model",
    "relUrl": "/new-age-model#future-application-development--new-age-model"
  },"221": {
    "doc": "New Age Model",
    "title": "New Age Model",
    "content": " ",
    "url": "http://localhost:4000/githubPage/new-age-model",
    "relUrl": "/new-age-model"
  },"222": {
    "doc": "Organization",
    "title": "Organization &amp; Functions",
    "content": ". Table of Contents . | Overview | Centre of Excellence . | Strategy, Architecture &amp; Demand . | Purpose | Core Activities | Roles | . | Adoption &amp; Communities . | Purpose | Core Activities | Roles | . | Education, Talent, and Skills . | Purpose | Core Activities | Roles | . | Security, Risk &amp; Compliance . | Purpose | Core Activities | Roles | . | . | Platform Engineering &amp; Operations . | Brokerage &amp; Orchestration . | Purpose | Core Activities | Roles | . | API &amp; Integration Management . | Purpose | Core Activities | Roles | . | Infrastructure Management . | Purpose | Core Activities | Roles | . | Platform Product Engineering and Management . | Purpose | Core Activities | Roles | . | . | Application Engineering &amp; Ops . | Application Product Engineering Squads . | Purpose | Core Activities | Roles | . | . | Integrated Service Management . | Customer Assistance . | Purpose | Core Activities | Roles | . | Operations Command Centre . | Purpose | Core Activities | Roles | . | Service Excellence . | Purpose | Core Activities | Roles | . | . | . ",
    "url": "http://localhost:4000/githubPage/organization#organization--functions",
    "relUrl": "/organization#organization--functions"
  },"223": {
    "doc": "Organization",
    "title": "Overview",
    "content": "| 4 Pillars | . | | . You have earlier seen how enterprises are organising themselves in the digital and multicloud world and the four kind of organizations thus forming. Within each of these elements, there are various functions that support product development and operations: . | 4 Pillars Function | . | | . ",
    "url": "http://localhost:4000/githubPage/organization#overview",
    "relUrl": "/organization#overview"
  },"224": {
    "doc": "Organization",
    "title": "Centre of Excellence",
    "content": ". Strategy, Architecture &amp; Demand . Purpose . | Provides overall direction by being responsible for aligning both cloud and IT strategy with the broader enterprise strategy. | Owns and iterates a “to-be” cloud vision, and provides governance to ensure projects, services and products are operated in a standardised and strategical aligned method. | Defines and maintains the Enterprise’s cloud strategy, policy and architectural references (including integration, infrastructure, software &amp; licenses and disaster recovery). | Collects new requirements for functionality to be implemented. | Acts as a consultant and/or advisor for Lines of Business expected to consume its products and services, consulting on delivery and best practices associated with cloud technologies. | . Core Activities . Responsible and Accountable for: . | Continuously reviewing and communicating cloud strategy against business strategy objectives | Defining and maintaining cloud reference architecture, patterns, principles and standards for the Cloud Platform | Managing cloud architecture governance on behalf of the Centre of Excellence | Developing and maintaining guidelines and policies for cloud integration, cloud infrastructure, configuration management, software &amp; license management and disaster recovery | Defining and maintaining target “to-be” cloud estate and transition states for cloud implementation | Understanding customer needs and validating solutions from customer perspective. | Translating business requirements into Solution Design Documents and initial product or service roadmap (implemented by AEO/PEO) | Monitoring and tracking new business requirements and technology approaches based on demand | . Consulted on activities linked to Platform Engineering Operations, Application Engineering Operations, and Integrated Service Management. Roles . | Centre of Excellence Lead — Define and agree Vision, Mission, and Goals for Cloud Strategy Operating Model — in line with Business Outcomes set by Enterprise C-Suite—and drive alignment of IT products &amp; services with consumer demands and BoB innovation | Architecture Squads [Chief Solution Architect; Cloud Platform Architect; Transformation SRE; Security Consultant; DevSecOps Engineer] — Determine cloud architecture strategy (and associated policies and standards) and data stewardship standards in tandem with enterprise architecture strategy; Defines the service integration strategy and architecture for the enterprise; Drive alignment for a full stack product or service solution that meets customer demand | . Adoption &amp; Communities . Purpose . Collects new requirements for functionality to be implemented into the Centre of Excellence, Application Engineering &amp; Operations, Platform Engineering &amp; Operations, and Integrated Service Management. Acts as a consultant and/or advisor for Lines of Business and Delivery on best practices associated with cloud technologies, application modernisation and application migration. Core Activities . Responsible and Accountable for: . Communications Strategy and Marketing . | Continuously develop and communicate the vision to Enterprise stakeholders | Determine marketing communication methods and frequency  | . Methods and Knowledge . | Develop, maintain and share relevant industry and business knowledge | Maintain and create standard development methods to share across all areas of the business | Share information on existing cloud capabilities to Enterprise | Implement, update and maintain mechanisms for cloud knowledge management | Maximise reuse by sharing of solutions across AEO and PEO teams | . Cloud Transformation/Migration Support (as needed) . | Support the migration to Cloud of the legacy IT estate (where appropriate) by defining the approach and principles appropriate for an application centric migration | Perform cloud affinity assessment &amp; workload analysis for each service/application proposed to move to the Cloud | Run cloud affinity and workload analysis for all departments/sectors/businesses within the organisation | Create cloud transformation roadmap based on affinity, workload and difficulty ratings. | Provide estimates for modernisation of applications | . Roles . | Centre of Excellence Lead – Leads cloud within the Enterprise | Cloud Ambassador - Acts as a consumer advocate for cloud services | Cloud Engagement Manager - Interfaces with the Lines of Business to understand and interpret business needs | . Education, Talent, and Skills . Purpose . Manages resourcing, skills assessments, upskilling &amp; development and the arrival of expertise at the right time. Leverage existing talent within the Enterprise, procure talent outside the Enterprise, and upskill individuals with skills that best align to business and IT needs. Core Activities . Responsible and Accountable for - . | Analysing current team skills and  skill gaps | Leveraging partner organisations to augment roles within Organisation as required | Establishing re-skilling initiatives | Establishing initiatives, processes and policies to capture and retain cloud talent | Establishing cloud enablement education programs, workshops and seminars | Mapping cloud education and training to roles and responsibilities | Establishing cloud re-skilling programs | Conducting training of user communities and IT support resources | Educating relevant stakeholders in their responsibility for compliance | Organising and managing community events | Integrating external community partner services into portal | Setting up partner external community relationship | . Roles . | People and Talent Lead – Create business case for talent (upskilling of existing talent, acquisition of new talent, cross-skilling of talent, etc) to align with evolving needs of business delivered by the IT organisation; Own relationship between HR and the IT organisation to ensure both are tightly coupled | Cloud Education Specialist – Drives cloud operational efficiency by building cloud training offerings | Talent Acquisition Manager – Coordinate and drive acquisition of talent in line with business skillset requirements | . Security, Risk &amp; Compliance . Purpose . Provides confidence to Enterprise that IT products and services meet the regulatory and compliance requirements as outlined by geographical regions, industry bodies and Enterprise policy. Outline, review, iterate and communicate controls in line with the agreed organisational risk and compliance framework. Provide SecOps function with confirmation that cloud Platform and associated cloud products and services comply with Enterprise security rules and standards. Set, manage, and report on all security standards whilst assessing new and existing projects and tooling for compliance with regulation. Core Activities . Responsible and Accountable for: . | Defining application, network, data, identity and access management, and system security guidelines and policies (in line with best practice for cloud products &amp; services) | Understanding regulatory and confidentiality requirements and policies and add into products and services backlogs | Documenting the internal and external context of each security, privacy and data protection risk, and the evaluation criteria | Reviewing and monitoring physical and logical data security measures | Reviewing and monitoring application security controls | Monitoring network intrusion detection data | Creating and maintaining the policies and supporting controls for cloud under enterprise risk management framework | Managing, maintaining and reporting on compliance policies, standards and processes (following predefined cadence) | Determining and understanding the relevant regulatory and audit requirements | Embedding ownership and responsibility for IT-related risks within the business at an appropriate senior level | Identifying and evaluating risks related to information security, cloud project management, cloud sourcing, etc. | Identifying and prioritising projects to enhance cloud compliance and to remediate identified issues/risks | . Roles . | Cloud Auditor - Ensures Cloud environment compliance is being performed according to the agreed and documented processes | Cloud Service Security &amp; Risk Manager – Implements security and compliance policy | . ",
    "url": "http://localhost:4000/githubPage/organization#centre-of-excellence",
    "relUrl": "/organization#centre-of-excellence"
  },"225": {
    "doc": "Organization",
    "title": "Platform Engineering &amp; Operations",
    "content": ". Brokerage &amp; Orchestration . Purpose . Ensures the commercial and/or contractual arrangements associated with the third parties (e.g., ATOS, cloud vendors, etc.) that are part of the provision and implementation of cloud-related products and services are strategically planned and managed such that they deliver maximum value, low risk and are financially beneficial to the Enterprise. Core Activities . | Defining and implementing a category strategy for procuring cloud services (i.e. vendor management strategy) | Defining and running the partner qualification, selection and management approach | Managing active/ongoing contracts | Procuring/purchasing cloud services &amp; products | Providing financial guidance for funding requests, procurement, and business case development for cloud services | Developing and operating the financial model and process for cloud products and services | Publishing &amp; maintaining agreed cloud service pricing models | Providing a centralised billing capability and aggregating and reconciling bill for each cloud customer | Establishing CSOM business accounting and funding mechanisms | Creating and maintaining cloud budgets | Providing financial management of cloud assets | Developing and managing accounting controls and standards | Establishing and operating financial reporting framework including the monitoring of all costs and variances | Negotiating relationships between providers and cloud consumers | Developing standard cloud customer contracts and relationship management approaches | . Roles . | Cloud Pricer - Manages cost of Cloud services offering | Cloud Service Provider Management - Negotiates relationships between cloud providers and cloud consumers. Interfaces with external service suppliers to support purchasing and contract management | Cloud Financial Manager - Manages and enforces all aspects of CSP cost and consumer pricing | Cloud Marketplace Manager –Manages the marketplace evolution and ensures access to the services for end users | . API &amp; Integration Management . Purpose . Publish, promote and oversee application programming interfaces in a secure, scalable environment, including end user support resources that define and document the API. Create business agility to drive rapid business reconfiguration (i.e. to revamp customer experience, address regulatory challenges (GDPR), etc.). Provide ease of use for APIs. Track and manage APIs in use. Build a framework for enforcing agreements on API use and security access control. Core Activities . | Planning, design, implementation, testing, publication, operation, consumption, maintenance, versioning and retirement of APIs | Elaborating channel IT design requirements | Delivering a developer portal through which to target, market to and govern communities of developers who embed APIs | Conducting runtime management | Enforcing relevant API security policies and requests and also guarantees authorization and security.  | Estimating APIs’ value | Using analytics to understand patterns of API usage [i.e. monitor API usage, load, transaction logs, historical data and other metrics that better inform the status as well as the success of the APIs available]  | Informing developers when an API is scheduled to be retired or deprecated, and what alternative APIs might be offered (frequently with a new pricing plan) | . Roles . | Cloud Service Integration Architect—Defines the service integration strategy and architecture for the enterprise | Cloud Service Composer &amp; Developer—Creates/bundles products and services for end-user consumption | Cloud Service Integration Engineer—Technical focal point for cloud service integration within the IT ecosystem | . Infrastructure Management . Purpose . Maintain legacy estate (data centres, hardware, networks, mainframe, etc.) to ensure access and performance are uncompromised while exploring areas for transformation in alignment with business transformation goals and strategic priorities. Core Activities . | Managing physical infrastructure platform according to internal service level agreements | Managing physical capacity expansion in-line with growth forecasts | Managing physical assets usage optimisation and currency | Managing to policy compliance | Conducting server configuration, server commissioning and decommissioning, server and backup management, server management remote and onsite, desktop management, database management, and network management LAN/WAN | Understanding implications/constraints of enterprise application and infrastructure architectures | Determining integration requirements between IT services and solutions | Identify opportunities for shared capabilities at a service/solution implementation level | Developing and maintaining service and solution architecture | . Roles . | Infrastructure Manager—Manage existing core infrastructure to maintain performance and availability | . Platform Product Engineering and Management . Purpose . Design, develop, test and deploy cloud platform to best align with evolving marketplace demands for cloud products in coordination with Application Development Squads. Ensure platform is built in line with security, risk, and compliance policies and standards and adheres to standards around capacity, availability, and reliability. Perform proactive and reactive management of platform products. Core Activities . | Defining platform architecture specific to the applications (in consultation with App Dev Squads) | Designing development tools environment changes specific to the applications (in consultation with App Dev Squads) | Configuring DevOps environments, tools &amp; automation frameworks for the development of platform | Leveraging DevOps &amp; Automation to build &amp; deploy the platforms needed for common services | Ensuring reliability is built into common platform | Proactively managing and continuously improving the platform | Developing and deploying automation and other best practices to address opportunities | Performing reactive management of the platform(s) to the required SLAs and contractual agreements | . Roles . | Platform Build Squads [Cloud Platform Architect; Platform Security and Quality Assurance; Platform SREs; Platform Specialists; Platform Developer (patterns/automation)] — Build and manage platform services in Agile manner | Shared Platform Squad — DevOps and Tooling — jointly design the development tools environment changes specific to the application group, leveraging common services | . ",
    "url": "http://localhost:4000/githubPage/organization#platform-engineering--operations",
    "relUrl": "/organization#platform-engineering--operations"
  },"226": {
    "doc": "Organization",
    "title": "Application Engineering &amp; Ops",
    "content": ". Application Product Engineering Squads . Purpose . Innovate, design, develop, test and deploy an application (in Agile teams) to align with evolving marketplace demands for cloud products. Coordinate with Lines of Business, in alignment with a particular Value Stream (e.g.) to define and manage information throughout the product lifecycle. Leverage cutting edge technologies and ways of working to remain tightly coupled with delivering products that delight customers. Core Activities . | Understanding consumer demands and trends to ensure product development properly addresses needs in the marketplace | Translating consumer needs into requirements and own lifecycle for application (in consultation with Platform Squads, as applicable) | Developing common services to meet the functional requirements, following design created by architects | Ensuring non-functional requirements are met and Build to Manage principles are followed | Leveraging DevOps &amp; Automation to build &amp; deploy products | Proactively managing and continuously improving the applications | Developing and deploy automation and other best practices to address opportunities | Maintaining the functional elements of applications | Maintaining non-functional elements (in consultation with Platform SREs) | . Roles . | Application Development Squads [Application Developers; Application Security and Quality Assurance; Application SREs]—Build and manage applications in Agile manner | User Researcher—Research marketplace demands to ensure aliment with existing product portfolio; Identify areas for new products based on evolutions in the marketplace | User Experience Designer—Design products based on insights from UX in consultation with App Dev Squads | DevOps Transformation Lead Engineer—Coordinates DevOps practices definition and adoption | . ",
    "url": "http://localhost:4000/githubPage/organization#application-engineering--ops",
    "relUrl": "/organization#application-engineering--ops"
  },"227": {
    "doc": "Organization",
    "title": "Integrated Service Management",
    "content": ". Customer Assistance . Purpose . Own and manage relationship with consumers of cloud products and services to ensure consistent, high quality interactions. Monitor quality using OKRs and report results to stakeholders to visibility into quality on regular basis. Core Activities . | Owning and managing relationship with consumers of cloud products and services | Managing day to day interactions with customers (internal and external) | Ensuring high quality customer interactions via comprehensive monitoring and reporting | Monitoring quality using OKRs agreed by Cloud Customer Support &amp; Care resource | Liaising with Operations Command Centre on resolution of complex customer service situations | . Roles . | Cloud Customer Support &amp; Care –Assists with usage and change management as first point of contact | . Operations Command Centre . Purpose . Owns the operation of the cloud platform and ensures its alignment with standard Enterprise operating procedures. Implement and operate Enterprise cloud platform services, enabling accelerated delivery of new cloud products and services in a controlled and secure manner. Ensure the guidelines and policies for operations are clearly defined and maintained. Core Activities . | Forecasting business volumes for cloud services | Providing Cloud User and Account Management | Conducting compliance/risk monitoring and reporting (upon consulting Risk &amp; SecOps) | Monitoring and managing risks related to ongoing cloud activities based on cadence identified in the Enterprise Risk Framework (including information security, cloud project management, and cloud sourcing) | Developing and maintaining Cloud infrastructure Release and deployment guidelines and policies | Developing and maintaining Configuration management guidelines and policies | Developing and maintaining software and license management guidelines &amp; policies | Developing back up and archive strategy and supporting guidelines &amp; policies | Developing and maintaining DR management guidelines &amp; policies | Developing event management strategy, guidelines and policies | . Roles . | Cloud Operations Manager –Owns daily operational activities | First Responder –Ensures rapid response to service level breaches | Site Reliability Engineer –Executes and optimizes automated operations | Cloud Operator –Supports cloud products &amp; services | . Service Excellence . Purpose . Monitoring and managing service to ensure quality is aligned with Service Management Strategy. Action changes in service (at either a strategic or tactical level) in accordance with Customer Service quality ratings. Monitor and manage service quality to ensure quality is sufficiently high enough to meet customer expectations. Core Activities . | Developing and maintaining KPI data collection guidelines | Developing and maintaining checklist of introduction of infrastructure to production | Understanding implications/constraints of enterprise applications | Assisting cloud migration execution (provision new environment, validate compliance, prepare tools, create runbook) | Providing support for early operational problems | Defining processes for resolving service performance issues and proactively identifying improvement opportunities | . Roles . | Cloud Service Manager –Owns daily operational activities | . ",
    "url": "http://localhost:4000/githubPage/organization#integrated-service-management",
    "relUrl": "/organization#integrated-service-management"
  },"228": {
    "doc": "Organization",
    "title": "Organization",
    "content": " ",
    "url": "http://localhost:4000/githubPage/organization",
    "relUrl": "/organization"
  },"229": {
    "doc": "Partner Reference Models",
    "title": "Partner Reference Models",
    "content": "Chapter Layout . | Objective | Key Questions to Ask | IBM’s Approach | Sighed Benefits  | Risks | Client Stories | . ",
    "url": "http://localhost:4000/githubPage/partner#partner-reference-models",
    "relUrl": "/partner#partner-reference-models"
  },"230": {
    "doc": "Partner Reference Models",
    "title": "Partner Reference Models",
    "content": " ",
    "url": "http://localhost:4000/githubPage/partner",
    "relUrl": "/partner"
  },"231": {
    "doc": "Digital Platforms",
    "title": "Digital Platforms &amp; Supply Chains",
    "content": ". Table of Contents . | Overview | Digital Platforms | Partnering &amp; Sourcing . | Considerations Design when designing your IT Supply Chain | . | Dynamic Delivery . | Shift 1 | Shift 2 | . | . ",
    "url": "http://localhost:4000/githubPage/platforms#digital-platforms--supply-chains",
    "relUrl": "/platforms#digital-platforms--supply-chains"
  },"232": {
    "doc": "Digital Platforms",
    "title": "Overview",
    "content": "This section tries to answer the following questions: . | How do I create a platform architecture to enable developers and business users to create business value? | How do I best utilise and access an ecosystem of partners to add value to what we do? | How has the recent pandemic changed traditional location-based working? | . ",
    "url": "http://localhost:4000/githubPage/platforms#overview",
    "relUrl": "/platforms#overview"
  },"233": {
    "doc": "Digital Platforms",
    "title": "Digital Platforms",
    "content": "Leading digital organisations are creating scalable, flexible and multi-level platforms that can be easily consumed by developers, business users and partners: . | Business Platforms - Developing business platforms and services that can be re-used within the rest of the organisation (both business users and development teams) and with partners and B2B customers | Technology Platforms - Creating platforms on-top of cloud services that embed automation, controls and security that are easily consumable by developers ‘as code’ or via a self-service portal via a single pane of glass | . | Digital Platforms | . | | . The key drivers for such platforms are: . | Engineered with open interfaces, ’standard’ based and easy to connect to | Are aligned and pre-built to architectural, service, compliance and security standards | Removes a level of operations that needs to be performed by developers so they can focus on adding value and creating digital products | Adds value on-top of ‘out-of-the-box’ cloud and technology services | Provides a set of stable core components, but which can support variety and evolution | Easily extensible, enabling scalability, not just on demand but based on strategy  | Encourages and facilitates innovation and novel usage | Opens up a business capability to be accessed by partners and customers | Allows the rest of the organisation and partners to add value and capture value through the interfaces | . Business platforms are built upon a foundation of technology platforms, connected in an open and transparent way . | Each platform provides a range of services that can be easily publish and consumed to construct digital products and services | These are surfaced either via APIs, CLI, self-service catalogues or through other user interfaces and visualisation engines (i.e. codeless BPM) | These can be combined together via open API and connectors | Each platform service should also provide APIs to monitor and operate services to provide visibility of availability and performance | . | Digital Platform Functions | . | | . ",
    "url": "http://localhost:4000/githubPage/platforms",
    "relUrl": "/platforms"
  },"234": {
    "doc": "Digital Platforms",
    "title": "Partnering &amp; Sourcing",
    "content": "An effective IT Supply Chain improves and organisation flexibility, cost effectiveness and ability to build capability. What is the IT Supply Chain? . The process off managing interactions with cloud providers (platform vendors, SaaS providers, service vendors) formally by selecting them based on their ability to meet identified requirements and managing performance against the agreed upon commitments within the Service Catalogue. Some of the key focus areas in this function include: . | Migrating to as-a-service commercial models for both technology infrastructure enabled by cloud computing and a flexible workforce that delivers outcome-driven engagements | Adopting a more liquid workforce as they try to bridge the skill gaps resulting from rapid changes in technology | Implement the right mix of local and global focus given the flexibility on cloud, new ways of working and need for shorter delivery cycles with high business involvement | . What benefits does it bring? . | Benefits from IT Supply Chain | . | | . Considerations Design when designing your IT Supply Chain . | Collaborative Sourcing Relationships - Shift away from focus on deliverables and service levels. Focus on a sourcing philosophy to embrace a partnership approach that can result in collaborative processes, skill development | Managed Capacity - As enterprises continuously evolve their strategies and roadmap, they would need to pull in capabilities and services on Demand | Open collaboration tooling - The new ways of working would need collaborative working practices across microsites. Supplier base model will need to adjust to match the DevOps Lifecycle | Service Brokerage and Orchestration - Provide a centralized catalog of internal and external cloud, business platform and Open APIs to fuel rapid innovation across the enterprise | Multisourcing Strategies - Unlike the conventional tower-based sourcing, Continuous Delivery in a cloud world require flexible sourcing strategies that allow multiple vendors collaborate with clearly defined roes | Opex Centric Consumption driven planning - The IT service framework now prices IT Services incrementally from the IT vendors to get accurate Operational Expense forecasting. Consumption driven planning with Business Product Owners | Predictive Demand Planning - Use in-depth analysis and predictive forecasts to anticipate demand and initiate sourcing with sufficient lead times | Monitoring - Globally monitor and control all local / regional cloud service consumption for continuous platform optimisation. Drive usage analytics and chargebacks through automated single unified plane of control across cloud service providers | Mitigation - Managing Cloud Platform risks like Vendor-lock-in, latency, security, and availability as an integral part of the Sourcing Strategy | . ",
    "url": "http://localhost:4000/githubPage/platforms#partnering--sourcing",
    "relUrl": "/platforms#partnering--sourcing"
  },"235": {
    "doc": "Digital Platforms",
    "title": "Dynamic Delivery",
    "content": "Pre-pandemic, delivery models were optimised with delivery across client site, nearshore and offshore delivery centres. To balance cost and value, enterprises had Co-located garages for experience and innovation complimented by distributed Agile practices. With the pandemic, this was challenges as business continuity came to the fore. Enterprises had to deal with Dispersed teams &amp; Individual silos, lack of certainty and change in priority of projects as well as security concerns - truly hindering innovation. Shift 1 . Enterprises initially focussed on recovering from this situation and getting back into being a resilient organization. This shift was focused on Employee safety, making existing team members virtually equidistant, bringing certainty to Program delivery, increasing portability of tasks and restoring Innovation. | Resilient | . | | . This shifted focus on 100% Cloudified Delivery Methods by adopting to Virtual Garage models and updated Security procedures and getting their teams skilled in virtual execution. Shift 2 . Having realized the longetivity of the situation and the ability to change, enterprises are evolving further to generate value in spite of the handicap. Delivery across sites and individuals are being optimized for skill and cost (hybrid and variable), Automation has been taken up a notch in the Cloudified Delivery Platforms. | New Normal | . | | . The focus is now on building the ability in the enterprise to absorb shocks from external unforeseen events and be able to carry out any task, at any time, from anywhere. This, in turn, as offered an improved value and cost profile compared to pre-pandemic model, . ",
    "url": "http://localhost:4000/githubPage/platforms#dynamic-delivery",
    "relUrl": "/platforms#dynamic-delivery"
  },"236": {
    "doc": "Software Engg Practices for Polyglot Apps",
    "title": "Software Engineering Practices for Polyglot apps",
    "content": "Contents . | Challenges, Patterns for these, Evolution we see and anticipate, IBM approach and capability | . ",
    "url": "http://localhost:4000/githubPage/polyglot#software-engineering-practices-for-polyglot-apps",
    "relUrl": "/polyglot#software-engineering-practices-for-polyglot-apps"
  },"237": {
    "doc": "Software Engg Practices for Polyglot Apps",
    "title": "Software Engg Practices for Polyglot Apps",
    "content": " ",
    "url": "http://localhost:4000/githubPage/polyglot",
    "relUrl": "/polyglot"
  },"238": {
    "doc": "Software Engg Practices for Polyglot Apps",
    "title": "Software Engineering Practices for Polyglot apps",
    "content": "Contents . | Challenges, Patterns for these, Evolution we see and anticipate, IBM approach and capability | . ",
    "url": "http://localhost:4000/githubPage/polyglot#software-engineering-practices-for-polyglot-apps",
    "relUrl": "/polyglot#software-engineering-practices-for-polyglot-apps"
  },"239": {
    "doc": "Software Engg Practices for Polyglot Apps",
    "title": "Software Engg Practices for Polyglot Apps",
    "content": " ",
    "url": "http://localhost:4000/githubPage/polyglot",
    "relUrl": "/polyglot"
  },"240": {
    "doc": "Agile Metrics, Processes & Toolchain",
    "title": "Agile Metrics, Processes &amp; Toolchain",
    "content": ". Table of Contents . | Performance - Measuring KPIs and SLAs . | OKR | . | IT Processes . | Example Process: Engagement &amp; Demand &gt; Demand Management | . | IT Toolchain | . ",
    "url": "http://localhost:4000/githubPage/process-kpi-tools#agile-metrics-processes--toolchain",
    "relUrl": "/process-kpi-tools#agile-metrics-processes--toolchain"
  },"241": {
    "doc": "Agile Metrics, Processes & Toolchain",
    "title": "Performance - Measuring KPIs and SLAs",
    "content": "How can I measure performance within my organisation in a way that proactively informs future decision-making while offering line of sight into accountability and decision ownership? . Metrics and KPIs need to change from retrospectively assessing performance to proactively driving action. Historically, performance metrics and KPIs were static measures that looked backward to assess progress and lacked traceability to an organisation’s strategic objectives. | Metrics | . | | . However, performance metrics must evolve to meet the demands of today’s modern IT estate. Instead of measuring past activity, metrics should be designed and implemented in a way that helps drive the organisation’s overall objectives and appropriate action, where possible, in a pre-emptive or predictive fashion. To do so, leading practice suggests organisations should use the industry recognised OKR (Objective &amp; Key Results) approach for measuring and improving performance. OKR . OKR is an Effective Methodology for Measuring Performance, OKR, known as Objective and Key Results, is a methodology is a system for setting, cascading and communicating goals throughout your organization. OKRs bring focus because they make it easy for companies, departments, and teams to recognize what the current priorities are, and to align them to employees personal objectives. Benefits of an OKR: . | Focus: OKR creates clarity and focus. Everyone has fewer defined objectives, which forces prioritisation at all levels. | Fast paced: OKRs are short term goals—typically 3 months in length. This gives employees more chances to assess and improve work. | Better results: using short term goals creates a culture where problems are solved quickly and where every contribution matters. This results in better and more consistent results | . An OKR consists of two things: . Objective - memorable qualitative description of what you want to achieve. Objectives should be short, inspirational and engaging. Objectives should motivate and challenge the team. Key Results - set of metrics that measure progress towards objective. They should not be a task. Each objective has 2-5 key results. For example, Objective #1: We will improve release velocity of the sprint Q3-2019 for version X.Y of product B as measured by …. Key Results: - Reduce bugs found during development process by 20% - Improve Unit testing coverage from 50% to 70% - increase sprint capacity from 85 item (Sprint Planning ) to 100 SP - Individual developer contributes to 20% more code review by end of every sprint . OKRs will vary across the organisation based on their priorities mapping reference OKRs to each CSOM component, such as: . | Center of Excellence Sample Objective: Drive adoption and usage of cloud services in consistent way; Sample Key Results: Enhanced skills on cloud technologies, Increased brand perception by customers | Platform Engineering &amp; Operations Sample Objective: Incubate new technology platforms; Sample Key Results: Faster time to provision infrastructure, Increased Automation with DevSecOps toolchain enabled | Integrated Service Management Sample Objective: Protect live service; Sample Key Results: Increase Process Efficiency, Proactively monitor services, Provide real-time reporting on service performance | Application Engineering &amp; Operations Sample Objective: Accelerate the development team speed; Sample Key Results: Reduced time to delivery, Change, release, incident and problem tightly coupled | . | OKR Pillars | . | | . ",
    "url": "http://localhost:4000/githubPage/process-kpi-tools#performance---measuring-kpis-and-slas",
    "relUrl": "/process-kpi-tools#performance---measuring-kpis-and-slas"
  },"242": {
    "doc": "Agile Metrics, Processes & Toolchain",
    "title": "IT Processes",
    "content": "How do I adapt my processes to drive agility &amp; speed, minimising friction without sacrificing control? . Key process underpin the capabilities required within the organisation - . Engagement &amp; Demand: the capabilities needed to translate business strategy, needs and requests into products and platforms and ensure the right skills are available across the organisation . Product Lifecycle Management: the capabilities needed to architect, develop and release both application and platform/technology products and services . Fulfillment &amp; Provisioning: the capabilities required to make products and services easily consumable and rapidly available for use when needed . Service Operations: the capabilities required to support and operate products and services and ensure they are performant, resilient and meet business and user service levels . Governance &amp; Control: the capabilities required to support and operate products and services and ensure they are performant, resilient and meet business and user service levels . | IT Processes | . | | . Example Process: Engagement &amp; Demand &gt; Demand Management . Its the process of understanding the patterns of the business’ behaviors and relate those patterns to the impact on the supply of cloud services, synchronizing the consumption (demand) with the capacity (supply) of IT resources, as documented in the Service Catalogue. Objectives . | Minimize the mismatch between demand and supply | Minimize the mismatch between demand and the service catalog | Maximize reuse of the services in the service catalog | Identify the need for new or modified services and capabilities | Identify investment implications | Minimize the lead time to service the demand | . Outcomes . | Demand and capacity forecast model | Demand Servicing Lead time | Ability to service both cloud and legacy assets | Predictability of budget estimates | . Activities . | Define Demand Patterns | Forecast Demand and Provision | Manage Demand Provision Control | Evaluate Demand Management Performance | . Cloud Adoption Considerations . | Multi-cloud service catalogs | Cloud consumption/demand policies | Trained BRMs to represent the CIO | Private cloud capacity constraints and AD resources | Demand for IT technology services to support requests from AD teams | Demand implications between cloud and legacy environments | Auto detect service usage patterns (e.g. seasonal spikes) | Continual refresh of 30-60-90 day demand forecasts | . Process View . | Demand Management Process | . | | . | Process Activities | Enterprise Business | Centre of Excellence | Application Eng &amp; Ops | Platform Eng &amp; Ops | 3rd Party Suppliers | Comments | . | Investigate demand patterns &amp; forecasts | C | A/R | R | I | I | Business IT to operate a pull function | . | Define demand patterns &amp; forecasts | A/R | C | C |   |   |   | . | Value &amp; classify business demands |   | A/R | C | I |   |   | . | Consolidate business demands &amp; forecasts |   | A/R | A/R |   |   | CSP together with Enterprise IT will consolidate demand across its customers. | . | Manage Demand provision control |   | C | A/R | A/R | I |   | . | IT Demand Strategy &amp; Plan |   | A/R | C | C | I | Business IT own development, and any subsequent revision of the IT Demand Strategy and Plan | . | Platform Infrastructure Demand Strategy &amp; Plan |   | C | C | A/R | I |   | . | Application and Services planning | I | C | A/R | C | I |   | . | Track utilization | I | I | A/R | A/R |   |   | . ",
    "url": "http://localhost:4000/githubPage/process-kpi-tools#it-processes",
    "relUrl": "/process-kpi-tools#it-processes"
  },"243": {
    "doc": "Agile Metrics, Processes & Toolchain",
    "title": "IT Toolchain",
    "content": "Toolchain is a set of tool integrations that support development, deployment, and operations tasks. In the landscape of fragmented tools and processes, it is important for the enterprise to integrate tools and practices from multiple vendors and open source projects. For an agile and innovative team, a IT tool chain is required which offers right tools for the job while keeping some standardization, allow for automating and redefining processes with AI and removes bottlenecks &amp; delays in the production line, enables collaboration across eco-system &amp; integration across team silos and embeds digital processes &amp; methods and auto-checks for compliance &amp; quality. | Toolchain | . | | . Key focus and principles of the toolchains: . | Best of Breed | Meet business demands, Objectives and maximize flow in IT Value Streams | Fully Secured and Complaint to standards | Improve Visibility and Aid Automation | Reduce Integration Challenges | Supports Continuous Integration and Delivery | . | Tooling | . | | . What are the tools required at the minimum? . | Planning and Collaboration: These are the tools that help accelerate the plan and provide transparency to stakeholders and collaboration opportunities (Slack) | Source Control: Manage the source code across all properties and assets | Tracking and escalation issues: To maintain a catalog when they troubleshoot problems or resolving issues and followed by a surge in responsiveness (Jira) | Development (CI/CD): Collaborative tools that provide dashboards to stakeholders and developers, providing transparency and the opportunity to work together during the CI/CD process | API/Integration | Security: Integrating security tools, processes and policies to the DevOps toolchain | Configuration management: A convenient way to standardize configurations across assets (Chef and Puppet) | Automation and Testing: Collaborative tools to enable continuous testing and test automation | Brokerage and Orchestration | Operations and Monitoring: Tools for monitoring provide essential information about the functioning of the software and the presence of security issues and threats. | Cloud Management | Observability and Analysis | . | Periodic Table of DevOps Tools | . | | . ",
    "url": "http://localhost:4000/githubPage/process-kpi-tools#it-toolchain",
    "relUrl": "/process-kpi-tools#it-toolchain"
  },"244": {
    "doc": "Agile Metrics, Processes & Toolchain",
    "title": "Agile Metrics, Processes & Toolchain",
    "content": " ",
    "url": "http://localhost:4000/githubPage/process-kpi-tools",
    "relUrl": "/process-kpi-tools"
  },"245": {
    "doc": "Release Engineering",
    "title": "Release Engineering",
    "content": "Contents . | Challenges, Patterns for these, Evolution we see and anticipate, IBM approach and capability | . ",
    "url": "http://localhost:4000/githubPage/release#release-engineering",
    "relUrl": "/release#release-engineering"
  },"246": {
    "doc": "Release Engineering",
    "title": "Release Engineering",
    "content": " ",
    "url": "http://localhost:4000/githubPage/release",
    "relUrl": "/release"
  },"247": {
    "doc": "Release Engineering",
    "title": "Release Engineering",
    "content": "Contents . | Challenges, Patterns for these, Evolution we see and anticipate, IBM approach and capability | . ",
    "url": "http://localhost:4000/githubPage/release#release-engineering",
    "relUrl": "/release#release-engineering"
  },"248": {
    "doc": "Release Engineering",
    "title": "Release Engineering",
    "content": " ",
    "url": "http://localhost:4000/githubPage/release",
    "relUrl": "/release"
  },"249": {
    "doc": "Security",
    "title": "Security",
    "content": " ",
    "url": "http://localhost:4000/githubPage/security#security",
    "relUrl": "/security#security"
  },"250": {
    "doc": "Security",
    "title": "Chapter Layout",
    "content": ". | Objective | Key Questions to Ask | IBM’s Approach | Sighed Benefits  | Risks | Client Stories | . ",
    "url": "http://localhost:4000/githubPage/security#chapter-layout",
    "relUrl": "/security#chapter-layout"
  },"251": {
    "doc": "Security",
    "title": "Security",
    "content": " ",
    "url": "http://localhost:4000/githubPage/security",
    "relUrl": "/security"
  },"252": {
    "doc": "Services Porfolio",
    "title": "Services Portfolio",
    "content": ". Table of Contents . | What services does the next generation IT organisation offer? | Key principles for establishing Service Porfolios | . ",
    "url": "http://localhost:4000/githubPage/services#services-portfolio",
    "relUrl": "/services#services-portfolio"
  },"253": {
    "doc": "Services Porfolio",
    "title": "What services does the next generation IT organisation offer?",
    "content": "The service portfolio outlines what is offered, developed and supported across the organisation using a common taxonomy, language and service definition. It covers all products and services across the organisation in a clear and transparent way. Why is it important? . | Provides clarity on what products and services are offered, develop and managed within the organisation and by suppliers, with identified internal product owners | Clearly defines the role between and application and platform engineering teams and establishes clear ownership | Creates clear line of sight between business services and underlying technology simplifying supportability and operations | Helps avoid duplication of functionality and accelerates development through patterns that are architected and built with the necessary security and compliance approval | Make services more consumable by describing them in a common way, make costs transparent, presenting in a curated catalogue and automating access and provisioning where possible | Helps manage the lifecycle of services driving investment decisions, architectural choices and service strategy | . | Target Operating Model - Service Portfolio | . | | . ",
    "url": "http://localhost:4000/githubPage/services#what-services-does-the-next-generation-it-organisation-offer",
    "relUrl": "/services#what-services-does-the-next-generation-it-organisation-offer"
  },"254": {
    "doc": "Services Porfolio",
    "title": "Key principles for establishing Service Porfolios",
    "content": "| Principle | Description | . | Single &amp; Hybrid Offering | - The same offering must be delivered everywhere.- Leverage capabilities from Private and Public Cloud Providers and facilitate Exit Strategies. | . | Technology Agnostic | - Focus on the abstraction and the interface defined by the service and the value which gets delivered rather than particular implementations that might be supporting it. | . | Standardized &amp; Prescriptive | - Consumer applications and services rely on the set of standardized set of Cloud Platform Services that have been published for each customer.- Each service might require a number of other mandatory service components (either by policy or as part of a bundle) in order to meet other key objectives and requirements. | . | Managed &amp; Platform-centric | - As opposed to Workload centric offering, a Platform-centric offering provides Cloud Services that presents building blocks that can be used by consumer applications and services.- A managed Cloud Service Portfolio must ensure and deliver: - Value that contributes to fulfill the vision and the strategy - Enterprise grade SLAs. | . | Differentiated | - A Single, Managed, Secure and Hybrid Multicloud offering presents a distinctive value chain that goes way beyond the one obtained by just doing a pass-through of technology proposals from different CSPs. | . | Compelling &amp; Competitive | - The Service Offering must be compelling and competitive so that customers are willing to adopt it - The CCC must make sure the solution proposed as a Service Offering is cost effective and cost efficient. | . | Agile | - The Service Definition process must follow a regular cadence of small, frequent and incremental iterations. - Prioritization efforts must take into consideration a positive feedback loop with customers and prospects. | . ",
    "url": "http://localhost:4000/githubPage/services#key-principles-for-establishing-service-porfolios",
    "relUrl": "/services#key-principles-for-establishing-service-porfolios"
  },"255": {
    "doc": "Services Porfolio",
    "title": "Services Porfolio",
    "content": " ",
    "url": "http://localhost:4000/githubPage/services",
    "relUrl": "/services"
  },"256": {
    "doc": "Squad Model",
    "title": "Squads",
    "content": "Chapter Layout . | Objective | Key Questions to Ask | IBM’s Approach | Sighed Benefits  | Risks | Client Stories | . ",
    "url": "http://localhost:4000/githubPage/squads#squads",
    "relUrl": "/squads#squads"
  },"257": {
    "doc": "Squad Model",
    "title": "Squad Model",
    "content": " ",
    "url": "http://localhost:4000/githubPage/squads",
    "relUrl": "/squads"
  },"258": {
    "doc": "Continuous Improvement with SRE",
    "title": "Continuous Improvement with Site Reliability Engineering (SRE)",
    "content": ". Table of Contents . | What is SRE? . | SRE and Application Development | . | SRE Practices . | Principles and Tenets of SRE | Tenets of SRE | . | AIOps for SRE . | Shift Left | Automated Operations | AIOps in Integrated Service Management Toolchain | . | IBM SRE Approach | . ",
    "url": "http://localhost:4000/githubPage/devsecops-sre#continuous-improvement-with-site-reliability-engineering-sre",
    "relUrl": "/devsecops-sre#continuous-improvement-with-site-reliability-engineering-sre"
  },"259": {
    "doc": "Continuous Improvement with SRE",
    "title": "What is SRE?",
    "content": "Site Reliability Engineering (SRE) is an DevSecOps discipline that aims at developing automation in operational aspects of running IT application. This includes infrastructure provisioning, responding to faults and events, managing performance, etc. It was conceptualized in Google in 2003 for their systems and has been adopted by the industry over a period as an essential set of practices. SRE is an engineering approach to deliberately introduce reliability into site operations. Fundamentally, it’s what happens when you ask a software engineer to design an operations function. SRE fundamentally changes the approach to systems management. It recognizes that throwing bodies at the problem is not a long term approach. Introducing ever and more complex layers of management does not yield a winning strategy. SRE and Application Development . SRE is an important discipline for application development in the current and future. It has evolved into a product development paradigm - an ongoing process of incrementally delivering new capabilities, continuous focus in required on maintaining and improving the resilience of the application. ",
    "url": "http://localhost:4000/githubPage/devsecops-sre#what-is-sre",
    "relUrl": "/devsecops-sre#what-is-sre"
  },"260": {
    "doc": "Continuous Improvement with SRE",
    "title": "SRE Practices",
    "content": "| SRE Practices | . | | . Key elements of an Engineering approach to Operations . | Understand your system System thinking! You can get still things done without understanding, but if you know what’s happening, you can innovate. | Ask great questions Asking a really good question is like telepathically moving information from someone else’s brain to yours. | Read the code Even difficult codebases, even in languages you don’t know. It can be easier than you expect to go look at the code and see what’s really happening. | Debug like a wizard To be better at debugging. | Write down a design Understand what all can go wrong. Use design docs, project briefs, etc. They help you catch misconceptions early and get good ideas. | Understand the big picture Better technical decisions. Either be sure that you know why the thing’s important, or go do something else. | . Principles and Tenets of SRE . An effect SRE practice has the following principles: . | System Thinking | Data-Driven Decisions | Engineering Rigid | Embracing Risk | Eliminating Toil | Remove Technical Debt | Simplicity | Collaboration | Shared Responsibility | Trust &amp; Transparency | . If you take these principles and try to apply them to operating a system you will recognize some common features: . | Ops to scale with load through Automation, but don’t stop at Automation | Cap operational load: 50% time spent on toil - 50% on engineering projects (improvements) | Excess Ops work overflows to the Dev Team, share 5% of Ops work with Dev Team | Have an SLA / SLO for the service, measure against the SLA / SLO | Error budget to control velocity. Effective self-regulation of features vs. stability | Observability, including the Golden Signals: Latency, Traffic, Errors, Saturation, Requests | Actionable symptom-based alerts, from the user perspective. (Automated) runbooks to govern actions. | Blameless post-mortem for every event | Common staffing pool for SRE and Dev | . Tenets of SRE . | Scale Ops sub-linearly with load | Cap Operational load at 50% | Handle Overflow | SLA/SLO/SLI | ORP &amp; Error Budget | Golden Signals | Symptom-based Alerting | Blameless Postmortems | Staffing Pool | . ",
    "url": "http://localhost:4000/githubPage/devsecops-sre#sre-practices",
    "relUrl": "/devsecops-sre#sre-practices"
  },"261": {
    "doc": "Continuous Improvement with SRE",
    "title": "AIOps for SRE",
    "content": "Capture the most value from your cloud operations with Site Reliability Engineers (SRE) that are central to the ‘operations-centric’ design of our cloud management solutions. SRE principles enable enterprises to optimize their hybrid multicloud management and deliver: . | High resiliency and elastic architecture | Reduction in errors and service requests | Optimized saturation ensuring resources are not constrained | Reduced latency by identifying hotspots and providing feedback to the development squads to automate the fixes | Engineering automation into the operations process | . Infusion of AIOps techniques in operations can allow systems to continuously improve and become more reliable. | AIOps PoV | . | | . Shift Left . Automation with AI techniques enable us to shift incident resolution closer to the source of problems – towards zero touch application management. Shift left resulting in improved service resilience, efficiency &amp; user experience through proactive, automated &amp; cognitive application maintenance. | AIOps Shift Left | . | | . Automated Operations . Embrace automated operations and intelligent management utilizing AI Ops to enhance performance across your enterprise. | Automated Operations | . | | . Automate manual tasks involved in incident diagnosis, incident resolution, service request fulfillment, change implementation, scheduled / preventive maintenance &amp; more to shift incident resolution closer to the source of problems. AIOps in Integrated Service Management Toolchain . | Integrated Service Management Toolchain | . | | . ",
    "url": "http://localhost:4000/githubPage/devsecops-sre#aiops-for-sre",
    "relUrl": "/devsecops-sre#aiops-for-sre"
  },"262": {
    "doc": "Continuous Improvement with SRE",
    "title": "IBM SRE Approach",
    "content": "IBM SRE approach brings measurable benefits through balance between Development &amp; Operations. | IBM Approach | . | | . Key components of IBM’s SRE approach are: . | Identify and Design for the real reliability need . | Baseline SLOs (instead of 100% of 3 sec response time, target for 90%) | Review architecture for significant use cases | Define IaaC framework | Enhance and Configure custom code quality checks | . | Use the error budget to release new features . | Architect for Blue Green / Canary deployments with minimum or no downtime | Arrive at error budgets and approve new features to be deployed. (When the availability targets are met) | . | Observe everything . | Implement Build to manage with heartbeat APIs when functional APIs are secured | Design and Implement Dashboards for various KPIs | Introduce distributed tracing to pinpoint the transaction failures | . | Automate everything which is repeatable . | Identify toil and implement automated runbook | Create manual runbooks for toils which cannot be automated for L1.5 | . | Consistent CI/CD . | Implement touchless CI/CD with quality gates and automated (e.g. code coverage) regression tests | Introduce Performance tests with Jmeter in CI/CD | . | Ease of Operations . | SREs jump in as first responders for Sev 1 issues | Introduce blameless port-mortem | Chat Ops with Bots for creating automated ticketing and resolution | Document RCAs and raise enhancement tickets for Dev Team | . | . IBM has infused SRE into its Integrated Operations Reference Architecture too: . | Infused SRE in Integrated Operations Reference Architecture | . | | . ",
    "url": "http://localhost:4000/githubPage/devsecops-sre#ibm-sre-approach",
    "relUrl": "/devsecops-sre#ibm-sre-approach"
  },"263": {
    "doc": "Continuous Improvement with SRE",
    "title": "Continuous Improvement with SRE",
    "content": " ",
    "url": "http://localhost:4000/githubPage/devsecops-sre",
    "relUrl": "/devsecops-sre"
  },"264": {
    "doc": "Organizational Structure",
    "title": "How are enterprises organising themselves in the digital and multicloud world?",
    "content": ". Table of Contents . | Overview | Center of Excellence | Platform Provider | Digital Product Co-creator | Integrated Service Management | . ",
    "url": "http://localhost:4000/githubPage/structure#how-are-enterprises-organising-themselves-in-the-digital-and-multicloud-world",
    "relUrl": "/structure#how-are-enterprises-organising-themselves-in-the-digital-and-multicloud-world"
  },"265": {
    "doc": "Organizational Structure",
    "title": "Overview",
    "content": "Modern IT functions are typically supporting the development of cloud enabled, digital products in several ways: . | 4 Pillars | . | | . | Center of Excellence - Drives adoption and usage of cloud services in a controlled, secure and consistent way . | Advise on cloud adoption to lines of business | Develop guardrails and standards for cloud consumption | Curate a catalogue of cloud services approved for use within the enterprise | Provide education and talent development | . | Platform Provider - Creates a platform of business &amp; technology services to support digital transformation . | Incubate the new technology platforms to be consumed by Digital/LoB teams | Act as a broker and integrator across hybrid multi cloud environments | Provide operational platform support | Enable DevSecOps toolchain and capabilities | . | Digital Product Co-creator - Partners with lines of business to compete successfully in the marketplace . | Operate on product lines towards new digital propositions | Operate agile end to end teams | Drive design thinking for new engineering practices | Experiment and scale fast | Full end to end service accountability | . | Integrated Service Management - Centralised command centre to protect live service and drive service excellence across the enterprise . | Level 1/1.5 support function | Command centre proactively monitoring services, enabled by analytics and automation | Provides real-time reporting on service performance | Drives service excellence across the enterprise for operations including Site Reliability best practices | . | . These elements interact together to provide an integrate eco-system to create and operate digital products. Within each of these elements, there are various functions that support product development and operations. | Target Operating Model - 4 Pillars | . | | . ",
    "url": "http://localhost:4000/githubPage/structure#overview",
    "relUrl": "/structure#overview"
  },"266": {
    "doc": "Organizational Structure",
    "title": "Center of Excellence",
    "content": "The role of the Centre of Excellence is to drive cloud adoption through governance, to build communities and advise on consumption and usage to meet business requirements: . | Sets guardrails and policies to provide choice and agility, without comprising security and compliance | Sets technical architecture and guidelines for development of products and services | Delivers education to product teams with line of business and cloud/infrastructure platform teams | Promote asset reuse, best practices and recommendations | Manages communities to encourage collaboration | Provide advice on sourcing and vendor selection | Deliver professional services to support and augment product teams | . ",
    "url": "http://localhost:4000/githubPage/structure#center-of-excellence",
    "relUrl": "/structure#center-of-excellence"
  },"267": {
    "doc": "Organizational Structure",
    "title": "Platform Provider",
    "content": "The role of Platform Provider (Platform Engineering &amp; Operations) is to develop and operate cloud, technology and infrastructure products for use across the enterprise: . | Develops and operates enterprise wide services technical services to be consumed by lines of business IT or value streams | Develops services on public and private cloud | Manage end-to-end lifecycle of platform services | Manage performance and capacity of platform services | Develop and operate shared tooling services for consumption across the enterprise | Providers data platform services for consumption across the enterprise | Develops and operates integration services including API publishing and management | Manages and maintains the core infrastructure including network, physical infrastructure and data centres | . ",
    "url": "http://localhost:4000/githubPage/structure#platform-provider",
    "relUrl": "/structure#platform-provider"
  },"268": {
    "doc": "Organizational Structure",
    "title": "Digital Product Co-creator",
    "content": "The role of Digital Product co-creator (Application Engineering &amp; Operations) is to develop and operate business and application services to deliver customer and business outcomes: . | Develops and operates business and application services | Focused on delivering business outcomes and creating new customer/consumer experiences | Help re-defining business process and workflows to deliver better outcomes | Work in cross-functional, agile teams working in conjunction with business users | Manages the end-to-end lifecycle of business and application services | Have overall end-to-end service responsibility | Consume and utilise products and services developed and operated by platform teams | Optimise business service/application performance and service levels | . ",
    "url": "http://localhost:4000/githubPage/structure#digital-product-co-creator",
    "relUrl": "/structure#digital-product-co-creator"
  },"269": {
    "doc": "Organizational Structure",
    "title": "Integrated Service Management",
    "content": "The role of Integrated Service Management is to provide centralised first line support, Operations Command Centre and driving service management excellence across the organisation: . | Provides level 1/1.5 support capability across applications and platforms | Operations Command Centre providing centralised monitoring and enabled by analytics and automation | Drive service excellence across the organisation for incident, problem and change management | Managing configuration and asset management across the organization to have a fully integrated view of all assets within the IT estate | Provide centralised real-time reporting and dashboarding capability on service performance | . ",
    "url": "http://localhost:4000/githubPage/structure#integrated-service-management",
    "relUrl": "/structure#integrated-service-management"
  },"270": {
    "doc": "Organizational Structure",
    "title": "Organizational Structure",
    "content": " ",
    "url": "http://localhost:4000/githubPage/structure",
    "relUrl": "/structure"
  },"271": {
    "doc": "Testing Practices",
    "title": "Testing Practices for Future apps",
    "content": "Contents . | Challenges, Patterns for these, Evolution we see and anticipate, IBM approach and capability | . ",
    "url": "http://localhost:4000/githubPage/testing-practices#testing-practices-for-future-apps",
    "relUrl": "/testing-practices#testing-practices-for-future-apps"
  },"272": {
    "doc": "Testing Practices",
    "title": "Testing Practices",
    "content": " ",
    "url": "http://localhost:4000/githubPage/testing-practices",
    "relUrl": "/testing-practices"
  },"273": {
    "doc": "Testing Approach",
    "title": "Testing Approach",
    "content": "Chapter Layout . | Objective | Key Questions to Ask | IBM’s Approach | Sighed Benefits  | Risks | Client Stories | . ",
    "url": "http://localhost:4000/githubPage/testing-approach#testing-approach",
    "relUrl": "/testing-approach#testing-approach"
  },"274": {
    "doc": "Testing Approach",
    "title": "Testing Approach",
    "content": " ",
    "url": "http://localhost:4000/githubPage/testing-approach",
    "relUrl": "/testing-approach"
  },"275": {
    "doc": "Tooling",
    "title": "DevSecOps Tooling",
    "content": ". Table of Contents . | Background . | Managing Tool Sprawl | Centralized Management | Toolchain monitoring and management | Optimization with continuous feedback | . | IBM Approach | . ",
    "url": "http://localhost:4000/githubPage/devsecops-tooling#devsecops-tooling",
    "relUrl": "/devsecops-tooling#devsecops-tooling"
  },"276": {
    "doc": "Tooling",
    "title": "Background",
    "content": "Security and compliance have become top priorities in today’s volatile and uncertain business environment and must be integrated into software lifecycle to maintain velocity. | DevSecOps | . | | . Enterprises are under pressure to implement DevSecOps to modernize and be agile, deliver faster, acquire industry best practices, but they face serious implementation challenges. Managing Tool Sprawl . Individual business units choose from a plethora of DevSecOps tools in the market, usually without an enterprise level view resulting in tool sprawl . Centralized Management . Without a management layer and GitOps, pipeline definitions become too decentralized and creates pipeline sprawl. Centralized access control with pattern-based approach is necessary for every organization. Toolchain monitoring and management . Ongoing management of the toolchain instances and application delivery pipelines require significant efforts. Optimization with continuous feedback . The need for continuous feedback and optimization to evaluate the impact of each release and gauge end user satisfaction is essential to deliver the right business outcomes. ",
    "url": "http://localhost:4000/githubPage/devsecops-tooling#background",
    "relUrl": "/devsecops-tooling#background"
  },"277": {
    "doc": "Tooling",
    "title": "IBM Approach",
    "content": ". | Help Clients to adopt a sustainable DevSecOps model, building upon any transformation they have already undertaken | Understand and build upon existing target model and transformation plan on different fronts: people, process, technologies as required | Build upon any foundational steps taken for creating a DevSecOps culture and roll-out across the client organization | . | IBM Approach | . | | . Solution Design Points . | Assessment of client’s current SDLC state/development process and pain points | Build upon existing definition of target DevSecOps processes, toolchain and implementation plan, filling gaps where required | Provide DevSecOps framework artifacts and reusable materials for the client organization customization and roll-out | . Key Outcomes and Benefits . | Defined the target DevSecOps framework | Maximise shift left approach in DevSecOps environment | Defined optimization points for security activities within the SDLC | . ",
    "url": "http://localhost:4000/githubPage/devsecops-tooling#ibm-approach",
    "relUrl": "/devsecops-tooling#ibm-approach"
  },"278": {
    "doc": "Tooling",
    "title": "Tooling",
    "content": " ",
    "url": "http://localhost:4000/githubPage/devsecops-tooling",
    "relUrl": "/devsecops-tooling"
  },"279": {
    "doc": "Tooling",
    "title": "Tooling Architecture",
    "content": " ",
    "url": "http://localhost:4000/githubPage/tooling#tooling-architecture",
    "relUrl": "/tooling#tooling-architecture"
  },"280": {
    "doc": "Tooling",
    "title": "Chapter Layout",
    "content": ". | Objective | Key Questions to Ask | IBM’s Approach | Sighed Benefits  | Risks | Client Stories | . ",
    "url": "http://localhost:4000/githubPage/tooling#chapter-layout",
    "relUrl": "/tooling#chapter-layout"
  },"281": {
    "doc": "Tooling",
    "title": "Tooling",
    "content": " ",
    "url": "http://localhost:4000/githubPage/tooling",
    "relUrl": "/tooling"
  },"282": {
    "doc": "Relevance of Tranditional Practices",
    "title": "Relevance of traditional practices",
    "content": "e.g. Component model, Critical Design Review, Req Traceability . Contents . | Challenges, Patterns for these, Evolution we see and anticipate, IBM approach and capability | . ",
    "url": "http://localhost:4000/githubPage/traditional-practices#relevance-of-traditional-practices",
    "relUrl": "/traditional-practices#relevance-of-traditional-practices"
  },"283": {
    "doc": "Relevance of Tranditional Practices",
    "title": "Relevance of Tranditional Practices",
    "content": " ",
    "url": "http://localhost:4000/githubPage/traditional-practices",
    "relUrl": "/traditional-practices"
  },"284": {
    "doc": "Transformation & Filtering Engine",
    "title": "Transformation &amp; Filtering Engine",
    "content": ". | Filtering Engine | . | | . When an event driven integration approach is followed, there could be lot of unwanted events being raised. This pattern helps to reduce this noise and process only relevant events by applying transformation and filtering rules and caching processed events. | Filtering and transformation rules are used to: . | Drop irrelevant events (i.e. CDC might send everything) | Set event destination | . | Event cache is used to: . | Filter out duplicate events | Aggregate low-level events | . | Transformation engine is used to: . | Apply event transformation logic | Applying data transformation rules, splitting complex events and aggregating low level events | Defines destination for transformed events | . | . ",
    "url": "http://localhost:4000/githubPage/trans-filtering-engine#transformation--filtering-engine",
    "relUrl": "/trans-filtering-engine#transformation--filtering-engine"
  },"285": {
    "doc": "Transformation & Filtering Engine",
    "title": "Transformation & Filtering Engine",
    "content": " ",
    "url": "http://localhost:4000/githubPage/trans-filtering-engine",
    "relUrl": "/trans-filtering-engine"
  },"286": {
    "doc": "Vision, Mission & Goals",
    "title": "Vision, Mission &amp; Goals",
    "content": ". Table of Contents . | What do we want to be and what do we want to achieve? | Constructing a vision, mission and principles . | Guiding Principles | How Vision, Mission &amp; Goals effect the organizations in the enterprise | . | . ",
    "url": "http://localhost:4000/githubPage/transform#vision-mission--goals",
    "relUrl": "/transform#vision-mission--goals"
  },"287": {
    "doc": "Vision, Mission & Goals",
    "title": "What do we want to be and what do we want to achieve?",
    "content": "| Target Operating Model Vision | . | | . Defining the vision for your organization, the services it will provide and the capabilities needed to deliver these, shaping how all other dimensions of the operating model need to be designed and the priority in which they need to be addressed. Defining from the outset a clear vision, mission &amp; set of principles is critical as it sets the direction, tone and shape of all other operating dimensions. ",
    "url": "http://localhost:4000/githubPage/transform#what-do-we-want-to-be-and-what-do-we-want-to-achieve",
    "relUrl": "/transform#what-do-we-want-to-be-and-what-do-we-want-to-achieve"
  },"288": {
    "doc": "Vision, Mission & Goals",
    "title": "Constructing a vision, mission and principles",
    "content": "| Target Operating Model Vision | . | | . The two top layers of the detail are the ways in which vision and mission statements align to clients overarching strategy. Vision describes what we aspire to achieve e.g. Creating the very best digital products, enabled by flexible platforms and the best talent. Mission describles what we do and who we do it for. These mission statements are subsequently met through key transformation levers which describe how the vision and mission be achieved. The foundational layer introduces the key principles that guide the direction of transformation and underpin the way in which digital products are created and cloud is adopted within IT and adopted across the enterprise. Guiding Principles . The guiding principles set the direction and guardrails for operating model transformation. A strong principle is supported by a clear rationale and associated implications. | Target Operating Model Principles | . | | . A Guiding Principle is a high level statement of intent or purpose intended to guide and govern the design &amp; operation of the operating model. The Rationale highlights the business benefits of adopting the principle. These are directly related to the mission statements. Implications are tasks that must happen in order to successfully implement the principle to realise the associated benefits. Here is a set of principles that are emerging in transformative enterprises: . | Business Oriented - In everything we do, have what we are trying to achieve as a business and what delivers value at heart. | Innovation - Constantly innovate in the products we create, the way we work, looking for inspiration in how we can transform our business by embracing the latest technology. | Agile culture - Create an agile culture where everyone is looking to deliver value through every action we take and making that available to others. | Developer First - Put the developer experience at the forefront of what we do and enable them to create business value and digital products that delight our customers. | Self-Service - Create products and platforms that can be easy accessed, understood and consumed, in a self-service way wherever possible. | Sharing - Share for the benefit of your colleagues, teams and the enterprise through assets, practices, experiences, skills and support. | . Principles will have an impact on the design and elaboration across a range of operating model dimensions, such as the 2nd example above i.e. Innovation. | Principle . | Constantly innovate in the products we create, the way we work, looking for inspiration in how we can transform our business by embracing the latest technology. | . | Rationale . | Our industry is underthreat from new start-up market entrants and therefore we need to make sure we innovate what we do to ensure we are constantly providing the best customer experience. | We need to transform the way we think and work as an organistion to ensure we are always improving what we do to be more effective and efficient, focusing on what adds value. | . | Implications . | Ways of Working: Provide Design Thinking and User Journey training to everyone in the organisation | Workforce Model: Allow headroom to our colleague to tackle ‘pet projects’ | Financial Model: Provide funding for ‘sandpitting’ of new technology to every employee | Processes: Create a mechanism to showcase potential initiatives to receive innovation funding | Leadership &amp; Culture: Develop a culture where people are encouraged to think differently and beyond the everyday | Partnering &amp; Sourcing: Establish a regular forum for our partners to showcase their latest thinking and technology | Financial Model: Provide funding for colleauges to attend conferences and technology showcases | Governance: Develop and innovation forum as part of our governance structure | Locations &amp; Facilities: Create innovation spaces where people can collaborate outside of normal teams | . | . How Vision, Mission &amp; Goals effect the organizations in the enterprise . Over the course of the operating model transformation, each area of the organisation may elaborate their own set of principles in-line with the overall vision &amp; mission. | . ",
    "url": "http://localhost:4000/githubPage/transform#constructing-a-vision-mission-and-principles",
    "relUrl": "/transform#constructing-a-vision-mission-and-principles"
  },"289": {
    "doc": "Vision, Mission & Goals",
    "title": "Vision, Mission & Goals",
    "content": " ",
    "url": "http://localhost:4000/githubPage/transform",
    "relUrl": "/transform"
  },"290": {
    "doc": "Saga",
    "title": "Saga",
    "content": "Saga is an architectural pattern that provides an elegant approach to implement a transaction that spans multiple services and is asynchronous and reactive in nature. Each service in a Saga performs its own local transaction and publishes an event. The successive services listen to that event and perform the next local transaction. If one transaction fails for some reason, the Saga also executes compensating transactions to undo the impact of the preceding transactions. | Saga | . | | . Different ways to implement the Saga pattern . There are two logical ways to implement the Saga pattern: choreography and orchestration. 1. Choreography . In the Saga choreography pattern, each individual microservice that is part of a process publishes an event that is picked up by the successive microservice. You must make decisions early in the microservice development lifecycle to understand if it will be part of a Saga pattern or not, since you must choose an appropriate framework that will help implement this pattern. To adopt a particular framework code, the microservice must be decorated with annotations, class initializations, or other configuration changes. In the Saga choreography pattern, the SEC (Saga Execution Coordinator) can be embedded within the microservice or in most of the scenarios is a standalone component. 2. Orchestration . As the name of the Saga orchestration pattern suggests, there is a single orchestrator component that is responsible for managing the overall process flow. If the process encounters an error while calling any individual microservice, then it is responsible for calling the compensating service too. The orchestrator helps model the Saga flow, but also relies on the underlying framework to call the services in a sequence and make compensating calls if any of the services fail. For more in depth view on the saga pattern, please refer here. ",
    "url": "http://localhost:4000/githubPage/txn-patterns-saga",
    "relUrl": "/txn-patterns-saga"
  },"291": {
    "doc": "Try-Confirm-Cancel",
    "title": "Try-Confirm-Cancel",
    "content": ". | What is TCC | Roles in TCC | TCC Stages | . ",
    "url": "http://localhost:4000/githubPage/txn-patterns-tcc",
    "relUrl": "/txn-patterns-tcc"
  },"292": {
    "doc": "Try-Confirm-Cancel",
    "title": "What is TCC",
    "content": ". | TCC patterns helps in managing distributed transactions for Systems using REST APIs. | It helps implementing distributed atomic transactions across REST services that aligns well with HTTP and REST principles. | It provides a reservation process across multiple, independent REST services where failures before completion are guaranteed to have no effect and the system returns to its earlier consistent state. | . ",
    "url": "http://localhost:4000/githubPage/txn-patterns-tcc#what-is-tcc",
    "relUrl": "/txn-patterns-tcc#what-is-tcc"
  },"293": {
    "doc": "Try-Confirm-Cancel",
    "title": "Roles in TCC",
    "content": ". | TCC Participant services are TCC-aware REST service providers. | Coordinator service is what we provide: a reusable service to manage the consistent confirmation (or cancellation) of a set of related participant service invocations (including recovery). | Business process manages business flow by making use of the TCC participants and coordinator service. | . ",
    "url": "http://localhost:4000/githubPage/txn-patterns-tcc#roles-in-tcc",
    "relUrl": "/txn-patterns-tcc#roles-in-tcc"
  },"294": {
    "doc": "Try-Confirm-Cancel",
    "title": "TCC Stages",
    "content": ". | Every REST service that participates in a distributed transaction needs to be TCC aware and is called as a TCC participant | A TCC coordinator that manages the TCC flow on behalf of the client process. | Client process invokes services on the TCC participants and TCC coordinator. | . | TCC | . | | . There are 2 stages in TCC: . | Try / Reserve . | The TCC coordinator makes an invocation via HTTP REST Call (mostly POST) on each TCC participant so that the participant can reserve a resource on behalf of the client process. | The reserved state is identified by a unique URI that can be used to confirm the reservation later and an optional expiration date/time after which the participant can autonomously cancel and move the resource back to the initial state. | . | Confirm / Cancel . | This state is reached when a reserved participant receives a confirmation message within the specified time frame. | If the business transaction fails, then the TCC coordinator makes an invocation via HTTP REST Call (mostly DELETE) on each participant so that the participant can unreserve the resource reserved in the previous stage. | If the business transaction succeeds, then the TCC coordinator makes an invocation via HTTP REST Call (mostly PUT) on each participant so that the participant can finalize the reservation on the resource reserved in the previous stage. | . | . If there is any failures before stage #2, then everything will time out and cancel because of confirmations not being sent to TCC participants within stipulated time. If there are failures during stage#2, then they are handled by the coordinator service, including recovery after crashes, network failures etc. This yields transactional guarantees for REST services. ",
    "url": "http://localhost:4000/githubPage/txn-patterns-tcc#tcc-stages",
    "relUrl": "/txn-patterns-tcc#tcc-stages"
  },"295": {
    "doc": "Strangler Pattern",
    "title": "Strangler Pattern",
    "content": "| Strangler Pattern | . | | . | While the modernization is in progress, there is a need to route the incoming requests to either current state or target state based on a pre-determined logic. Strangler pattern helps us do this. | Strangler component helps us to incrementally migrate a legacy system by gradually replacing specific pieces of functionality with new applications and services. As features from the legacy system are replaced, the new system eventually replaces all of the old system’s features, strangling the old system and allowing you to decommission it. | This strangler component could be a standalone component sitting outside the application or a wrapper of core programs sitting inside the current state. | The strangler component captures requests initiated and does one of the following: . | Streams them to the modernized side for synchronization purposes | Calls either current or modernized state applications | Calls both current and modernized state applications | . | The strangler component could use a ‘routing store’ that keeps logic for request handling and it can be based on request type or/and specific field(s) values (i.e. account type or branch based). | . ",
    "url": "http://localhost:4000/githubPage/txreqmgmt-patterns-strangler",
    "relUrl": "/txreqmgmt-patterns-strangler"
  },"296": {
    "doc": "Workforce",
    "title": "Workforce Management &amp; Culture",
    "content": ". This section tries to answer the following two questions: . | How should resources be best organised and deployed to develop digital products? | How do I change my culture to become more agile and business outcome centric? | . Table of Contents . | Workforce Organization | Culture &amp; Leadership . | Culture | Leadership | References | . | 6. Workforce Management &amp; Culture . | Workforce Organization | Culture &amp; Leadership . | Culture | Leadership | References | . | . | . ",
    "url": "http://localhost:4000/githubPage/workforce#workforce-management--culture",
    "relUrl": "/workforce#workforce-management--culture"
  },"297": {
    "doc": "Workforce",
    "title": "Workforce Organization",
    "content": "The next-generation operating model is made up of agile teams. | Tribe Squads | . | | . A Tribe can be visualized as an incubator for mini start-ups or major product development. The size of a Tribe is typically not more than 100 people. Example: Loans &amp; Deposit Services, Mortgage Services, Data Services could be valid tribes in a banking enterprise. A Chapter is a horizontal slice across Squads within a Tribe. They have similar skills, work within the same general competency area, and within the same Tribe. A Guild is an enterprise-level Chapter. Example: Mobile Development, Service Management &amp; Operational Testing. Squads, as autonomous service product development unit. Example: Account balance checking, Mortgage application process &amp; Data Warehouse in the context mentioned above. This structure is applied across all areas of the organisation: . | Tribes within Application Engineering &amp; Operations are organised around value streams or lines of business, with individual squads for individual services/products within them | Tribes within Platform Engineering &amp; Operations are organised around major platform areas (i.e. tooling, security etc) , with individual squads for platform product sets (i.e. automation tooling) or individual services (i.e. Terraform) | Guilds are managed and co-ordinated out of the Centre of Excellence and Integrated Service Management for various disciplines and communities of practice. They also have their own squads for developing their own products (i.e. Learning Management Platform, ITSM Tooling) and artefacts (i.e. architecture patterns, process definition) | . | Tribes and Pillars | . | | . Application and Platform Squads may also form an integrated service tribe: . Service Tribe . Integrated Services Delivery Leader for an Service is the Tribe leader for that Service, cutting across the various Application and Platform squads that support that application. Application Squad . Manages the end to end lifecycle of the Application. Platform Squad . Manages the end to end lifecycle of the Platform. Squad leader (or designate) of the Platform squad participates in the sprint planning of the applications they support; and translate platform related backlog items to the Platform squad backlog. Designated resources in the Platform squad provide coverage for each Application that the Platform supports. Platform developers in the Platform squad own creation of patterns needed for the application they are assigned to. Platform SREs in the Platform squad work with the Application SREs to support end to end service reliability for the application they are assigned to. | Tribes and Pillars | . | | . ",
    "url": "http://localhost:4000/githubPage/workforce#workforce-organization",
    "relUrl": "/workforce#workforce-organization"
  },"298": {
    "doc": "Workforce",
    "title": "Culture &amp; Leadership",
    "content": ". Culture . An Agile Culture captures and creates more business value. It is based on more digital behaviour therefore, is Consumer &amp; Product Value Centric as opposed to the traditional business and IT cultures which were People, Process &amp; Technology Centric. Here are some ways in which Agile culture is different from a traditional cultures: . | Traditional Cultures | Agile Culture | . | 1.People intensive, Office hour desk culture | 1.Automation &amp; Self-Service knowledge culture 24x7 | . | 2.Budget &amp; cost centric operations | 2.Value &amp; outcome centric operations | . | 3.Hierarchical Structure &amp; KPIs drive dept silos | 3.Empowered Teams &amp; KPIs cut-across depts | . | 4.Expertise &amp; information silos - protective | 4.Open Collaboration, Sharing, Transparency | . | 5.Project Roadmaps - deliver to spec, do not fail | 5.Product Roadmaps – visionary aspiration, experimental | . | 6.Corp Planning &amp; resource allocation processes - for certainty | 6.Dynamic planning &amp; resource allocation – for uncertainty | . | 7.Waterfall / Sequential - stability, risk-averse | 7.Iterative, agile – continuous release, multi-thread | . | 8.Service Quality through process &amp; audits | 8.Service Quality through prediction &amp; automation | . Developers are at the center of a Cloud Transformation and key drivers of digital product success: . | Cultural Drivers | . | | . There are 26 leading practices to help drive culture &amp; leadership change. Dynamic Teams . | Network of empowered teams | Data insights drive actions | Innovation driven process changes | Active partnerships and eco-system | Investments in disruptive technology | Distributed decision making | . Next Generation Leadership . | Exec Leaders 100% aligned to a BOLD North Star | Translate the North Star vision to actionable strategic guidance | Active, capable, and visible leadership | Performance transparency and metrics | Sensing and seizing opportunities | . Accelerated Culture . | Experimentation and rapid iteration | Anchor to underlining core values | Innovation and intrinsic rewards | Adaptable ways of working | Customer experience drives decision making | Enterprise-wide thinking | . Talent Reinvented . | Exponential learning investments | Flexible talent allocation | Engaged experts | Autonomous teams | Realtime performance feedback | . Engaged &amp; Inspired . | Seamless continuous feedback across the eco-system | Inspired, co-created experiences | Mindset of curiosity | High levels of community, trust and respect | . Leadership . There are a number of factors that make good leaders that can be measured and assessed. Here is a model that consists of three constructs: Confidence, Openness and Impact. There are 5 sub-components for each of the three constructs grouped into development priorities: Accelerator, Growth, Inhibitors. | Leadership Model | . | | . This model measures factors which underpin behaviours, rather than the behaviours themselves. Facilitates a deeper insight for Leaders individual barriers and motivational drivers to change. References . Cultural Practices . | https://www.washingtonpost.com/lifestyle/travel/new-artificial-intelligence-promises-to-make-travel-a-little-smarter-does-it/2017/06/22/819c0880-3590-11e7-b373-418f6849a004_story.html?utm_term=.34f4be9cca8a | https://www.ibm.com/blogs/think/2017/03/iot-shared-data/ | http://duettoresearch.com/expedia-report-mobile-travel-booking-will-get-bigger/ | http://www-935.ibm.com/services/multimedia/uk_en_airlines_2020.pdf | https://skift.com/2017/07/27/expedia-to-invest-350-million-in-indonesia-booking-site-traveloka/ | https://www.recode.net/2017/1/31/14462256/amazon-air-cargo-hub-kentucky-airport-prime-air | . ",
    "url": "http://localhost:4000/githubPage/workforce#culture--leadership",
    "relUrl": "/workforce#culture--leadership"
  },"299": {
    "doc": "Workforce",
    "title": "6. Workforce Management &amp; Culture",
    "content": "This section tries to answer the following two questions: . | How should resources be best organised and deployed to develop digital products? | How do I change my culture to become more agile and business outcome centric? | . ",
    "url": "http://localhost:4000/githubPage/workforce#6-workforce-management--culture",
    "relUrl": "/workforce#6-workforce-management--culture"
  },"300": {
    "doc": "Workforce",
    "title": "Workforce Organization",
    "content": "The next-generation operating model is made up of agile teams. | Tribe Squads | . | | . A Tribe can be visualized as an incubator for mini start-ups or major product development. The size of a Tribe is typically not more than 100 people. Example: Loans &amp; Deposit Services, Mortgage Services, Data Services could be valid tribes in a banking enterprise. A Chapter is a horizontal slice across Squads within a Tribe. They have similar skills, work within the same general competency area, and within the same Tribe. A Guild is an enterprise-level Chapter. Example: Mobile Development, Service Management &amp; Operational Testing. Squads, as autonomous service product development unit. Example: Account balance checking, Mortgage application process &amp; Data Warehouse in the context mentioned above. This structure is applied across all areas of the organisation: . | Tribes within Application Engineering &amp; Operations are organised around value streams or lines of business, with individual squads for individual services/products within them | Tribes within Platform Engineering &amp; Operations are organised around major platform areas (i.e. tooling, security etc) , with individual squads for platform product sets (i.e. automation tooling) or individual services (i.e. Terraform) | Guilds are managed and co-ordinated out of the Centre of Excellence and Integrated Service Management for various disciplines and communities of practice. They also have their own squads for developing their own products (i.e. Learning Management Platform, ITSM Tooling) and artefacts (i.e. architecture patterns, process definition) | . | Tribes and Pillars | . | | . Application and Platform Squads may also form an integrated service tribe: . Service Tribe . Integrated Services Delivery Leader for an Service is the Tribe leader for that Service, cutting across the various Application and Platform squads that support that application. Application Squad . Manages the end to end lifecycle of the Application. Platform Squad . Manages the end to end lifecycle of the Platform. Squad leader (or designate) of the Platform squad participates in the sprint planning of the applications they support; and translate platform related backlog items to the Platform squad backlog. Designated resources in the Platform squad provide coverage for each Application that the Platform supports. Platform developers in the Platform squad own creation of patterns needed for the application they are assigned to. Platform SREs in the Platform squad work with the Application SREs to support end to end service reliability for the application they are assigned to. | Tribes and Pillars | . | | . ",
    "url": "http://localhost:4000/githubPage/workforce#workforce-organization-1",
    "relUrl": "/workforce#workforce-organization-1"
  },"301": {
    "doc": "Workforce",
    "title": "Culture &amp; Leadership",
    "content": "Culture . An Agile Culture captures and creates more business value. It is based on more digital behaviour therefore, is Consumer &amp; Product Value Centric as opposed to the traditional business and IT cultures which were People, Process &amp; Technology Centric. Here are some ways in which Agile culture is different from a traditional cultures: . | Traditional Cultures | Agile Culture | . | 1.People intensive, Office hour desk culture | 1.Automation &amp; Self-Service knowledge culture 24x7 | . | 2.Budget &amp; cost centric operations | 2.Value &amp; outcome centric operations | . | 3.Hierarchical Structure &amp; KPIs drive dept silos | 3.Empowered Teams &amp; KPIs cut-across depts | . | 4.Expertise &amp; information silos - protective | 4.Open Collaboration, Sharing, Transparency | . | 5.Project Roadmaps - deliver to spec, do not fail | 5.Product Roadmaps – visionary aspiration, experimental | . | 6.Corp Planning &amp; resource allocation processes - for certainty | 6.Dynamic planning &amp; resource allocation – for uncertainty | . | 7.Waterfall / Sequential - stability, risk-averse | 7.Iterative, agile – continuous release, multi-thread | . | 8.Service Quality through process &amp; audits | 8.Service Quality through prediction &amp; automation | . Developers are at the center of a Cloud Transformation and key drivers of digital product success: . | Cultural Drivers | . | | . There are 26 leading practices to help drive culture &amp; leadership change. Dynamic Teams . | Network of empowered teams | Data insights drive actions | Innovation driven process changes | Active partnerships and eco-system | Investments in disruptive technology | Distributed decision making | . Next Generation Leadership . | Exec Leaders 100% aligned to a BOLD North Star | Translate the North Star vision to actionable strategic guidance | Active, capable, and visible leadership | Performance transparency and metrics | Sensing and seizing opportunities | . Accelerated Culture . | Experimentation and rapid iteration | Anchor to underlining core values | Innovation and intrinsic rewards | Adaptable ways of working | Customer experience drives decision making | Enterprise-wide thinking | . Talent Reinvented . | Exponential learning investments | Flexible talent allocation | Engaged experts | Autonomous teams | Realtime performance feedback | . Engaged &amp; Inspired . | Seamless continuous feedback across the eco-system | Inspired, co-created experiences | Mindset of curiosity | High levels of community, trust and respect | . Leadership . There are a number of factors that make good leaders that can be measured and assessed. Here is a model that consists of three constructs: Confidence, Openness and Impact. There are 5 sub-components for each of the three constructs grouped into development priorities: Accelerator, Growth, Inhibitors. | Leadership Model | . | | . This model measures factors which underpin behaviours, rather than the behaviours themselves. Facilitates a deeper insight for Leaders individual barriers and motivational drivers to change. References . Cultural Practices . | https://www.washingtonpost.com/lifestyle/travel/new-artificial-intelligence-promises-to-make-travel-a-little-smarter-does-it/2017/06/22/819c0880-3590-11e7-b373-418f6849a004_story.html?utm_term=.34f4be9cca8a | https://www.ibm.com/blogs/think/2017/03/iot-shared-data/ | http://duettoresearch.com/expedia-report-mobile-travel-booking-will-get-bigger/ | http://www-935.ibm.com/services/multimedia/uk_en_airlines_2020.pdf | https://skift.com/2017/07/27/expedia-to-invest-350-million-in-indonesia-booking-site-traveloka/ | https://www.recode.net/2017/1/31/14462256/amazon-air-cargo-hub-kentucky-airport-prime-air | . ",
    "url": "http://localhost:4000/githubPage/workforce#culture--leadership-1",
    "relUrl": "/workforce#culture--leadership-1"
  },"302": {
    "doc": "Workforce",
    "title": "Workforce",
    "content": " ",
    "url": "http://localhost:4000/githubPage/workforce",
    "relUrl": "/workforce"
  }
}
